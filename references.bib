
@software{imagemagick,
  author = {{ImageMagick Studio LLC}},
  title = {ImageMagick},
  url = {https://imagemagick.org},
  version = {7.1.1},
  date = {2024-01-04},
}

@article{cook_why_2021,
	title = {Why is the literature on first impressions so focused on {White} faces?},
	volume = {8},
	issn = {2054-5703},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211146},
	doi = {10.1098/rsos.211146},
	abstract = {We spontaneously attribute to strangers a wide variety of character traits based on their facial appearance. While these first impressions have little or no basis in reality, they exert a strong influence over our behaviour. Cognitive scientists have revealed a great deal about first impressions from faces including their factor structure, the cues on which they are based, the neurocognitive mechanisms responsible, and their developmental trajectory. In this field, authors frequently strive to remove as much ethnic variability from stimulus sets as possible. Typically, this convention means that participants are asked to judge the likely traits of White faces only. In the present article, we consider four possible reasons for the lack of facial diversity in this literature and find that it is unjustified. Next, we illustrate how the focus on White faces has undermined scientific efforts to understand first impressions from faces and argue that it reinforces socially regressive ideas about ‘race’ and status. We go on to articulate our concern that opportunities may be lost to leverage the knowledge derived from the study of first impressions against the dire consequences of prejudice and discrimination. Finally, we highlight some promising developments in the field.},
	language = {en},
	number = {9},
	urldate = {2022-04-22},
	journal = {Royal Society Open Science},
	author = {Cook, Richard and Over, Harriet},
	month = sep,
	year = {2021},
	pages = {211146},
	file = {Cook and Over - 2021 - Why is the literature on first impressions so focu.pdf:/Users/thora/Zotero/storage/E9TUQ5BW/Cook and Over - 2021 - Why is the literature on first impressions so focu.pdf:application/pdf},
}

@article{jones_which_2021,
	title = {To which world regions does the valence–dominance model of social perception apply?},
	volume = {5},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-020-01007-2},
	doi = {10.1038/s41562-020-01007-2},
	abstract = {Over the past 10 years, Oosterhof and Todorov’s valence–dominance model has emerged as the most prominent account of how people evaluate faces on social dimensions. In this model, two dimensions (valence and dominance) underpin social judgements of faces. Because this model has primarily been developed and tested in Western regions, it is unclear whether these findings apply to other regions. We addressed this question by replicating Oosterhof and Todorov’s methodology across 11 world regions, 41 countries and 11,570 participants. When we used Oosterhof and Todorov’s original analysis strategy, the valence–dominance model generalized across regions. When we used an alternative methodology to allow for correlated dimensions, we observed much less generalization. Collectively, these results suggest that, while the valence–dominance model generalizes very well across regions when dimensions are forced to be orthogonal, regional differences are revealed when we use different extraction methods and correlate and rotate the dimension reduction solution.},
	language = {en},
	number = {1},
	urldate = {2022-04-22},
	journal = {Nature Human Behaviour},
	author = {Jones, Benedict C. and DeBruine, Lisa M. and Flake, Jessica K. and Liuzza, Marco Tullio and Antfolk, Jan and Arinze, Nwadiogo C. and Ndukaihe, Izuchukwu L. G. and Bloxsom, Nicholas G. and Lewis, Savannah C. and Foroni, Francesco and Willis, Megan L. and Cubillas, Carmelo P. and Vadillo, Miguel A. and Turiegano, Enrique and Gilead, Michael and Simchon, Almog and Saribay, S. Adil and Owsley, Nicholas C. and Jang, Chaning and Mburu, Georgina and Calvillo, Dustin P. and Wlodarczyk, Anna and Qi, Yue and Ariyabuddhiphongs, Kris and Jarukasemthawee, Somboon and Manley, Harry and Suavansri, Panita and Taephant, Nattasuda and Stolier, Ryan M. and Evans, Thomas R. and Bonick, Judson and Lindemans, Jan W. and Ashworth, Logan F. and Hahn, Amanda C. and Chevallier, Coralie and Kapucu, Aycan and Karaaslan, Aslan and Leongómez, Juan David and Sánchez, Oscar R. and Valderrama, Eugenio and Vásquez-Amézquita, Milena and Hajdu, Nandor and Aczel, Balazs and Szecsi, Peter and Andreychik, Michael and Musser, Erica D. and Batres, Carlota and Hu, Chuan-Peng and Liu, Qing-Lan and Legate, Nicole and Vaughn, Leigh Ann and Barzykowski, Krystian and Golik, Karolina and Schmid, Irina and Stieger, Stefan and Artner, Richard and Mues, Chiel and Vanpaemel, Wolf and Jiang, Zhongqing and Wu, Qi and Marcu, Gabriela M. and Stephen, Ian D. and Lu, Jackson G. and Philipp, Michael C. and Arnal, Jack D. and Hehman, Eric and Xie, Sally Y. and Chopik, William J. and Seehuus, Martin and Azouaghe, Soufian and Belhaj, Abdelkarim and Elouafa, Jamal and Wilson, John P. and Kruse, Elliott and Papadatou-Pastou, Marietta and De La Rosa-Gómez, Anabel and Barba-Sánchez, Alan E. and González-Santoyo, Isaac and Hsu, Tsuyueh and Kung, Chun-Chia and Wang, Hsiao-Hsin and Freeman, Jonathan B. and Oh, Dong Won and Schei, Vidar and Sverdrup, Therese E. and Levitan, Carmel A. and Cook, Corey L. and Chandel, Priyanka and Kujur, Pratibha and Parganiha, Arti and Parveen, Noorshama and Pati, Atanu Kumar and Pradhan, Sraddha and Singh, Margaret M. and Pande, Babita and Bavolar, Jozef and Kačmár, Pavol and Zakharov, Ilya and Álvarez-Solas, Sara and Baskin, Ernest and Thirkettle, Martin and Schmidt, Kathleen and Christopherson, Cody D. and Leonis, Trinity and Suchow, Jordan W. and Olofsson, Jonas K. and Jernsäther, Teodor and Lee, Ai-Suan and Beaudry, Jennifer L. and Gogan, Taylor D. and Oldmeadow, Julian A. and Balas, Benjamin and Stevens, Laura M. and Colloff, Melissa F. and Flowe, Heather D. and Gülgöz, Sami and Brandt, Mark J. and Hoyer, Karlijn and Jaeger, Bastian and Ren, Dongning and Sleegers, Willem W. A. and Wissink, Joeri and Kaminski, Gwenaël and Floerke, Victoria A. and Urry, Heather L. and Chen, Sau-Chin and Pfuhl, Gerit and Vally, Zahir and Basnight-Brown, Dana M. and Jzerman, Hans I. and Sarda, Elisa and Neyroud, Lison and Badidi, Touhami and Van der Linden, Nicolas and Tan, Chrystalle B. Y. and Kovic, Vanja and Sampaio, Waldir and Ferreira, Paulo and Santos, Diana and Burin, Debora I. and Gardiner, Gwendolyn and Protzko, John and Schild, Christoph and Ścigała, Karolina A. and Zettler, Ingo and O’Mara Kunz, Erin M. and Storage, Daniel and Wagemans, Fieke M. A. and Saunders, Blair and Sirota, Miroslav and Sloane, Guyan V. and Lima, Tiago J. S. and Uittenhove, Kim and Vergauwe, Evie and Jaworska, Katarzyna and Stern, Julia and Ask, Karl and van Zyl, Casper J. J. and Körner, Anita and Weissgerber, Sophia C. and Boudesseul, Jordane and Ruiz-Dodobara, Fernando and Ritchie, Kay L. and Michalak, Nicholas M. and Blake, Khandis R. and White, David and Gordon-Finlayson, Alasdair R. and Anne, Michele and Janssen, Steve M. J. and Lee, Kean Mun and Nielsen, Tonje K. and Tamnes, Christian K. and Zickfeld, Janis H. and Rosa, Anna Dalla and Vianello, Michelangelo and Kocsor, Ferenc and Kozma, Luca and Putz, Ádám and Tressoldi, Patrizio and Irrazabal, Natalia and Chatard, Armand and Lins, Samuel and Pinto, Isabel R. and Lutz, Johannes and Adamkovic, Matus and Babincak, Peter and Baník, Gabriel and Ropovik, Ivan and Coetzee, Vinet and Dixson, Barnaby J. W. and Ribeiro, Gianni and Peters, Kim and Steffens, Niklas K. and Tan, Kok Wei and Thorstenson, Christopher A. and Fernandez, Ana Maria and Hsu, Rafael M. C. S. and Valentova, Jaroslava V. and Varella, Marco A. C. and Corral-Frías, Nadia S. and Frías-Armenta, Martha and Hatami, Javad and Monajem, Arash and Sharifian, MohammadHasan and Frohlich, Brooke and Lin, Hause and Inzlicht, Michael and Alaei, Ravin and Rule, Nicholas O. and Lamm, Claus and Pronizius, Ekaterina and Voracek, Martin and Olsen, Jerome and Giolla, Erik Mac and Akgoz, Aysegul and Özdoğru, Asil A. and Crawford, Matthew T. and Bennett-Day, Brooke and Koehn, Monica A. and Okan, Ceylan and Gill, Tripat and Miller, Jeremy K. and Dunham, Yarrow and Yang, Xin and Alper, Sinan and Borras-Guevara, Martha Lucia and Cai, Sun Jun and Tiantian, Dong and Danvers, Alexander F. and Feinberg, David R. and Armstrong, Marie M. and Gilboa-Schechtman, Eva and McCarthy, Randy J. and Muñoz-Reyes, Jose Antonio and Polo, Pablo and Shiramazu, Victor K. M. and Yan, Wen-Jing and Carvalho, Lilian and Forscher, Patrick S. and Chartier, Christopher R. and Coles, Nicholas A.},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Psychology},
	pages = {159--169},
	file = {Full Text:/Users/thora/Zotero/storage/5Y9XM9BR/Jones et al. - 2021 - To which world regions does the valence–dominance .pdf:application/pdf;Snapshot:/Users/thora/Zotero/storage/MQQ3TJDR/s41562-020-01007-2.html:text/html},
}

@article{workman_face_2021,
	title = {The {Face} {Image} {Meta}-{Database} ({fIMDb}) \& {ChatLab} {Facial} {Anomaly} {Database} ({CFAD}): {Tools} for research on face perception and social stigma},
	volume = {5},
	issn = {2590-2601},
	shorttitle = {The {Face} {Image} {Meta}-{Database} ({fIMDb}) \& {ChatLab} {Facial} {Anomaly} {Database} ({CFAD})},
	url = {https://www.sciencedirect.com/science/article/pii/S2590260121000205},
	doi = {10.1016/j.metip.2021.100063},
	abstract = {Investigators increasingly need high quality face photographs that they can use in service of their scholarly pursuits—whether serving as experimental stimuli or to benchmark face recognition algorithms. Up to now, an index of known face databases, their features, and how to access them has not been available. This absence has had at least two negative repercussions: First, without alternatives, some researchers may have used face databases that are widely known but not optimal for their research. Second, a reliance on databases comprised only of young white faces will lead to science that isn't representative of all the people whose tax contributions, in many cases, make that research possible. The “Face Image Meta-Database” (fIMDb) provides researchers with the tools to find the face images best suited to their research, with filters to locate databases with people of a varied racial and ethnic backgrounds and ages. Problems of representation in face databases are not restricted to race and ethnicity or age – there is a dearth of databases with faces that have visible differences (e.g., scars, port wine stains, and cleft lip and palate). A well-characterized database is needed to support programmatic research into perceivers' attitudes, behaviors, and neural responses to anomalous faces. The “ChatLab Facial Anomaly Database” (CFAD) was constructed to fill this gap, with photographs of faces with visible differences of various types, etiologies, sizes, locations, and that depict individuals from various ethnic backgrounds and age groups. Both the fIMDb and CFAD are available from: https://cliffordworkman.com/resources/.},
	language = {en},
	urldate = {2022-04-22},
	journal = {Methods in Psychology},
	author = {Workman, Clifford I. and Chatterjee, Anjan},
	month = dec,
	year = {2021},
	keywords = {Faces, Perception, Photographs, Stigma, Stimuli, Visible difference},
	pages = {100063},
	file = {ScienceDirect Snapshot:/Users/thora/Zotero/storage/HYHB6VS8/S2590260121000205.html:text/html;Submitted Version:/Users/thora/Zotero/storage/DSJIDQAB/Workman and Chatterjee - 2021 - The Face Image Meta-Database (fIMDb) & ChatLab Fac.pdf:application/pdf},
}

@article{ma_chicago_2021,
	title = {Chicago {Face} {Database}: {Multiracial} expansion},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Chicago {Face} {Database}},
	url = {https://doi.org/10.3758/s13428-020-01482-5},
	doi = {10.3758/s13428-020-01482-5},
	abstract = {Multiracial individuals represent a growing segment of the population and have been increasingly the focus of empirical study. Much of this research centers on the perception and racial categorization of multiracial individuals. The current paper reviews some of this research and describes the different types of stimuli that have been used in these paradigms. We describe the strengths and weaknesses associated with different operationalizations of multiracialism and highlight the dearth of research using faces of real multiracial individuals, which we posit may be due to the lack of available stimuli. Our research seeks to satisfy this need by providing a free set of high-resolution, standardized images featuring 88 real multiracial individuals along with extensive norming data and objective physical measures of these faces. These data are offered as an extension of the widely used Chicago Face Database and are available for download at www.chicagofaces.orgfor use in research.},
	number = {3},
	urldate = {2022-04-22},
	journal = {Behavior Research Methods},
	author = {Ma, Debbie S. and Kantner, Justin and Wittenbrink, Bernd},
	month = jun,
	year = {2021},
	keywords = {Face perception, Face database, Biracial, Multiracial, Racial categorization},
	pages = {1289--1300},
	file = {Full Text PDF:/Users/thora/Zotero/storage/3MLU9K8F/Ma et al. - 2021 - Chicago Face Database Multiracial expansion.pdf:application/pdf},
}

@book{zebrowitz_reading_1997,
	title = {Reading {Faces}: {Window} {To} {The} {Soul}?},
	isbn = {978-0-429-97281-2},
	shorttitle = {Reading {Faces}},
	abstract = {Do we read character in faces? What information do faces actually provide? What are the social and psychological consequences of reading character in faces? Zebrowitz unmasks the face and provides the first systematic, scientific account of our tendency to judge people by their appearance. Offering an in-depth discussion of two appearance qualities that influence our impressions of others—“baby-faceness” and “attractiveness”—and an analysis of these impressions, Zebrowitz has written an accessible and valuable book for professionals and general readers alike.},
	language = {en},
	publisher = {Routledge},
	author = {Zebrowitz, Leslie},
	year = {1997},
	note = {Google-Books-ID: 9pRLDwAAQBAJ},
	keywords = {Social Science / General, Social Science / Sociology / General},
}

@article{ma_chicago_2015,
	title = {The {Chicago} face database: {A} free stimulus set of faces and norming data},
	volume = {47},
	issn = {1554-3528},
	shorttitle = {The {Chicago} face database},
	url = {https://doi.org/10.3758/s13428-014-0532-5},
	doi = {10.3758/s13428-014-0532-5},
	abstract = {Researchers studying a range of psychological phenomena (e.g., theory of mind, emotion, stereotyping and prejudice, interpersonal attraction, etc.) sometimes employ photographs of people as stimuli. In this paper, we introduce the Chicago Face Database, a free resource consisting of 158 high-resolution, standardized photographs of Black and White males and females between the ages of 18 and 40 years and extensive data about these targets. In Study 1, we report pre-testing of these faces, which includes both subjective norming data and objective physical measurements of the images included in the database. In Study 2 we surveyed psychology researchers to assess the suitability of these targets for research purposes and explored factors that were associated with researchers’ judgments of suitability. Instructions are outlined for those interested in obtaining access to the stimulus set and accompanying ratings and measures.},
	language = {en},
	number = {4},
	urldate = {2022-10-03},
	journal = {Behavior Research Methods},
	author = {Ma, Debbie S. and Correll, Joshua and Wittenbrink, Bernd},
	month = dec,
	year = {2015},
	keywords = {Face database, Multiracial faces, Normed face stimuli},
	pages = {1122--1135},
	file = {Full Text PDF:/Users/thora/Zotero/storage/FZW97YIR/Ma et al. - 2015 - The Chicago face database A free stimulus set of .pdf:application/pdf},
}

@article{oosterhof_functional_2008,
	title = {The functional basis of face evaluation},
	volume = {105},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.0805664105},
	doi = {10.1073/pnas.0805664105},
	abstract = {People automatically evaluate faces on multiple trait dimensions, and these evaluations predict important social outcomes, ranging from electoral success to sentencing decisions. Based on behavioral studies and computer modeling, we develop a 2D model of face evaluation. First, using a principal components analysis of trait judgments of emotionally neutral faces, we identify two orthogonal dimensions, valence and dominance, that are sufficient to describe face evaluation and show that these dimensions can be approximated by judgments of trustworthiness and dominance. Second, using a data-driven statistical model for face representation, we build and validate models for representing face trustworthiness and face dominance. Third, using these models, we show that, whereas valence evaluation is more sensitive to features resembling expressions signaling whether the person should be avoided or approached, dominance evaluation is more sensitive to features signaling physical strength/weakness. Fourth, we show that important social judgments, such as threat, can be reproduced as a function of the two orthogonal dimensions of valence and dominance. The findings suggest that face evaluation involves an overgeneralization of adaptive mechanisms for inferring harmful intentions and the ability to cause harm and can account for rapid, yet not necessarily accurate, judgments from faces.},
	number = {32},
	urldate = {2023-03-13},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Oosterhof, Nikolaas N. and Todorov, Alexander},
	month = aug,
	year = {2008},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {11087--11092},
	file = {Full Text PDF:/Users/thora/Zotero/storage/VGDU9S69/Oosterhof and Todorov - 2008 - The functional basis of face evaluation.pdf:application/pdf},
}

@book{perrett_your_2017,
	title = {In {Your} {Face}: {The} new science of human attraction},
	isbn = {978-0-230-36484-4},
	shorttitle = {In {Your} {Face}},
	abstract = {In our daily lives, in our memories and fantasies, our mental worlds overflow with faces. But what do we really know about this most remarkable feature of the human body? Why do we have faces at all, and brains that are good at reading them? What do our looks say – and not say – about our personalities?And perhaps the most compelling question of all: Why are we attracted to some faces more than others? In Your Face is an engaging and authoritative tour of the science of facial beauty and face perception.David Perrett, the pre-eminent scholar in the field, reveals and interprets the most remarkable findings and in the process demolishes many popular myths, setting the record straight on what neuroscience and evolutionary psychology are teaching us about beauty. The record is more surprising and often more unsettling than you might think.},
	language = {en},
	publisher = {Bloomsbury Publishing},
	author = {Perrett, D. I.},
	month = sep,
	year = {2017},
	note = {Google-Books-ID: 7ZhGEAAAQBAJ},
	keywords = {Psychology / Cognitive Psychology \& Cognition},
}

@article{lakshmi_india_2021,
	title = {The {India} {Face} {Set}: {International} and {Cultural} {Boundaries} {Impact} {Face} {Impressions} and {Perceptions} of {Category} {Membership}},
	volume = {12},
	issn = {1664-1078},
	shorttitle = {The {India} {Face} {Set}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.627678},
	abstract = {This paper serves three specific goals. First, it reports the development of an Indian Asian face set, to serve as a free resource for psychological research. Second, it examines whether the use of pre-tested U.S.-specific norms for stimulus selection or weighting may introduce experimental confounds in studies involving non-U.S. face stimuli and/or non-U.S. participants. Specifically, it examines whether subjective impressions of the face stimuli are culturally dependent, and the extent to which these impressions reflect social stereotypes and ingroup favoritism. Third, the paper investigates whether differences in face familiarity impact accuracy in identifying face ethnicity. To this end, face images drawn from volunteers in India as well as a subset of Caucasian face images from the Chicago Face Database were presented to Indian and U.S. participants, and rated on a range of measures, such as perceived attractiveness, warmth, and social status. Results show significant differences in the overall valence of ratings of ingroup and outgroup faces. In addition, the impression ratings show minor differentiation along two basic stereotype dimensions, competence and trustworthiness, but not warmth. We also find participants to show significantly greater accuracy in correctly identifying the ethnicity of ingroup faces, relative to outgroup faces. This effect is found to be mediated by ingroup-outgroup differences in perceived group typicality of the target faces. Implications for research on intergroup relations in a cross-cultural context are discussed.},
	urldate = {2023-11-13},
	journal = {Frontiers in Psychology},
	author = {Lakshmi, Anjana and Wittenbrink, Bernd and Correll, Joshua and Ma, Debbie S.},
	year = {2021},
	file = {Full Text PDF:/Users/thora/Zotero/storage/JBZSBS6Q/Lakshmi et al. - 2021 - The India Face Set International and Cultural Bou.pdf:application/pdf},
}

@article{meyers_hawaii_2024,
	title = {Hawai‘i {Face} {Database}: {A} racially and ethnically diverse set of facial stimuli},
	shorttitle = {Hawai‘i {Face} {Database}},
	url = {https://osf.io/wde4b},
	doi = {10.31234/osf.io/wde4b},
	abstract = {Within psychology, face perception processes have been widely studied, examining traits and social categories that a face can communicate. However, much of this research has predominately focused on White faces. We review existing face databases that include racially diverse stimuli and note the lack of representation of Asian subgroups (e.g., East, South, and Southeast Asian), Pacific Islanders, as well as both Multiracial and multiethnic faces (especially with multiple minoritized backgrounds). We provide a new racially diverse set of free, standardized images including 140 unique faces representing eight different groups that vary in ethnicity and race (Asian, East Asian, Southeast Asian, Pacific Islander/Native Hawaiian, Hispanic/Latinx, White, Multiracial, and Multiracial Asian), along with norming data. These images and data are available for access use in research: https://osf.io/fkn7y},
	language = {en-us},
	urldate = {2024-01-24},
	author = {Meyers, Chanel and Garay, Maria and Pauker, Kristin},
	month = jan,
	year = {2024},
	note = {Publisher: OSF},
	file = {Snapshot:/Users/thora/Zotero/storage/4M6PT3VM/wde4b.html:text/html;Submitted Version:/Users/thora/Zotero/storage/KWSRKNKX/Meyers et al. - 2024 - Hawai‘i Face Database A racially and ethnically d.pdf:application/pdf},
}

@article{sutherland_social_2013,
	title = {Social inferences from faces: {Ambient} images generate a three-dimensional model},
	volume = {127},
	issn = {0010-0277},
	shorttitle = {Social inferences from faces},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027712002739},
	doi = {10.1016/j.cognition.2012.12.001},
	abstract = {Three experiments are presented that investigate the two-dimensional valence/trustworthiness by dominance model of social inferences from faces (Oosterhof \& Todorov, 2008). Experiment 1 used image averaging and morphing techniques to demonstrate that consistent facial cues subserve a range of social inferences, even in a highly variable sample of 1000 ambient images (images that are intended to be representative of those encountered in everyday life, see Jenkins, White, Van Montfort, \& Burton, 2011). Experiment 2 then tested Oosterhof and Todorov’s two-dimensional model on this extensive sample of face images. The original two dimensions were replicated and a novel ‘youthful-attractiveness’ factor also emerged. Experiment 3 successfully cross-validated the three-dimensional model using face averages directly constructed from the factor scores. These findings highlight the utility of the original trustworthiness and dominance dimensions, but also underscore the need to utilise varied face stimuli: with a more realistically diverse set of face images, social inferences from faces show a more elaborate underlying structure than hitherto suggested.},
	number = {1},
	urldate = {2024-02-01},
	journal = {Cognition},
	author = {Sutherland, Clare A. M. and Oldmeadow, Julian A. and Santos, Isabel M. and Towler, John and Michael Burt, D. and Young, Andrew W.},
	month = apr,
	year = {2013},
	keywords = {Face perception, First impressions, Social inferences},
	pages = {105--118},
	file = {Full Text:/Users/thora/Zotero/storage/DQRQQS53/Sutherland et al. - 2013 - Social inferences from faces Ambient images gener.pdf:application/pdf;ScienceDirect Snapshot:/Users/thora/Zotero/storage/C8MGDKAQ/S0010027712002739.html:text/html},
}

@article{trebicky_focal_2016,
	title = {Focal {Length} {Affects} {Depicted} {Shape} and {Perception} of {Facial} {Images}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149313},
	doi = {10.1371/journal.pone.0149313},
	abstract = {Static photographs are currently the most often employed stimuli in research on social perception. The method of photograph acquisition might affect the depicted subject’s facial appearance and thus also the impression of such stimuli. An important factor influencing the resulting photograph is focal length, as different focal lengths produce various levels of image distortion. Here we tested whether different focal lengths (50, 85, 105 mm) affect depicted shape and perception of female and male faces. We collected three portrait photographs of 45 (22 females, 23 males) participants under standardized conditions and camera setting varying only in the focal length. Subsequently, the three photographs from each individual were shown on screen in a randomized order using a 3-alternative forced-choice paradigm. The images were judged for attractiveness, dominance, and femininity/masculinity by 369 raters (193 females, 176 males). Facial width-to-height ratio (fWHR) was measured from each photograph and overall facial shape was analysed employing geometric morphometric methods (GMM). Our results showed that photographs taken with 50 mm focal length were rated as significantly less feminine/masculine, attractive, and dominant compared to the images taken with longer focal lengths. Further, shorter focal lengths produced faces with smaller fWHR. Subsequent GMM revealed focal length significantly affected overall facial shape of the photographed subjects. Thus methodology of photograph acquisition, focal length in this case, can significantly affect results of studies using photographic stimuli perhaps due to different levels of perspective distortion that influence shapes and proportions of morphological traits.},
	language = {en},
	number = {2},
	urldate = {2024-02-19},
	journal = {PLOS ONE},
	author = {Třebický, Vít and Fialová, Jitka and Kleisner, Karel and Havlíček, Jan},
	month = feb,
	year = {2016},
	note = {Publisher: Public Library of Science},
	keywords = {Face, Photography, Eyes, Cameras, Eye lens, Light, Morphometry, Optical lenses},
	pages = {e0149313},
	file = {Full Text PDF:/Users/thora/Zotero/storage/EP45G8B9/Třebický et al. - 2016 - Focal Length Affects Depicted Shape and Perception.pdf:application/pdf},
}

@misc{lundqvist_karolinska_2015,
	title = {Karolinska {Directed} {Emotional} {Faces}},
	url = {https://doi.apa.org/doi/10.1037/t27732-000},
	doi = {10.1037/t27732-000},
	language = {en},
	urldate = {2025-05-14},
	author = {Lundqvist, D. and Flykt, A. and Öhman, A.},
	month = may,
	year = {2015},
	note = {Institution: American Psychological Association},
}

@article{gao_cas-peal_2008,
	title = {The {CAS}-{PEAL} {Large}-{Scale} {Chinese} {Face} {Database} and {Baseline} {Evaluations}},
	volume = {38},
	issn = {1558-2426},
	url = {https://ieeexplore.ieee.org/document/4404053},
	doi = {10.1109/TSMCA.2007.909557},
	abstract = {In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.},
	number = {1},
	urldate = {2025-05-14},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
	author = {Gao, Wen and Cao, Bo and Shan, Shiguang and Chen, Xilin and Zhou, Delong and Zhang, Xiaohua and Zhao, Debin},
	month = jan,
	year = {2008},
	keywords = {Face recognition, face recognition, Cameras, expression, Access protocols, Accessory, Application software, Computer science, Computer security, evaluation protocol, face databases, Image databases, Large-scale systems, lighting, Object recognition, pose, Research and development},
	pages = {149--161},
	file = {Snapshot:/Users/thora/Zotero/storage/GL66YRQN/4404053.html:text/html},
}

@article{chen_broadening_2021,
	title = {Broadening the stimulus set: {Introducing} the {American} {Multiracial} {Faces} {Database}},
	volume = {53},
	issn = {1554-3528},
	shorttitle = {Broadening the stimulus set},
	url = {https://doi.org/10.3758/s13428-020-01447-8},
	doi = {10.3758/s13428-020-01447-8},
	abstract = {Face-based perceptions form the basis for how people behave towards each other and, hence, are central to understanding human interaction. Studying face perception requires a large and diverse set of stimuli in order to make ecologically valid, generalizable conclusions. To date, there are no publicly available databases with a substantial number of Multiracial or racially ambiguous faces. Our systematic review of the literature on Multiracial person perception documented that published studies have relied on computer-generated faces (84\% of stimuli), Black-White faces (74\%), and male faces (63\%). We sought to address these issues, and to broaden the diversity of available face stimuli, by creating the American Multiracial Faces Database (AMFD). The AMFD is a novel collection of 110 faces with mixed-race heritage and accompanying ratings of those faces by naive observers that are freely available to academic researchers. The faces (smiling and neutral expression poses) were rated on attractiveness, emotional expression, racial ambiguity, masculinity, racial group membership(s), gender group membership(s), warmth, competence, dominance, and trustworthiness. The large majority of the AMFD faces are racially ambiguous and can pass into at least two different racial categories. These faces will be useful to researchers seeking to study Multiracial person perception as well as those looking for racially ambiguous faces in order to study categorization processes in general. Consequently, the AMFD will be useful to a broad group of researchers who are studying face perception.},
	language = {en},
	number = {1},
	urldate = {2025-10-29},
	journal = {Behavior Research Methods},
	author = {Chen, Jacqueline M. and Norman, Jasmine B. and Nam, Yeseul},
	month = feb,
	year = {2021},
	keywords = {Face perception, Impression formation, Multiracial, Face stimuli, Mixed-race},
	pages = {371--389},
	file = {Full Text PDF:/Users/thora/Zotero/storage/USL9SVWN/Chen et al. - 2021 - Broadening the stimulus set Introducing the American Multiracial Faces Database.pdf:application/pdf},
}

@article{trzewik_israeli_2025,
	title = {The {Israeli} {Face} {Database} ({IFD}): {A} multi-ethnic database of faces with supporting social norming data},
	volume = {57},
	issn = {1554-3528},
	shorttitle = {The {Israeli} {Face} {Database} ({IFD})},
	url = {https://doi.org/10.3758/s13428-025-02723-1},
	doi = {10.3758/s13428-025-02723-1},
	abstract = {Human faces and their representations play a key role in social interactions. Consequently, face images are widely used as stimuli across psychological research. However, most available face databases primarily represent Western, predominantly White populations, raising concerns about the generalizability of findings derived from such nondiverse stimuli sets. The current work addresses this problem by introducing the Israeli Face Database (IFD), a novel database presenting face images of real individuals from an ethnically diverse population underrepresented in psychological research on face perception—people residing in Israel. The IFD features multiple highly controlled images per face identity, presenting several facial expressions. Additionally, we provide detailed demographic data on the face identities, expression validation data rated by Israeli participants, and social norming data rated by both Israeli and US-based participants. Validation data (Study 1) confirmed that the expression images in the IFD accurately convey their intended expressions. Norming data analysis (Study 2) revealed that judgments of IFD faces were broadly consistent across cultures, with notable variations in perceived typicality and perceived ethnic diversity. These results highlight the IFD’s strong psychometric properties and its ability to capture nuanced cultural and individual differences in face perception. We propose that the IFD can serve as a valuable tool for diversifying stimuli in social-psychological research.},
	language = {en},
	number = {7},
	urldate = {2025-10-29},
	journal = {Behavior Research Methods},
	author = {Trzewik, Maayan and Navon, Mayan and Moran, Tal and Wardi, Hadas and Langer, Adi and Hadad, Bat-Sheva and Sofer, Carmel and Reggev, Niv},
	month = jun,
	year = {2025},
	keywords = {Face perception, Face database, Social perception, Diversity, Emotional expression, Israel},
	pages = {197},
	file = {Full Text PDF:/Users/thora/Zotero/storage/S98IC9C4/Trzewik et al. - 2025 - The Israeli Face Database (IFD) A multi-ethnic database of faces with supporting social norming dat.pdf:application/pdf},
}

@article{dawel_faking_2025,
	title = {Faking {It} {Isn}’t {Making} {It}: {Research} {Needs} {Spontaneous} and {Naturalistic} {Facial} {Expressions}},
	issn = {2662-205X},
	shorttitle = {Faking {It} {Isn}’t {Making} {It}},
	url = {https://doi.org/10.1007/s42761-025-00320-1},
	doi = {10.1007/s42761-025-00320-1},
	abstract = {Facial expressions play a pivotal role in shaping social interactions. However, the conceptualization of facial expressions as direct readouts of internal emotional experience has led to the conflation of three distinct question types. Specifically, there is confusion between questions concerning: (Q1) the production of facial expressions, (Q2) how accurately perceivers interpret expressors’ internal emotions from their outward expressions, and (Q3) perceiver responses to the outward appearance of expressions independent of the expressor’s internal emotional state. The disentanglement of these three question types highlights that, because the facial stimuli traditionally used in research are posed rather than reflective of internal emotions, they can only test perceiver responses (Q3), though they have often been interpreted as measures of perceptual accuracy (Q2). Moreover, due to their exaggerated and prototypical nature, these stimuli fail to capture the nuance and complexity of real-world expressions, potentially leading to ecologically invalid findings even for Q3. New data presented here also suggest that many of these stimuli are not perceived as genuinely emotional and may appear unnatural. We review evidence demonstrating that stimuli that are naturally- or spontaneously-elicited and/or appear genuinely emotional can produce different findings than traditional posed stimuli. Fortunately, naturalistic and spontaneous expression stimuli are now readily available for the field to move forward. We conclude with seven recommendations for advancing facial expression research.},
	language = {en},
	urldate = {2025-11-27},
	journal = {Affective Science},
	author = {Dawel, Amy and Krumhuber, Eva G. and Palermo, Romina},
	month = jul,
	year = {2025},
	keywords = {Emotion perception, Facial expression, Posed, Artificial, Genuine, Naturalistic},
	file = {Full Text PDF:/Users/thora/Zotero/storage/C47V3C9K/Dawel et al. - 2025 - Faking It Isn’t Making It Research Needs Spontaneous and Naturalistic Facial Expressions.pdf:application/pdf},
}

@article{sneddon_belfast_2012,
	title = {The {Belfast} {Induced} {Natural} {Emotion} {Database}},
	volume = {3},
	issn = {1949-3045},
	url = {https://ieeexplore.ieee.org/abstract/document/5975142},
	doi = {10.1109/T-AFFC.2011.26},
	abstract = {For many years psychological research on facial expression of emotion has relied heavily on a recognition paradigm based on posed static photographs. There is growing evidence that there may be fundamental differences between the expressions depicted in such stimuli and the emotional expressions present in everyday life. Affective computing, with its pragmatic emphasis on realism, needs examples of natural emotion. This paper describes a unique database containing recordings of mild to moderate emotionally colored responses to a series of laboratory-based emotion induction tasks. The recordings are accompanied by information on self-report of emotion and intensity, continuous trace-style ratings of valence and intensity, the sex of the participant, the sex of the experimenter, the active or passive nature of the induction task, and it gives researchers the opportunity to compare expressions from people from more than one culture.},
	number = {1},
	urldate = {2025-11-27},
	journal = {IEEE Transactions on Affective Computing},
	author = {Sneddon, Ian and McRorie, Margaret and McKeown, Gary and Hanratty, Jennifer},
	month = jan,
	year = {2012},
	keywords = {Context, Emotion recognition, Databases, affective annotation., database, Decoding, emotion induction, emotional corpora, Encoding, IEEE Transactions on Affective Computing, Natural emotion, Wires},
	pages = {32--41},
	file = {Full Text:/Users/thora/Zotero/storage/77I5RAQD/Sneddon et al. - 2012 - The Belfast Induced Natural Emotion Database.pdf:application/pdf},
}

@article{tottenham_nimstim_2009,
	title = {The {NimStim} set of facial expressions: {Judgments} from untrained research participants},
	volume = {168},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01651781},
	shorttitle = {The {NimStim} set of facial expressions},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165178108001480},
	doi = {10.1016/j.psychres.2008.05.006},
	language = {en},
	number = {3},
	urldate = {2025-11-27},
	journal = {Psychiatry Research},
	author = {Tottenham, Nim and Tanaka, James W. and Leon, Andrew C. and McCarry, Thomas and Nurse, Marcella and Hare, Todd A. and Marcus, David J. and Westerlund, Alissa and Casey, Bj and Nelson, Charles},
	month = aug,
	year = {2009},
	pages = {242--249},
	file = {PDF:/Users/thora/Zotero/storage/P8ICDCWC/Tottenham et al. - 2009 - The NimStim set of facial expressions Judgments from untrained research participants.pdf:application/pdf},
}

@article{dawel_perceived_2017,
	title = {Perceived emotion genuineness: normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets},
	volume = {49},
	issn = {1554-3528},
	shorttitle = {Perceived emotion genuineness},
	url = {https://doi.org/10.3758/s13428-016-0813-2},
	doi = {10.3758/s13428-016-0813-2},
	abstract = {In everyday social interactions, people’s facial expressions sometimes reflect genuine emotion (e.g., anger in response to a misbehaving child) and sometimes do not (e.g., smiling for a school photo). There is increasing theoretical interest in this distinction, but little is known about perceived emotion genuineness for existing facial expression databases. We present a new method for rating perceived genuineness using a neutral-midpoint scale (–7 = completely fake; 0 = don’t know; +7 = completely genuine) that, unlike previous methods, provides data on both relative and absolute perceptions. Normative ratings from typically developing adults for five emotions (anger, disgust, fear, sadness, and happiness) provide three key contributions. First, the widely used Pictures of Facial Affect (PoFA; i.e., “the Ekman faces”) and the Radboud Faces Database (RaFD) are typically perceived as not showing genuine emotion. Also, in the only published set for which the actual emotional states of the displayers are known (via self-report; the McLellan faces), percepts of emotion genuineness often do not match actual emotion genuineness. Second, we provide genuine/fake norms for 558 faces from several sources (PoFA, RaFD, KDEF, Gur, FacePlace, McLellan, News media), including a list of 143 stimuli that are event-elicited (rather than posed) and, congruently, perceived as reflecting genuine emotion. Third, using the norms we develop sets of perceived-as-genuine (from event-elicited sources) and perceived-as-fake (from posed sources) stimuli, matched on sex, viewpoint, eye-gaze direction, and rated intensity. We also outline the many types of research questions that these norms and stimulus sets could be used to answer.},
	language = {en},
	number = {4},
	urldate = {2025-11-28},
	journal = {Behavior Research Methods},
	author = {Dawel, Amy and Wright, Luke and Irons, Jessica and Dumbleton, Rachael and Palermo, Romina and O’Kearney, Richard and McKone, Elinor},
	month = aug,
	year = {2017},
	keywords = {Emotion, Norms, Face expression, Genuine, Authentic, Duchenne},
	pages = {1539--1562},
	file = {Full Text PDF:/Users/thora/Zotero/storage/RCWUSBCK/Dawel et al. - 2017 - Perceived emotion genuineness normative ratings for popular facial expression stimuli and the devel.pdf:application/pdf},
}

@article{miolla_padova_2023,
	title = {Padova {Emotional} {Dataset} of {Facial} {Expressions} ({PEDFE}): {A} unique dataset of genuine and posed emotional facial expressions},
	volume = {55},
	issn = {1554-3528},
	shorttitle = {Padova {Emotional} {Dataset} of {Facial} {Expressions} ({PEDFE})},
	url = {https://doi.org/10.3758/s13428-022-01914-4},
	doi = {10.3758/s13428-022-01914-4},
	abstract = {Facial expressions are among the most powerful signals for human beings to convey their emotional states. Indeed, emotional facial datasets represent the most effective and controlled method of examining humans’ interpretation of and reaction to various emotions. However, scientific research on emotion mainly relied on static pictures of facial expressions posed (i.e., simulated) by actors, creating a significant bias in emotion literature. This dataset tries to fill this gap, providing a considerable amount (N = 1458) of dynamic genuine (N = 707) and posed (N = 751) clips of the six universal emotions from 56 participants. The dataset is available in two versions: original clips, including participants’ body and background, and modified clips, where only the face of participants is visible. Notably, the original dataset has been validated by 122 human raters, while the modified dataset has been validated by 280 human raters. Hit rates for emotion and genuineness, as well as the mean, standard deviation of genuineness, and intensity perception, are provided for each clip to allow future users to select the most appropriate clips needed to answer their scientific questions.},
	language = {en},
	number = {5},
	urldate = {2025-11-28},
	journal = {Behavior Research Methods},
	author = {Miolla, A. and Cardaioli, M. and Scarpazza, C.},
	month = aug,
	year = {2023},
	keywords = {Facial expressions, Emotion dataset, Genuine emotions, Posed emotions},
	pages = {2559--2574},
	file = {Full Text PDF:/Users/thora/Zotero/storage/9ER56VGF/Miolla et al. - 2023 - Padova Emotional Dataset of Facial Expressions (PEDFE) A unique dataset of genuine and posed emotio.pdf:application/pdf},
}

@article{ebner_facesdatabase_2010,
	title = {{FACES}—{A} database of facial expressions in young, middle-aged, and older women and men: {Development} and validation},
	volume = {42},
	issn = {1554-3528},
	shorttitle = {{FACES}—{A} database of facial expressions in young, middle-aged, and older women and men},
	url = {https://doi.org/10.3758/BRM.42.1.351},
	doi = {10.3758/BRM.42.1.351},
	abstract = {Faces are widely used as stimuli in various research fields. Interest in emotion-related differences and age-associated changes in the processing of faces is growing. With the aim of systematically varying both expression and age of the face, we created FACES, a database comprising N=171 naturalistic faces of young, middle-aged, and older women and men. Each face is represented with two sets of six facial expressions (neutrality, sadness, disgust, fear, anger, and happiness), resulting in 2,052 individual images. A total of N=154 young, middleaged, and older women and men rated the faces in terms of facial expression and perceived age. With its large age range of faces displaying different expressions, FACES is well suited for investigating developmental and other research questions on emotion, motivation, and cognition, as well as their interactions. Information on using FACES for research purposes can be found at http://faces.mpib-berlin.mpg.de.},
	language = {en},
	number = {1},
	urldate = {2025-11-28},
	journal = {Behavior Research Methods},
	author = {Ebner, Natalie C. and Riediger, Michaela and Lindenberger, Ulman},
	month = feb,
	year = {2010},
	keywords = {Facial Expression, Face Model, Facial Expres, International Affective Picture System, International Affective Picture System Picture},
	pages = {351--362},
	file = {Full Text PDF:/Users/thora/Zotero/storage/VCG7FZKR/Ebner et al. - 2010 - FACES—A database of facial expressions in young, middle-aged, and older women and men Development a.pdf:application/pdf},
}

@article{van_der_schalk_moving_2011,
	title = {Moving faces, looking places: {Validation} of the {Amsterdam} {Dynamic} {Facial} {Expression} {Set} ({ADFES})},
	volume = {11},
	issn = {1931-1516},
	shorttitle = {Moving faces, looking places},
	doi = {10.1037/a0023853},
	abstract = {We report two studies validating a new standardized set of filmed emotion expressions, the Amsterdam Dynamic Facial Expression Set (ADFES). The ADFES is distinct from existing datasets in that it includes a face-forward version and two different head-turning versions (faces turning toward and away from viewers), North-European as well as Mediterranean models (male and female), and nine discrete emotions (joy, anger, fear, sadness, surprise, disgust, contempt, pride, and embarrassment). Study 1 showed that the ADFES received excellent recognition scores. Recognition was affected by social categorization of the model: displays of North-European models were better recognized by Dutch participants, suggesting an ingroup advantage. Head-turning did not affect recognition accuracy. Study 2 showed that participants more strongly perceived themselves to be the cause of the other's emotion when the model's face turned toward the respondents. The ADFES provides new avenues for research on emotion expression and is available for researchers upon request. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
	number = {4},
	journal = {Emotion},
	author = {van der Schalk, Job and Hawk, Skyler T. and Fischer, Agneta H. and Doosje, Bertjan},
	year = {2011},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Racial and Ethnic Differences, Racial and Ethnic Groups, Facial Expressions, Emotions, Ingroup Outgroup, Emotional Content, Pictorial Stimuli},
	pages = {907--920},
	file = {Snapshot:/Users/thora/Zotero/storage/C96BFCL2/2011-18271-006.html:text/html},
}

@article{burton_glasgow_2010,
	title = {The {Glasgow} {Face} {Matching} {Test}},
	volume = {42},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/BRM.42.1.286},
	doi = {10.3758/BRM.42.1.286},
	abstract = {We describe a new test for unfamiliar face matching, the Glasgow Face Matching Test (GFMT). Viewers are shown pairs of faces, photographed in full-face view but with different cameras, and are asked to make same/different judgments. The full version of the test comprises 168 face pairs, and we also describe a shortened version with 40 pairs. We provide normative data for these tests derived from large subject samples. We also describe associations between the GFMT and other tests of matching and memory. The new test correlates moderately with face memory but more strongly with object matching, a result that is consistent with previous research highlighting a link between object and face matching, specific to unfamiliar faces. The test is available free for scientific use.},
	language = {en},
	number = {1},
	urldate = {2025-11-28},
	journal = {Behavior Research Methods},
	author = {Burton, A. Mike and White, David and McNeill, Allan},
	month = feb,
	year = {2010},
	keywords = {Face Recognition, Eyewitness Identification, Face Match, Face Pair, Recognition Memory},
	pages = {286--291},
	file = {Full Text PDF:/Users/thora/Zotero/storage/TUVJ9YSA/Burton et al. - 2010 - The Glasgow Face Matching Test.pdf:application/pdf},
}

@article{langner_presentation_2010,
	title = {Presentation and validation of the {Radboud} {Faces} {Database}},
	volume = {24},
	issn = {0269-9931},
	url = {https://doi.org/10.1080/02699930903485076},
	doi = {10.1080/02699930903485076},
	abstract = {Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.},
	number = {8},
	urldate = {2025-11-28},
	journal = {Cognition and Emotion},
	author = {Langner, Oliver and Dotsch, Ron and Bijlstra, Gijsbert and Wigboldus, Daniel H. J. and Hawk, Skyler T. and van Knippenberg, Ad},
	month = dec,
	year = {2010},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/02699930903485076},
	keywords = {Emotion, Face database, Gaze direction, Validation},
	pages = {1377--1388},
	file = {Full Text PDF:/Users/thora/Zotero/storage/LYBZX77E/Langner et al. - 2010 - Presentation and validation of the Radboud Faces Database.pdf:application/pdf},
}

@article{yarkoni_generalizability_2022,
	title = {The generalizability crisis},
	volume = {45},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalizability-crisis/AD386115BA539A759ACB3093760F4824},
	doi = {10.1017/S0140525X20001685},
	abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned – that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology – the linear mixed model – I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the “random effect” formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
	language = {en},
	urldate = {2025-11-28},
	journal = {Behavioral and Brain Sciences},
	author = {Yarkoni, Tal},
	month = jan,
	year = {2022},
	keywords = {Generalization, inference, philosophy of science, psychology, random effects, statistics},
	pages = {e1},
}

@misc{debruine_reproducible_2022,
	title = {Reproducible {Methods} for {Face} {Research}},
	url = {https://osf.io/preprints/psyarxiv/j2754_v1/},
	doi = {10.31234/osf.io/j2754},
	abstract = {Face stimuli are commonly created in ways that are not explained well enough for others to reproduce them. In this paper, we document the irreproducibility of most face stimuli, explain the benefits of reproducible stimuli, and introduce the open-source R package webmorphR that facilitates scriptable face image processing. We explain the technical processes of morphing and transforming through a case study of creating face stimuli from an open-access image set. Finally, we discuss some ethical and methodological issues around the use of face images in research that may be ameliorated through the use of reproducible stimuli.},
	urldate = {2025-11-28},
	publisher = {PsyArXiv},
	author = {DeBruine, Lisa M and Holzleitner, Iris J and Tiddeman, Bernard and Jones, Benedict C},
	month = oct,
	year = {2022},
	keywords = {reproducibility, ERC\_KINSHIP\_647910, KINSHIP, methods, psychomorph, webmorph},
	file = {Preprint PDF:/Users/thora/Zotero/storage/KR5GHXFH/DeBruine et al. - 2022 - Reproducible Methods for Face Research.pdf:application/pdf},
}

@misc{trebicky_grim_2024,
	title = {A {Grim} {Image}: {Considerations} for {Methods} of portrait photography in psychological science},
	shorttitle = {A {Grim} {Image}},
	url = {https://osf.io/z4svb_v1/},
	doi = {10.31219/osf.io/z4svb},
	abstract = {Photographs of people play a central role in psychological research across various fields, yet methodological inconsistencies in how these images are captured and reported may hinder replication and comparability. Despite growing attention to the replication crisis in psychology, vagueness and insufficient detail in methodological reporting, particularly regarding photographic practices, remain an overlooked issue. Drawing an analogy between research methods and culinary recipes, we argue that the quality of a method depends on clear, comprehensive instructions that enable repeatability and reproducibility. We selected three basic key aspects of photography—focal length, lighting, and background colour— to show how varying them can substantially influence image outcomes and, consequently, research findings. Using examples, we illustrate how these variables impact the appearance of photographic stimuli and discuss their potential effects on study results, such as perceived traits, recognition accuracy, and morphological measurements. To address this gap, we propose minimal reporting standards for photographic methods in face research, including detailed descriptions of equipment, settings, and setup, along with visual examples. By adopting these standards, researchers can improve the clarity and transparency of their methods, facilitating robust and replicable research. While we focus on photography, this initiative underscores the broader need for detailed methodological reporting across disciplines. Ultimately, our goal is to inspire a culture of methodological rigour, fostering confidence in the replicability of findings and advancing the field of psychological science.},
	urldate = {2025-11-28},
	publisher = {OSF Preprints},
	author = {Třebický, Vít and Třebická Fialová, Jitka and Bjornsdottir, R. Thora and DeBruine, Lisa M},
	month = nov,
	year = {2024},
	file = {Preprint PDF:/Users/thora/Zotero/storage/YC3ETBT2/Třebický et al. - 2024 - A Grim Image Considerations for Methods of portrait photography in psychological science.pdf:application/pdf},
}

@article{henrich_weirdest_2010,
	title = {The weirdest people in the world?},
	volume = {33},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/weirdest-people-in-the-world/BF84F7517D56AFF7B7EB58411A554C17},
	doi = {10.1017/S0140525X0999152X},
	abstract = {Behavioral scientists routinely publish broad claims about human psychology and behavior in the world's top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers – often implicitly – assume that either there is little variation across human populations, or that these “standard subjects” are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species – frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior – hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.},
	language = {en},
	number = {2-3},
	urldate = {2025-11-28},
	journal = {Behavioral and Brain Sciences},
	author = {Henrich, Joseph and Heine, Steven J. and Norenzayan, Ara},
	month = jun,
	year = {2010},
	keywords = {cultural psychology, culture, behavioral economics, cross-cultural research, evolutionary psychology, experiments, external validity, generalizability, human universals, population variability},
	pages = {61--83},
	file = {Full Text PDF:/Users/thora/Zotero/storage/MXJ2YMIX/Henrich et al. - 2010 - The weirdest people in the world.pdf:application/pdf},
}

@article{saribay_bogazici_2018,
	title = {The {Bogazici} face database: {Standardized} photographs of {Turkish} faces with supporting materials},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {The {Bogazici} face database},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192018},
	doi = {10.1371/journal.pone.0192018},
	abstract = {Many sets of human facial photographs produced in Western cultures are available for scientific research. We report here on the development of a face database of Turkish undergraduate student targets. High-resolution standardized photographs were taken and supported by the following materials: (a) basic demographic and appearance-related information, (b) two types of landmark configurations (for Webmorph and geometric morphometrics (GM)), (c) facial width-to-height ratio (fWHR) measurement, (d) information on photography parameters, (e) perceptual norms provided by raters. We also provide various analyses and visualizations of facial variation based on rating norms using GM. Finally, we found that there is sexual dimorphism in fWHR in our sample but that this is accounted for by body mass index. We present the pattern of associations between rating norms, GM and fWHR measurements. The database and supporting materials are freely available for scientific research purposes.},
	language = {en},
	number = {2},
	urldate = {2025-11-28},
	journal = {PLOS ONE},
	author = {Saribay, S. Adil and Biten, Ali Furkan and Meral, Erdem Ozan and Aldan, Pinar and Třebický, Vít and Kleisner, Karel},
	month = feb,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Face, Face recognition, Personality, Population genetics, Body mass index, Morphometry, Undergraduates, Vision},
	pages = {e0192018},
	file = {Full Text PDF:/Users/thora/Zotero/storage/EYG6BQ9K/Saribay et al. - 2018 - The Bogazici face database Standardized photographs of Turkish faces with supporting materials.pdf:application/pdf},
}

@article{hehman_assessing_2025,
	title = {Assessing the {Point} at {Which} {Averages} {Are} {Stable}: {A} {Tutorial} in the {Context} of {Impression} {Formation}},
	volume = {43},
	shorttitle = {Assessing the {Point} at {Which} {Averages} {Are} {Stable}},
	url = {https://doi.org/10.1521/soco.2025.43.5.488},
	doi = {https://doi.org/10.1521/soco.2025.43.5.488},
	abstract = {Across many diverse areas of research, it is common to average a series of observations, and to use these averages in subsequent analyses. Research using this approach faces the challenge of knowing when these averages are “stable.” Meaning, to what extent do averages change when additional observations are included? Using averages that are not stable introduces error into any analysis, and knowing the point of stability can inform research design. The current research develops a tool, implemented in R, to assess when averages are stable. Using a sequential sampling approach, it determines how many observations are needed before additional observations would no longer meaningfully change an average. We illustrate how to use this tool with data from the impression formation literature, demonstrating that averages of some perceived traits (e.g., happy) stabilize with fewer observations than others (e.g., assertive). This tutorial provides step-by-step instructions for implementation in researchers’ own data.},
	language = {en},
	number = {5},
	urldate = {2025-11-28},
	journal = {Social Cognition},
	author = {Hehman, Eric and Xie, Sally Y. and Ofosu, Eugene K. and Nespoli, Gabriel A.},
	year = {2025},
	doi = {10.1521/soco.2025.43.5.488},
	note = {Archive Location: world
Publisher: Guilford},
	pages = {488--501},
	file = {Snapshot:/Users/thora/Zotero/storage/QQGPEFN7/soco.2025.43.5.html:text/html},
}

@article{young_recognizing_2017,
	title = {Recognizing {Faces}},
	volume = {26},
	issn = {0963-7214},
	url = {https://doi.org/10.1177/0963721416688114},
	doi = {10.1177/0963721416688114},
	abstract = {The idea that most of us are good at recognizing faces permeates everyday thinking and is widely used in the research literature. However, it is a correct characterization only of familiar-face recognition. In contrast, the perception and recognition of unfamiliar faces can be surprisingly error-prone, and this has important consequences in many real-life settings. We emphasize the variability in views of faces encountered in everyday life and point out how neglect of this important property has generated some misleading conclusions. Many approaches have treated image variability as unwanted noise, whereas we show how studies that use and explore the implications of image variability can drive substantial theoretical advances.},
	language = {EN},
	number = {3},
	urldate = {2025-11-28},
	journal = {Current Directions in Psychological Science},
	author = {Young, Andrew W. and Burton, A. Mike},
	month = jun,
	year = {2017},
	note = {Publisher: SAGE Publications Inc},
	pages = {212--217},
	file = {SAGE PDF Full Text:/Users/thora/Zotero/storage/4GZ5GMAT/Young and Burton - 2017 - Recognizing Faces.pdf:application/pdf},
}

@misc{debruine_experimentum_2020,
	title = {Experimentum},
	shorttitle = {Experimentum},
	url = {https://zenodo.org/records/4010579},
	abstract = {Web-based software for building, deploying, and managing psychology studies.},
	urldate = {2025-12-03},
	publisher = {Zenodo},
	author = {DeBruine, Lisa M and Lai, Rebecca and Jones, Benedict C and Abdullah, Rifah and Mahrholz, Gaby},
	month = sep,
	year = {2020},
	doi = {10.5281/zenodo.4010579},
	file = {Snapshot:/Users/thora/Zotero/storage/66Z47L2A/4010579.html:text/html},
}
