<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
    <meta charset="utf-8">
    <meta name="generator" content="quarto-1.7.32">

    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


    <title>Capturing Many Faces</title>
    <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.columns{display: flex; gap: min(4vw, 1.5em);}
      div.column{flex: auto; overflow-x: auto;}
      div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
      ul.task-list{list-style: none;}
      ul.task-list li input[type="checkbox"] {
        width: 0.8em;
        margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
        vertical-align: middle;
      }
      /* CSS for syntax highlighting */
      html { -webkit-text-size-adjust: 100%; }
      pre > code.sourceCode { white-space: pre; position: relative; }
      pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
      pre > code.sourceCode > span:empty { height: 1.2em; }
      .sourceCode { overflow: visible; }
      code.sourceCode > span { color: inherit; text-decoration: inherit; }
      div.sourceCode { margin: 1em 0; }
      pre.sourceCode { margin: 0; }
      @media screen {
      div.sourceCode { overflow: auto; }
      }
      @media print {
      pre > code.sourceCode { white-space: pre-wrap; }
      pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
      }
      pre.numberSource code
        { counter-reset: source-line 0; }
      pre.numberSource code > span
        { position: relative; left: -4em; counter-increment: source-line; }
      pre.numberSource code > span > a:first-child::before
        { content: counter(source-line);
          position: relative; left: -1em; text-align: right; vertical-align: baseline;
          border: none; display: inline-block;
          -webkit-touch-callout: none; -webkit-user-select: none;
          -khtml-user-select: none; -moz-user-select: none;
          -ms-user-select: none; user-select: none;
          padding: 0 4px; width: 4em;
        }
      pre.numberSource { margin-left: 3em;  padding-left: 4px; }
      div.sourceCode
        {   }
      @media screen {
      pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
      }
      /* CSS for citations */
      div.csl-bib-body { }
      div.csl-entry {
        clear: both;
        margin-bottom: 0em;
      }
      .hanging-indent div.csl-entry {
        margin-left:2em;
        text-indent:-2em;
      }
      div.csl-left-margin {
        min-width:2em;
        float:left;
      }
      div.csl-right-inline {
        margin-left:2em;
        padding-left:1em;
      }
      div.csl-indent {
        margin-left: 2em;
      }    </style>

    <style>
      body.hypothesis-enabled #quarto-embed-header {
        padding-right: 36px;
      }

      #quarto-embed-header {
        height: 3em;
        width: 100%;
        display: flex;
        justify-content: space-between;
        align-items: center;
        border-bottom: solid 1px;
      }

      #quarto-embed-header h6 {
        font-size: 1.1em;
        padding-top: 0.6em;
        margin-left: 1em;
        margin-right: 1em;
        font-weight: 400;
      }

      #quarto-embed-header a.quarto-back-link,
      #quarto-embed-header a.quarto-download-embed {
        font-size: 0.8em;
        margin-top: 1em;
        margin-bottom: 1em;
        margin-left: 1em;
        margin-right: 1em;
      }

      .quarto-back-container {
        padding-left: 0.5em;
        display: flex;
      }

      .headroom {
          will-change: transform;
          transition: transform 200ms linear;
      }

      .headroom--pinned {
          transform: translateY(0%);
      }

      .headroom--unpinned {
          transform: translateY(-100%);
      }      
    </style>

    <script>
    window.document.addEventListener("DOMContentLoaded", function () {

      var header = window.document.querySelector("#quarto-embed-header");
      const titleBannerEl = window.document.querySelector("body > #title-block-header");
      if (titleBannerEl) {
        titleBannerEl.style.paddingTop = header.clientHeight + "px";
      }
      const contentEl = window.document.getElementById('quarto-content');
      for (const child of contentEl.children) {
        child.style.paddingTop = header.clientHeight + "px";
        child.style.marginTop = "1em";
      }

      // Use the article root if the `back` call doesn't work. This isn't perfect
      // but should typically work
      window.quartoBackToArticle = () => {
        var currentUrl = window.location.href;
        window.history.back();
        setTimeout(() => {
            // if location was not changed in 100 ms, then there is no history back
            if(currentUrl === window.location.href){              
                // redirect to site root
                window.location.href = "index.html";
            }
        }, 100);
      }

      const headroom = new window.Headroom(header, {
        tolerance: 5,
        onPin: function () {
        },
        onUnpin: function () {
        },
      });
      headroom.init();
    });
    </script>

    
<script src="site_libs/manuscript-notebook/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8a894fcc3cb5e37eb5bf30def131be2f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
     <script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>  <link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>  
      <meta name="citation_title" content="Capturing Many Faces">
<meta name="citation_author" content="R. Thora Bjornsdottir">
<meta name="citation_author" content="Vít Třebický">
<meta name="citation_author" content="Lisa DeBruine">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=ImageMagick;,citation_author=ImageMagick Studio LLC;,citation_publication_date=2024-01-04;,citation_cover_date=2024-01-04;,citation_year=2024;,citation_fulltext_html_url=https://imagemagick.org;">
<meta name="citation_reference" content="citation_title=Why is the literature on first impressions so focused on White faces?;,citation_abstract=We spontaneously attribute to strangers a wide variety of character traits based on their facial appearance. While these first impressions have little or no basis in reality, they exert a strong influence over our behaviour. Cognitive scientists have revealed a great deal about first impressions from faces including their factor structure, the cues on which they are based, the neurocognitive mechanisms responsible, and their developmental trajectory. In this field, authors frequently strive to remove as much ethnic variability from stimulus sets as possible. Typically, this convention means that participants are asked to judge the likely traits of White faces only. In the present article, we consider four possible reasons for the lack of facial diversity in this literature and find that it is unjustified. Next, we illustrate how the focus on White faces has undermined scientific efforts to understand first impressions from faces and argue that it reinforces socially regressive ideas about “race” and status. We go on to articulate our concern that opportunities may be lost to leverage the knowledge derived from the study of first impressions against the dire consequences of prejudice and discrimination. Finally, we highlight some promising developments in the field.;,citation_author=Richard Cook;,citation_author=Harriet Over;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;,citation_fulltext_html_url=https://royalsocietypublishing.org/doi/10.1098/rsos.211146;,citation_issue=9;,citation_doi=10.1098/rsos.211146;,citation_issn=2054-5703;,citation_volume=8;,citation_journal_title=Royal Society Open Science;">
<meta name="citation_reference" content="citation_title=To which world regions does the valence–dominance model of social perception apply?;,citation_abstract=Over the past 10 years, Oosterhof and Todorov’s valence–dominance model has emerged as the most prominent account of how people evaluate faces on social dimensions. In this model, two dimensions (valence and dominance) underpin social judgements of faces. Because this model has primarily been developed and tested in Western regions, it is unclear whether these findings apply to other regions. We addressed this question by replicating Oosterhof and Todorov’s methodology across 11 world regions, 41 countries and 11,570 participants. When we used Oosterhof and Todorov’s original analysis strategy, the valence–dominance model generalized across regions. When we used an alternative methodology to allow for correlated dimensions, we observed much less generalization. Collectively, these results suggest that, while the valence–dominance model generalizes very well across regions when dimensions are forced to be orthogonal, regional differences are revealed when we use different extraction methods and correlate and rotate the dimension reduction solution.;,citation_author=Benedict C. Jones;,citation_author=Lisa M. DeBruine;,citation_author=Jessica K. Flake;,citation_author=Marco Tullio Liuzza;,citation_author=Jan Antfolk;,citation_author=Nwadiogo C. Arinze;,citation_author=Izuchukwu L. G. Ndukaihe;,citation_author=Nicholas G. Bloxsom;,citation_author=Savannah C. Lewis;,citation_author=Francesco Foroni;,citation_author=Megan L. Willis;,citation_author=Carmelo P. Cubillas;,citation_author=Miguel A. Vadillo;,citation_author=Enrique Turiegano;,citation_author=Michael Gilead;,citation_author=Almog Simchon;,citation_author=S. Adil Saribay;,citation_author=Nicholas C. Owsley;,citation_author=Chaning Jang;,citation_author=Georgina Mburu;,citation_author=Dustin P. Calvillo;,citation_author=Anna Wlodarczyk;,citation_author=Yue Qi;,citation_author=Kris Ariyabuddhiphongs;,citation_author=Somboon Jarukasemthawee;,citation_author=Harry Manley;,citation_author=Panita Suavansri;,citation_author=Nattasuda Taephant;,citation_author=Ryan M. Stolier;,citation_author=Thomas R. Evans;,citation_author=Judson Bonick;,citation_author=Jan W. Lindemans;,citation_author=Logan F. Ashworth;,citation_author=Amanda C. Hahn;,citation_author=Coralie Chevallier;,citation_author=Aycan Kapucu;,citation_author=Aslan Karaaslan;,citation_author=Juan David Leongómez;,citation_author=Oscar R. Sánchez;,citation_author=Eugenio Valderrama;,citation_author=Milena Vásquez-Amézquita;,citation_author=Nandor Hajdu;,citation_author=Balazs Aczel;,citation_author=Peter Szecsi;,citation_author=Michael Andreychik;,citation_author=Erica D. Musser;,citation_author=Carlota Batres;,citation_author=Chuan-Peng Hu;,citation_author=Qing-Lan Liu;,citation_author=Nicole Legate;,citation_author=Leigh Ann Vaughn;,citation_author=Krystian Barzykowski;,citation_author=Karolina Golik;,citation_author=Irina Schmid;,citation_author=Stefan Stieger;,citation_author=Richard Artner;,citation_author=Chiel Mues;,citation_author=Wolf Vanpaemel;,citation_author=Zhongqing Jiang;,citation_author=Qi Wu;,citation_author=Gabriela M. Marcu;,citation_author=Ian D. Stephen;,citation_author=Jackson G. Lu;,citation_author=Michael C. Philipp;,citation_author=Jack D. Arnal;,citation_author=Eric Hehman;,citation_author=Sally Y. Xie;,citation_author=William J. Chopik;,citation_author=Martin Seehuus;,citation_author=Soufian Azouaghe;,citation_author=Abdelkarim Belhaj;,citation_author=Jamal Elouafa;,citation_author=John P. Wilson;,citation_author=Elliott Kruse;,citation_author=Marietta Papadatou-Pastou;,citation_author=Anabel De La Rosa-Gómez;,citation_author=Alan E. Barba-Sánchez;,citation_author=Isaac González-Santoyo;,citation_author=Tsuyueh Hsu;,citation_author=Chun-Chia Kung;,citation_author=Hsiao-Hsin Wang;,citation_author=Jonathan B. Freeman;,citation_author=Dong Won Oh;,citation_author=Vidar Schei;,citation_author=Therese E. Sverdrup;,citation_author=Carmel A. Levitan;,citation_author=Corey L. Cook;,citation_author=Priyanka Chandel;,citation_author=Pratibha Kujur;,citation_author=Arti Parganiha;,citation_author=Noorshama Parveen;,citation_author=Atanu Kumar Pati;,citation_author=Sraddha Pradhan;,citation_author=Margaret M. Singh;,citation_author=Babita Pande;,citation_author=Jozef Bavolar;,citation_author=Pavol Kačmár;,citation_author=Ilya Zakharov;,citation_author=Sara Álvarez-Solas;,citation_author=Ernest Baskin;,citation_author=Martin Thirkettle;,citation_author=Kathleen Schmidt;,citation_author=Cody D. Christopherson;,citation_author=Trinity Leonis;,citation_author=Jordan W. Suchow;,citation_author=Jonas K. Olofsson;,citation_author=Teodor Jernsäther;,citation_author=Ai-Suan Lee;,citation_author=Jennifer L. Beaudry;,citation_author=Taylor D. Gogan;,citation_author=Julian A. Oldmeadow;,citation_author=Benjamin Balas;,citation_author=Laura M. Stevens;,citation_author=Melissa F. Colloff;,citation_author=Heather D. Flowe;,citation_author=Sami Gülgöz;,citation_author=Mark J. Brandt;,citation_author=Karlijn Hoyer;,citation_author=Bastian Jaeger;,citation_author=Dongning Ren;,citation_author=Willem W. A. Sleegers;,citation_author=Joeri Wissink;,citation_author=Gwenaël Kaminski;,citation_author=Victoria A. Floerke;,citation_author=Heather L. Urry;,citation_author=Sau-Chin Chen;,citation_author=Gerit Pfuhl;,citation_author=Zahir Vally;,citation_author=Dana M. Basnight-Brown;,citation_author=Hans I. Jzerman;,citation_author=Elisa Sarda;,citation_author=Lison Neyroud;,citation_author=Touhami Badidi;,citation_author=Nicolas Van der Linden;,citation_author=Chrystalle B. Y. Tan;,citation_author=Vanja Kovic;,citation_author=Waldir Sampaio;,citation_author=Paulo Ferreira;,citation_author=Diana Santos;,citation_author=Debora I. Burin;,citation_author=Gwendolyn Gardiner;,citation_author=John Protzko;,citation_author=Christoph Schild;,citation_author=Karolina A. Ścigała;,citation_author=Ingo Zettler;,citation_author=Erin M. O’Mara Kunz;,citation_author=Daniel Storage;,citation_author=Fieke M. A. Wagemans;,citation_author=Blair Saunders;,citation_author=Miroslav Sirota;,citation_author=Guyan V. Sloane;,citation_author=Tiago J. S. Lima;,citation_author=Kim Uittenhove;,citation_author=Evie Vergauwe;,citation_author=Katarzyna Jaworska;,citation_author=Julia Stern;,citation_author=Karl Ask;,citation_author=Casper J. J. Zyl;,citation_author=Anita Körner;,citation_author=Sophia C. Weissgerber;,citation_author=Jordane Boudesseul;,citation_author=Fernando Ruiz-Dodobara;,citation_author=Kay L. Ritchie;,citation_author=Nicholas M. Michalak;,citation_author=Khandis R. Blake;,citation_author=David White;,citation_author=Alasdair R. Gordon-Finlayson;,citation_author=Michele Anne;,citation_author=Steve M. J. Janssen;,citation_author=Kean Mun Lee;,citation_author=Tonje K. Nielsen;,citation_author=Christian K. Tamnes;,citation_author=Janis H. Zickfeld;,citation_author=Anna Dalla Rosa;,citation_author=Michelangelo Vianello;,citation_author=Ferenc Kocsor;,citation_author=Luca Kozma;,citation_author=Ádám Putz;,citation_author=Patrizio Tressoldi;,citation_author=Natalia Irrazabal;,citation_author=Armand Chatard;,citation_author=Samuel Lins;,citation_author=Isabel R. Pinto;,citation_author=Johannes Lutz;,citation_author=Matus Adamkovic;,citation_author=Peter Babincak;,citation_author=Gabriel Baník;,citation_author=Ivan Ropovik;,citation_author=Vinet Coetzee;,citation_author=Barnaby J. W. Dixson;,citation_author=Gianni Ribeiro;,citation_author=Kim Peters;,citation_author=Niklas K. Steffens;,citation_author=Kok Wei Tan;,citation_author=Christopher A. Thorstenson;,citation_author=Ana Maria Fernandez;,citation_author=Rafael M. C. S. Hsu;,citation_author=Jaroslava V. Valentova;,citation_author=Marco A. C. Varella;,citation_author=Nadia S. Corral-Frías;,citation_author=Martha Frías-Armenta;,citation_author=Javad Hatami;,citation_author=Arash Monajem;,citation_author=MohammadHasan Sharifian;,citation_author=Brooke Frohlich;,citation_author=Hause Lin;,citation_author=Michael Inzlicht;,citation_author=Ravin Alaei;,citation_author=Nicholas O. Rule;,citation_author=Claus Lamm;,citation_author=Ekaterina Pronizius;,citation_author=Martin Voracek;,citation_author=Jerome Olsen;,citation_author=Erik Mac Giolla;,citation_author=Aysegul Akgoz;,citation_author=Asil A. Özdoğru;,citation_author=Matthew T. Crawford;,citation_author=Brooke Bennett-Day;,citation_author=Monica A. Koehn;,citation_author=Ceylan Okan;,citation_author=Tripat Gill;,citation_author=Jeremy K. Miller;,citation_author=Yarrow Dunham;,citation_author=Xin Yang;,citation_author=Sinan Alper;,citation_author=Martha Lucia Borras-Guevara;,citation_author=Sun Jun Cai;,citation_author=Dong Tiantian;,citation_author=Alexander F. Danvers;,citation_author=David R. Feinberg;,citation_author=Marie M. Armstrong;,citation_author=Eva Gilboa-Schechtman;,citation_author=Randy J. McCarthy;,citation_author=Jose Antonio Muñoz-Reyes;,citation_author=Pablo Polo;,citation_author=Victor K. M. Shiramazu;,citation_author=Wen-Jing Yan;,citation_author=Lilian Carvalho;,citation_author=Patrick S. Forscher;,citation_author=Christopher R. Chartier;,citation_author=Nicholas A. Coles;,citation_publication_date=2021-01;,citation_cover_date=2021-01;,citation_year=2021;,citation_fulltext_html_url=https://www.nature.com/articles/s41562-020-01007-2;,citation_issue=1;,citation_doi=10.1038/s41562-020-01007-2;,citation_issn=2397-3374;,citation_volume=5;,citation_journal_title=Nature Human Behaviour;">
<meta name="citation_reference" content="citation_title=The Face Image Meta-Database (fIMDb) &amp;amp;amp; ChatLab Facial Anomaly Database (CFAD): Tools for research on face perception and social stigma;,citation_abstract=Investigators increasingly need high quality face photographs that they can use in service of their scholarly pursuits—whether serving as experimental stimuli or to benchmark face recognition algorithms. Up to now, an index of known face databases, their features, and how to access them has not been available. This absence has had at least two negative repercussions: First, without alternatives, some researchers may have used face databases that are widely known but not optimal for their research. Second, a reliance on databases comprised only of young white faces will lead to science that isn’t representative of all the people whose tax contributions, in many cases, make that research possible. The “Face Image Meta-Database” (fIMDb) provides researchers with the tools to find the face images best suited to their research, with filters to locate databases with people of a varied racial and ethnic backgrounds and ages. Problems of representation in face databases are not restricted to race and ethnicity or age – there is a dearth of databases with faces that have visible differences (e.g., scars, port wine stains, and cleft lip and palate). A well-characterized database is needed to support programmatic research into perceivers’ attitudes, behaviors, and neural responses to anomalous faces. The “ChatLab Facial Anomaly Database” (CFAD) was constructed to fill this gap, with photographs of faces with visible differences of various types, etiologies, sizes, locations, and that depict individuals from various ethnic backgrounds and age groups. Both the fIMDb and CFAD are available from: https://cliffordworkman.com/resources/.;,citation_author=Clifford I. Workman;,citation_author=Anjan Chatterjee;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S2590260121000205;,citation_doi=10.1016/j.metip.2021.100063;,citation_issn=2590-2601;,citation_volume=5;,citation_journal_title=Methods in Psychology;">
<meta name="citation_reference" content="citation_title=Chicago Face Database: Multiracial expansion;,citation_abstract=Multiracial individuals represent a growing segment of the population and have been increasingly the focus of empirical study. Much of this research centers on the perception and racial categorization of multiracial individuals. The current paper reviews some of this research and describes the different types of stimuli that have been used in these paradigms. We describe the strengths and weaknesses associated with different operationalizations of multiracialism and highlight the dearth of research using faces of real multiracial individuals, which we posit may be due to the lack of available stimuli. Our research seeks to satisfy this need by providing a free set of high-resolution, standardized images featuring 88 real multiracial individuals along with extensive norming data and objective physical measures of these faces. These data are offered as an extension of the widely used Chicago Face Database and are available for download at www.chicagofaces.orgfor use in research.;,citation_author=Debbie S. Ma;,citation_author=Justin Kantner;,citation_author=Bernd Wittenbrink;,citation_publication_date=2021-06;,citation_cover_date=2021-06;,citation_year=2021;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-020-01482-5;,citation_issue=3;,citation_doi=10.3758/s13428-020-01482-5;,citation_issn=1554-3528;,citation_volume=53;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=Reading Faces: Window To The Soul?;,citation_abstract=Do we read character in faces? What information do faces actually provide? What are the social and psychological consequences of reading character in faces? Zebrowitz unmasks the face and provides the first systematic, scientific account of our tendency to judge people by their appearance. Offering an in-depth discussion of two appearance qualities that influence our impressions of others—“baby-faceness” and “attractiveness”—and an analysis of these impressions, Zebrowitz has written an accessible and valuable book for professionals and general readers alike.;,citation_author=Leslie Zebrowitz;,citation_publication_date=1997;,citation_cover_date=1997;,citation_year=1997;,citation_isbn=978-0-429-97281-2;">
<meta name="citation_reference" content="citation_title=The Chicago face database: A free stimulus set of faces and norming data;,citation_abstract=Researchers studying a range of psychological phenomena (e.g., theory of mind, emotion, stereotyping and prejudice, interpersonal attraction, etc.) sometimes employ photographs of people as stimuli. In this paper, we introduce the Chicago Face Database, a free resource consisting of 158 high-resolution, standardized photographs of Black and White males and females between the ages of 18 and 40&nbsp;years and extensive data about these targets. In Study 1, we report pre-testing of these faces, which includes both subjective norming data and objective physical measurements of the images included in the database. In Study 2 we surveyed psychology researchers to assess the suitability of these targets for research purposes and explored factors that were associated with researchers’ judgments of suitability. Instructions are outlined for those interested in obtaining access to the stimulus set and accompanying ratings and measures.;,citation_author=Debbie S. Ma;,citation_author=Joshua Correll;,citation_author=Bernd Wittenbrink;,citation_publication_date=2015-12;,citation_cover_date=2015-12;,citation_year=2015;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-014-0532-5;,citation_issue=4;,citation_doi=10.3758/s13428-014-0532-5;,citation_issn=1554-3528;,citation_volume=47;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=The functional basis of face evaluation;,citation_abstract=People automatically evaluate faces on multiple trait dimensions, and these evaluations predict important social outcomes, ranging from electoral success to sentencing decisions. Based on behavioral studies and computer modeling, we develop a 2D model of face evaluation. First, using a principal components analysis of trait judgments of emotionally neutral faces, we identify two orthogonal dimensions, valence and dominance, that are sufficient to describe face evaluation and show that these dimensions can be approximated by judgments of trustworthiness and dominance. Second, using a data-driven statistical model for face representation, we build and validate models for representing face trustworthiness and face dominance. Third, using these models, we show that, whereas valence evaluation is more sensitive to features resembling expressions signaling whether the person should be avoided or approached, dominance evaluation is more sensitive to features signaling physical strength/weakness. Fourth, we show that important social judgments, such as threat, can be reproduced as a function of the two orthogonal dimensions of valence and dominance. The findings suggest that face evaluation involves an overgeneralization of adaptive mechanisms for inferring harmful intentions and the ability to cause harm and can account for rapid, yet not necessarily accurate, judgments from faces.;,citation_author=Nikolaas N. Oosterhof;,citation_author=Alexander Todorov;,citation_publication_date=2008-08;,citation_cover_date=2008-08;,citation_year=2008;,citation_fulltext_html_url=https://www.pnas.org/doi/full/10.1073/pnas.0805664105;,citation_issue=32;,citation_doi=10.1073/pnas.0805664105;,citation_volume=105;,citation_journal_title=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=In Your Face: The new science of human attraction;,citation_abstract=In our daily lives, in our memories and fantasies, our mental worlds overflow with faces. But what do we really know about this most remarkable feature of the human body? Why do we have faces at all, and brains that are good at reading them? What do our looks say – and not say – about our personalities?And perhaps the most compelling question of all: Why are we attracted to some faces more than others? In Your Face is an engaging and authoritative tour of the science of facial beauty and face perception.David Perrett, the pre-eminent scholar in the field, reveals and interprets the most remarkable findings and in the process demolishes many popular myths, setting the record straight on what neuroscience and evolutionary psychology are teaching us about beauty. The record is more surprising and often more unsettling than you might think.;,citation_author=D. I. Perrett;,citation_publication_date=2017-09;,citation_cover_date=2017-09;,citation_year=2017;,citation_isbn=978-0-230-36484-4;">
<meta name="citation_reference" content="citation_title=The India Face Set: International and Cultural Boundaries Impact Face Impressions and Perceptions of Category Membership;,citation_abstract=This paper serves three specific goals. First, it reports the development of an Indian Asian face set, to serve as a free resource for psychological research. Second, it examines whether the use of pre-tested U.S.-specific norms for stimulus selection or weighting may introduce experimental confounds in studies involving non-U.S. face stimuli and/or non-U.S. participants. Specifically, it examines whether subjective impressions of the face stimuli are culturally dependent, and the extent to which these impressions reflect social stereotypes and ingroup favoritism. Third, the paper investigates whether differences in face familiarity impact accuracy in identifying face ethnicity. To this end, face images drawn from volunteers in India as well as a subset of Caucasian face images from the Chicago Face Database were presented to Indian and U.S. participants, and rated on a range of measures, such as perceived attractiveness, warmth, and social status. Results show significant differences in the overall valence of ratings of ingroup and outgroup faces. In addition, the impression ratings show minor differentiation along two basic stereotype dimensions, competence and trustworthiness, but not warmth. We also find participants to show significantly greater accuracy in correctly identifying the ethnicity of ingroup faces, relative to outgroup faces. This effect is found to be mediated by ingroup-outgroup differences in perceived group typicality of the target faces. Implications for research on intergroup relations in a cross-cultural context are discussed.;,citation_author=Anjana Lakshmi;,citation_author=Bernd Wittenbrink;,citation_author=Joshua Correll;,citation_author=Debbie S. Ma;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.frontiersin.org/articles/10.3389/fpsyg.2021.627678;,citation_issn=1664-1078;,citation_volume=12;,citation_journal_title=Frontiers in Psychology;">
<meta name="citation_reference" content="citation_title=Hawai‘i Face Database: A racially and ethnically diverse set of facial stimuli;,citation_abstract=Within psychology, face perception processes have been widely studied, examining traits and social categories that a face can communicate. However, much of this research has predominately focused on White faces. We review existing face databases that include racially diverse stimuli and note the lack of representation of Asian subgroups (e.g., East, South, and Southeast Asian), Pacific Islanders, as well as both Multiracial and multiethnic faces (especially with multiple minoritized backgrounds). We provide a new racially diverse set of free, standardized images including 140 unique faces representing eight different groups that vary in ethnicity and race (Asian, East Asian, Southeast Asian, Pacific Islander/Native Hawaiian, Hispanic/Latinx, White, Multiracial, and Multiracial Asian), along with norming data. These images and data are available for access use in research: https://osf.io/fkn7y;,citation_author=Chanel Meyers;,citation_author=Maria Garay;,citation_author=Kristin Pauker;,citation_publication_date=2024-01;,citation_cover_date=2024-01;,citation_year=2024;,citation_fulltext_html_url=https://osf.io/wde4b;,citation_doi=10.31234/osf.io/wde4b;">
<meta name="citation_reference" content="citation_title=Social inferences from faces: Ambient images generate a three-dimensional model;,citation_abstract=Three experiments are presented that investigate the two-dimensional valence/trustworthiness by dominance model of social inferences from faces (Oosterhof &amp;amp;amp; Todorov, 2008). Experiment 1 used image averaging and morphing techniques to demonstrate that consistent facial cues subserve a range of social inferences, even in a highly variable sample of 1000 ambient images (images that are intended to be representative of those encountered in everyday life, see Jenkins, White, Van Montfort, &amp; Burton, 2011). Experiment 2 then tested Oosterhof and Todorov’s two-dimensional model on this extensive sample of face images. The original two dimensions were replicated and a novel “youthful-attractiveness” factor also emerged. Experiment 3 successfully cross-validated the three-dimensional model using face averages directly constructed from the factor scores. These findings highlight the utility of the original trustworthiness and dominance dimensions, but also underscore the need to utilise varied face stimuli: with a more realistically diverse set of face images, social inferences from faces show a more elaborate underlying structure than hitherto suggested.;,citation_author=Clare A. M. Sutherland;,citation_author=Julian A. Oldmeadow;,citation_author=Isabel M. Santos;,citation_author=John Towler;,citation_author=D. Michael Burt;,citation_author=Andrew W. Young;,citation_publication_date=2013-04;,citation_cover_date=2013-04;,citation_year=2013;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0010027712002739;,citation_issue=1;,citation_doi=10.1016/j.cognition.2012.12.001;,citation_issn=0010-0277;,citation_volume=127;,citation_journal_title=Cognition;">
<meta name="citation_reference" content="citation_title=Focal Length Affects Depicted Shape and Perception of Facial Images;,citation_abstract=Static photographs are currently the most often employed stimuli in research on social perception. The method of photograph acquisition might affect the depicted subject’s facial appearance and thus also the impression of such stimuli. An important factor influencing the resulting photograph is focal length, as different focal lengths produce various levels of image distortion. Here we tested whether different focal lengths (50, 85, 105 mm) affect depicted shape and perception of female and male faces. We collected three portrait photographs of 45 (22 females, 23 males) participants under standardized conditions and camera setting varying only in the focal length. Subsequently, the three photographs from each individual were shown on screen in a randomized order using a 3-alternative forced-choice paradigm. The images were judged for attractiveness, dominance, and femininity/masculinity by 369 raters (193 females, 176 males). Facial width-to-height ratio (fWHR) was measured from each photograph and overall facial shape was analysed employing geometric morphometric methods (GMM). Our results showed that photographs taken with 50 mm focal length were rated as significantly less feminine/masculine, attractive, and dominant compared to the images taken with longer focal lengths. Further, shorter focal lengths produced faces with smaller fWHR. Subsequent GMM revealed focal length significantly affected overall facial shape of the photographed subjects. Thus methodology of photograph acquisition, focal length in this case, can significantly affect results of studies using photographic stimuli perhaps due to different levels of perspective distortion that influence shapes and proportions of morphological traits.;,citation_author=Vít Třebický;,citation_author=Jitka Fialová;,citation_author=Karel Kleisner;,citation_author=Jan Havlíček;,citation_publication_date=2016-02;,citation_cover_date=2016-02;,citation_year=2016;,citation_fulltext_html_url=https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0149313;,citation_issue=2;,citation_doi=10.1371/journal.pone.0149313;,citation_issn=1932-6203;,citation_volume=11;,citation_journal_title=PLOS ONE;">
<meta name="citation_reference" content="citation_title=Karolinska Directed Emotional Faces;,citation_author=D. Lundqvist;,citation_author=A. Flykt;,citation_author=A. Öhman;,citation_publication_date=2015-05;,citation_cover_date=2015-05;,citation_year=2015;,citation_fulltext_html_url=https://doi.apa.org/doi/10.1037/t27732-000;,citation_doi=10.1037/t27732-000;">
<meta name="citation_reference" content="citation_title=The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations;,citation_abstract=In this paper, we describe the acquisition and contents of a large-scale Chinese face database: the CAS-PEAL face database. The goals of creating the CAS-PEAL face database include the following: 1) providing the worldwide researchers of face recognition with different sources of variations, particularly pose, expression, accessories, and lighting (PEAL), and exhaustive ground-truth information in one uniform database; 2) advancing the state-of-the-art face recognition technologies aiming at practical applications by using off-the-shelf imaging equipment and by designing normal face variations in the database; and 3) providing a large-scale face database of Mongolian. Currently, the CAS-PEAL face database contains 99 594 images of 1040 individuals (595 males and 445 females). A total of nine cameras are mounted horizontally on an arc arm to simultaneously capture images across different poses. Each subject is asked to look straight ahead, up, and down to obtain 27 images in three shots. Five facial expressions, six accessories, and 15 lighting changes are also included in the database. A selected subset of the database (CAS-PEAL-R1, containing 30 863 images of the 1040 subjects) is available to other researchers now. We discuss the evaluation protocol based on the CAS-PEAL-R1 database and present the performance of four algorithms as a baseline to do the following: 1) elementarily assess the difficulty of the database for face recognition algorithms; 2) preference evaluation results for researchers using the database; and 3) identify the strengths and weaknesses of the commonly used algorithms.;,citation_author=Wen Gao;,citation_author=Bo Cao;,citation_author=Shiguang Shan;,citation_author=Xilin Chen;,citation_author=Delong Zhou;,citation_author=Xiaohua Zhang;,citation_author=Debin Zhao;,citation_publication_date=2008-01;,citation_cover_date=2008-01;,citation_year=2008;,citation_fulltext_html_url=https://ieeexplore.ieee.org/document/4404053;,citation_issue=1;,citation_doi=10.1109/TSMCA.2007.909557;,citation_issn=1558-2426;,citation_volume=38;,citation_journal_title=IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans;">
<meta name="citation_reference" content="citation_title=Broadening the stimulus set: Introducing the American Multiracial Faces Database;,citation_abstract=Face-based perceptions form the basis for how people behave towards each other and, hence, are central to understanding human interaction. Studying face perception requires a large and diverse set of stimuli in order to make ecologically valid, generalizable conclusions. To date, there are no publicly available databases with a substantial number of Multiracial or racially ambiguous faces. Our systematic review of the literature on Multiracial person perception documented that published studies have relied on computer-generated faces (84% of stimuli), Black-White faces (74%), and male faces (63%). We sought to address these issues, and to broaden the diversity of available face stimuli, by creating the American Multiracial Faces Database (AMFD). The AMFD is a novel collection of 110 faces with mixed-race heritage and accompanying ratings of those faces by naive observers that are freely available to academic researchers. The faces (smiling and neutral expression poses) were rated on attractiveness, emotional expression, racial ambiguity, masculinity, racial group membership(s), gender group membership(s), warmth, competence, dominance, and trustworthiness. The large majority of the AMFD faces are racially ambiguous and can pass into at least two different racial categories. These faces will be useful to researchers seeking to study Multiracial person perception as well as those looking for racially ambiguous faces in order to study categorization processes in general. Consequently, the AMFD will be useful to a broad group of researchers who are studying face perception.;,citation_author=Jacqueline M. Chen;,citation_author=Jasmine B. Norman;,citation_author=Yeseul Nam;,citation_publication_date=2021-02;,citation_cover_date=2021-02;,citation_year=2021;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-020-01447-8;,citation_issue=1;,citation_doi=10.3758/s13428-020-01447-8;,citation_issn=1554-3528;,citation_volume=53;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=The Israeli Face Database (IFD): A multi-ethnic database of faces with supporting social norming data;,citation_abstract=Human faces and their representations play a key role in social interactions. Consequently, face images are widely used as stimuli across psychological research. However, most available face databases primarily represent Western, predominantly White populations, raising concerns about the generalizability of findings derived from such nondiverse stimuli sets. The current work addresses this problem by introducing the Israeli Face Database (IFD), a novel database presenting face images of real individuals from an ethnically diverse population underrepresented in psychological research on face perception—people residing in Israel. The IFD features multiple highly controlled images per face identity, presenting several facial expressions. Additionally, we provide detailed demographic data on the face identities, expression validation data rated by Israeli participants, and social norming data rated by both Israeli and US-based participants. Validation data (Study 1) confirmed that the expression images in the IFD accurately convey their intended expressions. Norming data analysis (Study 2) revealed that judgments of IFD faces were broadly consistent across cultures, with notable variations in perceived typicality and perceived ethnic diversity. These results highlight the IFD’s strong psychometric properties and its ability to capture nuanced cultural and individual differences in face perception. We propose that the IFD can serve as a valuable tool for diversifying stimuli in social-psychological research.;,citation_author=Maayan Trzewik;,citation_author=Mayan Navon;,citation_author=Tal Moran;,citation_author=Hadas Wardi;,citation_author=Adi Langer;,citation_author=Bat-Sheva Hadad;,citation_author=Carmel Sofer;,citation_author=Niv Reggev;,citation_publication_date=2025-06;,citation_cover_date=2025-06;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-025-02723-1;,citation_issue=7;,citation_doi=10.3758/s13428-025-02723-1;,citation_issn=1554-3528;,citation_volume=57;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=Faking It Isn’t Making It: Research Needs Spontaneous and Naturalistic Facial Expressions;,citation_abstract=Facial expressions play a pivotal role in shaping social interactions. However, the conceptualization of facial expressions as direct readouts of internal emotional experience has led to the conflation of three distinct question types. Specifically, there is confusion between questions concerning: (Q1) the production of facial expressions, (Q2) how accurately perceivers interpret expressors’ internal emotions from their outward expressions, and (Q3) perceiver responses to the outward appearance of expressions independent of the expressor’s internal emotional state. The disentanglement of these three question types highlights that, because the facial stimuli traditionally used in research are posed rather than reflective of internal emotions, they can only test perceiver responses (Q3), though they have often been interpreted as measures of perceptual accuracy (Q2). Moreover, due to their exaggerated and prototypical nature, these stimuli fail to capture the nuance and complexity of real-world expressions, potentially leading to ecologically invalid findings even for Q3. New data presented here also suggest that many of these stimuli are not perceived as genuinely emotional and may appear unnatural. We review evidence demonstrating that stimuli that are naturally- or spontaneously-elicited and/or appear genuinely emotional can produce different findings than traditional posed stimuli. Fortunately, naturalistic and spontaneous expression stimuli are now readily available for the field to move forward. We conclude with seven recommendations for advancing facial expression research.;,citation_author=Amy Dawel;,citation_author=Eva G. Krumhuber;,citation_author=Romina Palermo;,citation_publication_date=2025-07;,citation_cover_date=2025-07;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.1007/s42761-025-00320-1;,citation_doi=10.1007/s42761-025-00320-1;,citation_issn=2662-205X;,citation_journal_title=Affective Science;">
<meta name="citation_reference" content="citation_title=The Belfast Induced Natural Emotion Database;,citation_abstract=For many years psychological research on facial expression of emotion has relied heavily on a recognition paradigm based on posed static photographs. There is growing evidence that there may be fundamental differences between the expressions depicted in such stimuli and the emotional expressions present in everyday life. Affective computing, with its pragmatic emphasis on realism, needs examples of natural emotion. This paper describes a unique database containing recordings of mild to moderate emotionally colored responses to a series of laboratory-based emotion induction tasks. The recordings are accompanied by information on self-report of emotion and intensity, continuous trace-style ratings of valence and intensity, the sex of the participant, the sex of the experimenter, the active or passive nature of the induction task, and it gives researchers the opportunity to compare expressions from people from more than one culture.;,citation_author=Ian Sneddon;,citation_author=Margaret McRorie;,citation_author=Gary McKeown;,citation_author=Jennifer Hanratty;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_fulltext_html_url=https://ieeexplore.ieee.org/abstract/document/5975142;,citation_issue=1;,citation_doi=10.1109/T-AFFC.2011.26;,citation_issn=1949-3045;,citation_volume=3;,citation_journal_title=IEEE Transactions on Affective Computing;">
<meta name="citation_reference" content="citation_title=The NimStim set of facial expressions: Judgments from untrained research participants;,citation_author=Nim Tottenham;,citation_author=James W. Tanaka;,citation_author=Andrew C. Leon;,citation_author=Thomas McCarry;,citation_author=Marcella Nurse;,citation_author=Todd A. Hare;,citation_author=David J. Marcus;,citation_author=Alissa Westerlund;,citation_author=Bj Casey;,citation_author=Charles Nelson;,citation_publication_date=2009-08;,citation_cover_date=2009-08;,citation_year=2009;,citation_fulltext_html_url=https://linkinghub.elsevier.com/retrieve/pii/S0165178108001480;,citation_issue=3;,citation_doi=10.1016/j.psychres.2008.05.006;,citation_issn=01651781;,citation_volume=168;,citation_journal_title=Psychiatry Research;">
<meta name="citation_reference" content="citation_title=Perceived emotion genuineness: Normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets;,citation_abstract=In everyday social interactions, people’s facial expressions sometimes reflect genuine emotion (e.g., anger in response to a misbehaving child) and sometimes do not (e.g., smiling for a school photo). There is increasing theoretical interest in this distinction, but little is known about perceived emotion genuineness for existing facial expression databases. We present a new method for rating perceived genuineness using a neutral-midpoint scale (–7 = completely fake; 0 = don’t know; +7 = completely genuine) that, unlike previous methods, provides data on both relative and absolute perceptions. Normative ratings from typically developing adults for five emotions (anger, disgust, fear, sadness, and happiness) provide three key contributions. First, the widely used Pictures of Facial Affect (PoFA; i.e., “the Ekman faces”) and the Radboud Faces Database (RaFD) are typically perceived as not showing genuine emotion. Also, in the only published set for which the actual emotional states of the displayers are known (via self-report; the McLellan faces), percepts of emotion genuineness often do not match actual emotion genuineness. Second, we provide genuine/fake norms for 558 faces from several sources (PoFA, RaFD, KDEF, Gur, FacePlace, McLellan, News media), including a list of 143 stimuli that are event-elicited (rather than posed) and, congruently, perceived as reflecting genuine emotion. Third, using the norms we develop sets of perceived-as-genuine (from event-elicited sources) and perceived-as-fake (from posed sources) stimuli, matched on sex, viewpoint, eye-gaze direction, and rated intensity. We also outline the many types of research questions that these norms and stimulus sets could be used to answer.;,citation_author=Amy Dawel;,citation_author=Luke Wright;,citation_author=Jessica Irons;,citation_author=Rachael Dumbleton;,citation_author=Romina Palermo;,citation_author=Richard O’Kearney;,citation_author=Elinor McKone;,citation_publication_date=2017-08;,citation_cover_date=2017-08;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-016-0813-2;,citation_issue=4;,citation_doi=10.3758/s13428-016-0813-2;,citation_issn=1554-3528;,citation_volume=49;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=Padova Emotional Dataset of Facial Expressions (PEDFE): A unique dataset of genuine and posed emotional facial expressions;,citation_abstract=Facial expressions are among the most powerful signals for human beings to convey their emotional states. Indeed, emotional facial datasets represent the most effective and controlled method of examining humans’ interpretation of and reaction to various emotions. However, scientific research on emotion mainly relied on static pictures of facial expressions posed (i.e., simulated) by actors, creating a significant bias in emotion literature. This dataset tries to fill this gap, providing a considerable amount (N = 1458) of dynamic genuine (N = 707) and posed (N = 751) clips of the six universal emotions from 56 participants. The dataset is available in two versions: original clips, including participants’ body and background, and modified clips, where only the face of participants is visible. Notably, the original dataset has been validated by 122 human raters, while the modified dataset has been validated by 280 human raters. Hit rates for emotion and genuineness, as well as the mean, standard deviation of genuineness, and intensity perception, are provided for each clip to allow future users to select the most appropriate clips needed to answer their scientific questions.;,citation_author=A. Miolla;,citation_author=M. Cardaioli;,citation_author=C. Scarpazza;,citation_publication_date=2023-08;,citation_cover_date=2023-08;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.3758/s13428-022-01914-4;,citation_issue=5;,citation_doi=10.3758/s13428-022-01914-4;,citation_issn=1554-3528;,citation_volume=55;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation;,citation_abstract=Faces are widely used as stimuli in various research fields. Interest in emotion-related differences and age-associated changes in the processing of faces is growing. With the aim of systematically varying both expression and age of the face, we created FACES, a database comprising N=171 naturalistic faces of young, middle-aged, and older women and men. Each face is represented with two sets of six facial expressions (neutrality, sadness, disgust, fear, anger, and happiness), resulting in 2,052 individual images. A total of N=154 young, middleaged, and older women and men rated the faces in terms of facial expression and perceived age. With its large age range of faces displaying different expressions, FACES is well suited for investigating developmental and other research questions on emotion, motivation, and cognition, as well as their interactions. Information on using FACES for research purposes can be found at http://faces.mpib-berlin.mpg.de.;,citation_author=Natalie C. Ebner;,citation_author=Michaela Riediger;,citation_author=Ulman Lindenberger;,citation_publication_date=2010-02;,citation_cover_date=2010-02;,citation_year=2010;,citation_fulltext_html_url=https://doi.org/10.3758/BRM.42.1.351;,citation_issue=1;,citation_doi=10.3758/BRM.42.1.351;,citation_issn=1554-3528;,citation_volume=42;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=Moving faces, looking places: Validation of the Amsterdam Dynamic Facial Expression Set (ADFES);,citation_abstract=We report two studies validating a new standardized set of filmed emotion expressions, the Amsterdam Dynamic Facial Expression Set (ADFES). The ADFES is distinct from existing datasets in that it includes a face-forward version and two different head-turning versions (faces turning toward and away from viewers), North-European as well as Mediterranean models (male and female), and nine discrete emotions (joy, anger, fear, sadness, surprise, disgust, contempt, pride, and embarrassment). Study 1 showed that the ADFES received excellent recognition scores. Recognition was affected by social categorization of the model: displays of North-European models were better recognized by Dutch participants, suggesting an ingroup advantage. Head-turning did not affect recognition accuracy. Study 2 showed that participants more strongly perceived themselves to be the cause of the other’s emotion when the model’s face turned toward the respondents. The ADFES provides new avenues for research on emotion expression and is available for researchers upon request. (PsycInfo Database Record (c) 2025 APA, all rights reserved);,citation_author=Job Schalk;,citation_author=Skyler T. Hawk;,citation_author=Agneta H. Fischer;,citation_author=Bertjan Doosje;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_doi=10.1037/a0023853;,citation_issn=1931-1516;,citation_volume=11;,citation_journal_title=Emotion;">
<meta name="citation_reference" content="citation_title=The Glasgow Face Matching Test;,citation_abstract=We describe a new test for unfamiliar face matching, the Glasgow Face Matching Test (GFMT). Viewers are shown pairs of faces, photographed in full-face view but with different cameras, and are asked to make same/different judgments. The full version of the test comprises 168 face pairs, and we also describe a shortened version with 40 pairs. We provide normative data for these tests derived from large subject samples. We also describe associations between the GFMT and other tests of matching and memory. The new test correlates moderately with face memory but more strongly with object matching, a result that is consistent with previous research highlighting a link between object and face matching, specific to unfamiliar faces. The test is available free for scientific use.;,citation_author=A. Mike Burton;,citation_author=David White;,citation_author=Allan McNeill;,citation_publication_date=2010-02;,citation_cover_date=2010-02;,citation_year=2010;,citation_fulltext_html_url=https://doi.org/10.3758/BRM.42.1.286;,citation_issue=1;,citation_doi=10.3758/BRM.42.1.286;,citation_issn=1554-3528;,citation_volume=42;,citation_journal_title=Behavior Research Methods;">
<meta name="citation_reference" content="citation_title=Presentation and validation of the Radboud Faces Database;,citation_abstract=Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.;,citation_author=Oliver Langner;,citation_author=Ron Dotsch;,citation_author=Gijsbert Bijlstra;,citation_author=Daniel H. J. Wigboldus;,citation_author=Skyler T. Hawk;,citation_author=Ad Knippenberg;,citation_publication_date=2010-12;,citation_cover_date=2010-12;,citation_year=2010;,citation_fulltext_html_url=https://doi.org/10.1080/02699930903485076;,citation_issue=8;,citation_doi=10.1080/02699930903485076;,citation_issn=0269-9931;,citation_volume=24;,citation_journal_title=Cognition and Emotion;">
<meta name="citation_reference" content="citation_title=The generalizability crisis;,citation_abstract=Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned – that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology – the linear mixed model – I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers’ actual generalization intentions. I demonstrate that although the “random effect” formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology’s ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.;,citation_author=Tal Yarkoni;,citation_publication_date=2022-01;,citation_cover_date=2022-01;,citation_year=2022;,citation_fulltext_html_url=https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalizability-crisis/AD386115BA539A759ACB3093760F4824;,citation_doi=10.1017/S0140525X20001685;,citation_issn=0140-525X, 1469-1825;,citation_volume=45;,citation_journal_title=Behavioral and Brain Sciences;">
<meta name="citation_reference" content="citation_title=Reproducible Methods for Face Research;,citation_abstract=Face stimuli are commonly created in ways that are not explained well enough for others to reproduce them. In this paper, we document the irreproducibility of most face stimuli, explain the benefits of reproducible stimuli, and introduce the open-source R package webmorphR that facilitates scriptable face image processing. We explain the technical processes of morphing and transforming through a case study of creating face stimuli from an open-access image set. Finally, we discuss some ethical and methodological issues around the use of face images in research that may be ameliorated through the use of reproducible stimuli.;,citation_author=Lisa M DeBruine;,citation_author=Iris J Holzleitner;,citation_author=Bernard Tiddeman;,citation_author=Benedict C Jones;,citation_publication_date=2022-10;,citation_cover_date=2022-10;,citation_year=2022;,citation_fulltext_html_url=https://osf.io/preprints/psyarxiv/j2754_v1/;,citation_doi=10.31234/osf.io/j2754;,citation_publisher=PsyArXiv;">
<meta name="citation_reference" content="citation_title=A Grim Image: Considerations for Methods of portrait photography in psychological science;,citation_abstract=Photographs of people play a central role in psychological research across various fields, yet methodological inconsistencies in how these images are captured and reported may hinder replication and comparability. Despite growing attention to the replication crisis in psychology, vagueness and insufficient detail in methodological reporting, particularly regarding photographic practices, remain an overlooked issue. Drawing an analogy between research methods and culinary recipes, we argue that the quality of a method depends on clear, comprehensive instructions that enable repeatability and reproducibility. We selected three basic key aspects of photography—focal length, lighting, and background colour— to show how varying them can substantially influence image outcomes and, consequently, research findings. Using examples, we illustrate how these variables impact the appearance of photographic stimuli and discuss their potential effects on study results, such as perceived traits, recognition accuracy, and morphological measurements. To address this gap, we propose minimal reporting standards for photographic methods in face research, including detailed descriptions of equipment, settings, and setup, along with visual examples. By adopting these standards, researchers can improve the clarity and transparency of their methods, facilitating robust and replicable research. While we focus on photography, this initiative underscores the broader need for detailed methodological reporting across disciplines. Ultimately, our goal is to inspire a culture of methodological rigour, fostering confidence in the replicability of findings and advancing the field of psychological science.;,citation_author=Vít Třebický;,citation_author=Jitka Třebická Fialová;,citation_author=R. Thora Bjornsdottir;,citation_author=Lisa M DeBruine;,citation_publication_date=2024-11;,citation_cover_date=2024-11;,citation_year=2024;,citation_fulltext_html_url=https://osf.io/z4svb_v1/;,citation_doi=10.31219/osf.io/z4svb;,citation_publisher=OSF Preprints;">
<meta name="citation_reference" content="citation_title=The weirdest people in the world?;,citation_abstract=Behavioral scientists routinely publish broad claims about human psychology and behavior in the world’s top journals based on samples drawn entirely from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. Researchers – often implicitly – assume that either there is little variation across human populations, or that these “standard subjects” are as representative of the species as any other population. Are these assumptions justified? Here, our review of the comparative database from across the behavioral sciences suggests both that there is substantial variability in experimental results across populations and that WEIRD subjects are particularly unusual compared with the rest of the species – frequent outliers. The domains reviewed include visual perception, fairness, cooperation, spatial reasoning, categorization and inferential induction, moral reasoning, reasoning styles, self-concepts and related motivations, and the heritability of IQ. The findings suggest that members of WEIRD societies, including young children, are among the least representative populations one could find for generalizing about humans. Many of these findings involve domains that are associated with fundamental aspects of psychology, motivation, and behavior – hence, there are no obvious a priori grounds for claiming that a particular behavioral phenomenon is universal based on sampling from a single subpopulation. Overall, these empirical patterns suggests that we need to be less cavalier in addressing questions of human nature on the basis of data drawn from this particularly thin, and rather unusual, slice of humanity. We close by proposing ways to structurally re-organize the behavioral sciences to best tackle these challenges.;,citation_author=Joseph Henrich;,citation_author=Steven J. Heine;,citation_author=Ara Norenzayan;,citation_publication_date=2010-06;,citation_cover_date=2010-06;,citation_year=2010;,citation_fulltext_html_url=https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/weirdest-people-in-the-world/BF84F7517D56AFF7B7EB58411A554C17;,citation_issue=2-3;,citation_doi=10.1017/S0140525X0999152X;,citation_issn=1469-1825, 0140-525X;,citation_volume=33;,citation_journal_title=Behavioral and Brain Sciences;">
<meta name="citation_reference" content="citation_title=The Bogazici face database: Standardized photographs of Turkish faces with supporting materials;,citation_abstract=Many sets of human facial photographs produced in Western cultures are available for scientific research. We report here on the development of a face database of Turkish undergraduate student targets. High-resolution standardized photographs were taken and supported by the following materials: (a) basic demographic and appearance-related information, (b) two types of landmark configurations (for Webmorph and geometric morphometrics (GM)), (c) facial width-to-height ratio (fWHR) measurement, (d) information on photography parameters, (e) perceptual norms provided by raters. We also provide various analyses and visualizations of facial variation based on rating norms using GM. Finally, we found that there is sexual dimorphism in fWHR in our sample but that this is accounted for by body mass index. We present the pattern of associations between rating norms, GM and fWHR measurements. The database and supporting materials are freely available for scientific research purposes.;,citation_author=S. Adil Saribay;,citation_author=Ali Furkan Biten;,citation_author=Erdem Ozan Meral;,citation_author=Pinar Aldan;,citation_author=Vít Třebický;,citation_author=Karel Kleisner;,citation_publication_date=2018-02;,citation_cover_date=2018-02;,citation_year=2018;,citation_fulltext_html_url=https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192018;,citation_issue=2;,citation_doi=10.1371/journal.pone.0192018;,citation_issn=1932-6203;,citation_volume=13;,citation_journal_title=PLOS ONE;">
<meta name="citation_reference" content="citation_title=Assessing the Point at Which Averages Are Stable: A Tutorial in the Context of Impression Formation;,citation_abstract=Across many diverse areas of research, it is common to average a series of observations, and to use these averages in subsequent analyses. Research using this approach faces the challenge of knowing when these averages are “stable.” Meaning, to what extent do averages change when additional observations are included? Using averages that are not stable introduces error into any analysis, and knowing the point of stability can inform research design. The current research develops a tool, implemented in R, to assess when averages are stable. Using a sequential sampling approach, it determines how many observations are needed before additional observations would no longer meaningfully change an average. We illustrate how to use this tool with data from the impression formation literature, demonstrating that averages of some perceived traits (e.g., happy) stabilize with fewer observations than others (e.g., assertive). This tutorial provides step-by-step instructions for implementation in researchers’ own data.;,citation_author=Eric Hehman;,citation_author=Sally Y. Xie;,citation_author=Eugene K. Ofosu;,citation_author=Gabriel A. Nespoli;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.1521/soco.2025.43.5.488;,citation_issue=5;,citation_doi=10.1521/soco.2025.43.5.488;,citation_volume=43;,citation_journal_title=Social Cognition;">
<meta name="citation_reference" content="citation_title=Recognizing Faces;,citation_abstract=The idea that most of us are good at recognizing faces permeates everyday thinking and is widely used in the research literature. However, it is a correct characterization only of familiar-face recognition. In contrast, the perception and recognition of unfamiliar faces can be surprisingly error-prone, and this has important consequences in many real-life settings. We emphasize the variability in views of faces encountered in everyday life and point out how neglect of this important property has generated some misleading conclusions. Many approaches have treated image variability as unwanted noise, whereas we show how studies that use and explore the implications of image variability can drive substantial theoretical advances.;,citation_author=Andrew W. Young;,citation_author=A. Mike Burton;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_fulltext_html_url=https://doi.org/10.1177/0963721416688114;,citation_issue=3;,citation_doi=10.1177/0963721416688114;,citation_issn=0963-7214;,citation_volume=26;,citation_journal_title=Current Directions in Psychological Science;">
<meta name="citation_reference" content="citation_title=Experimentum;,citation_abstract=Web-based software for building, deploying, and managing psychology studies.;,citation_author=Lisa M DeBruine;,citation_author=Rebecca Lai;,citation_author=Benedict C Jones;,citation_author=Rifah Abdullah;,citation_author=Gaby Mahrholz;,citation_publication_date=2020-09;,citation_cover_date=2020-09;,citation_year=2020;,citation_fulltext_html_url=https://zenodo.org/records/4010579;,citation_doi=10.5281/zenodo.4010579;,citation_publisher=Zenodo;">
</head>

  <body class="quarto-notebook quarto-light">
    <div id="quarto-embed-header" class="headroom fixed-top bg-primary">
      
      <a onclick="window.quartoBackToArticle(); return false;" class="btn btn-primary quarto-back-link" href=""><i class="bi bi-caret-left"></i> Back to Article</a>
      <h6><i class="bi bi-journal-code"></i> Article Notebook</h6>

            <a href="./index.qmd" class="btn btn-primary quarto-download-embed" download="index.qmd">Download Source</a>
          </div>

     <header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Capturing Many Faces</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">R. Thora Bjornsdottir <a href="mailto:thora.bjornsdottir@stir.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-1016-3829" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Stirling
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Vít Třebický <a href="mailto:vit.trebicky@natur.cuni.cz" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0003-1440-1772" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Charles University
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Lisa DeBruine <a href="mailto:lisa.debruine@glasgow.ac.uk" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-7523-5539" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Glasgow
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>



    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#existing-face-databases" id="toc-existing-face-databases" class="nav-link" data-scroll-target="#existing-face-databases"><span class="header-section-number">1.1</span> Existing face databases</a></li>
  <li><a href="#open-science-considerations" id="toc-open-science-considerations" class="nav-link" data-scroll-target="#open-science-considerations"><span class="header-section-number">1.2</span> Open Science considerations</a></li>
  <li><a href="#the-current-work" id="toc-the-current-work" class="nav-link" data-scroll-target="#the-current-work"><span class="header-section-number">1.3</span> The current work</a></li>
  </ul></li>
  <li><a href="#studyphase-1-protocol-development-stimulus-collection" id="toc-studyphase-1-protocol-development-stimulus-collection" class="nav-link" data-scroll-target="#studyphase-1-protocol-development-stimulus-collection"><span class="header-section-number">2</span> Study/phase 1: Protocol development &amp; stimulus collection</a>
  <ul class="collapse">
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method"><span class="header-section-number">2.1</span> Method</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">2.2</span> Results</a></li>
  </ul></li>
  <li><a href="#studyphase-2-validationnorming-data" id="toc-studyphase-2-validationnorming-data" class="nav-link" data-scroll-target="#studyphase-2-validationnorming-data"><span class="header-section-number">3</span> Study/phase 2: Validation/norming data</a>
  <ul class="collapse">
  <li><a href="#method-1" id="toc-method-1" class="nav-link" data-scroll-target="#method-1"><span class="header-section-number">3.1</span> Method</a></li>
  <li><a href="#results-1" id="toc-results-1" class="nav-link" data-scroll-target="#results-1"><span class="header-section-number">3.2</span> Results</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">4</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#the-value-of-the-current-work" id="toc-the-value-of-the-current-work" class="nav-link" data-scroll-target="#the-value-of-the-current-work"><span class="header-section-number">4.1</span> The value of the current work</a></li>
  <li><a href="#reflections-on-the-process-limitations" id="toc-reflections-on-the-process-limitations" class="nav-link" data-scroll-target="#reflections-on-the-process-limitations"><span class="header-section-number">4.2</span> Reflections on the Process &amp; Limitations</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">4.3</span> Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">      

       <!-- to add rest of team as authors w/ CRediT -->
<div class="cell-container"><div class="cell-decorator"><pre>In [1]:</pre></div><div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.1     ✔ stringr   1.5.1
✔ ggplot2   4.0.1     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.1.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#devtools::install_github("debruine/webmorphR@dev")</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(webmorphR)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
************
Welcome to webmorphR. For support and examples visit:
https://debruine.github.io/webmorphR/
************</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Rating data</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>data_models <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/manyfaces-pilot-models_cleaned.csv"</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Rating data</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>data_exp <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/manyfaces-pilot-exp_cleaned.csv"</span>,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Questionnaire data</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>data_qu <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/manyfaces-pilot-quest_cleaned.csv"</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>                    <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>data_excl <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/exclusions.csv"</span>,</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                    <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_bw</span>(<span class="at">base_size =</span> <span class="dv">14</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Faces are rich sources of information, playing a key role in human social perception. They strongly draw attention, serving as a primary means of person recognition and informing social inferences <span class="citation" data-cites="young_recognizing_2017 zebrowitz_reading_1997">(e.g., <a href="#ref-young_recognizing_2017" role="doc-biblioref">Young &amp; Burton, 2017</a>; <a href="#ref-zebrowitz_reading_1997" role="doc-biblioref">Zebrowitz, 1997</a>)</span><!--other refs?-->. Various subfields within psychology—including vision science, person perception, social cognition, affective science, behavioural science, and person recognition—address research questions involving faces and thus necessitate the use of face images as stimuli. Here, we introduce a novel face image database for research use. This database was collected as part of an international multi-lab collaboration and addresses various limitations of existing databases. Crucially, we take a transparent approach aligned with Open Science best practices and make openly available the reproducible protocol we followed to collect the images, enabling future expansion of the database.</p>
<section id="existing-face-databases" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="existing-face-databases"><span class="header-section-number">1.1</span> Existing face databases</h3>
<p>A wealth of face stimulus databases exists, created for various purposes. Many of these are available <span class="citation" data-cites="workman_face_2021">(see e.g., <a href="#ref-workman_face_2021" role="doc-biblioref">Workman &amp; Chatterjee, 2021</a> face image meta-database to search many of these)</span> for research purposes allowing access to a broad variety of face stimuli, well-suited to address questions in a given subfield. For example, there are many different databases in which photographed individuals (referred to as targets or models) display different facial expressions of emotion <span class="citation" data-cites="ebner_facesdatabase_2010 van_der_schalk_moving_2011">(e.g., <a href="#ref-ebner_facesdatabase_2010" role="doc-biblioref">Ebner et al., 2010</a>; <a href="#ref-van_der_schalk_moving_2011" role="doc-biblioref">Schalk et al., 2011</a>, of use in affective science)</span>, appear in varying lighting and angles <span class="citation" data-cites="burton_glasgow_2010 gao_cas-peal_2008">(e.g., <a href="#ref-burton_glasgow_2010" role="doc-biblioref">Burton et al., 2010</a>; <a href="#ref-gao_cas-peal_2008" role="doc-biblioref">Gao et al., 2008</a>, useful for person recognition research)</span>, or belong to various age or racial/ethnic groups (for perception researcher). However, many studies tend to rely on a small number of those stimulus databases, which introduces potential problems. For example, 4107 published papers cite the Karolinska Directed Emotional Faces <span class="citation" data-cites="lundqvist_karolinska_2015">(<a href="#ref-lundqvist_karolinska_2015" role="doc-biblioref">Lundqvist et al., 2015</a>)</span>, 4375 cite the NimStim Set of Facial Expressions <span class="citation" data-cites="tottenham_nimstim_2009">(<a href="#ref-tottenham_nimstim_2009" role="doc-biblioref">Tottenham et al., 2009</a>)</span>, 2461 cite the Chicago Face Database <span class="citation" data-cites="ma_chicago_2015">(<a href="#ref-ma_chicago_2015" role="doc-biblioref">Ma et al., 2015</a>)</span>, and 3219 the Radboud Faces Database <span class="citation" data-cites="langner_presentation_2010">(<a href="#ref-langner_presentation_2010" role="doc-biblioref">Langner et al., 2010</a>)</span>. This frequent (over)use of the same sets of stimuli can compromise generalizability when various research questions are tested on only a limited sample of face images <span class="citation" data-cites="debruine_reproducible_2022 yarkoni_generalizability_2022">(<a href="#ref-debruine_reproducible_2022" role="doc-biblioref">DeBruine et al., 2022</a>; <a href="#ref-yarkoni_generalizability_2022" role="doc-biblioref">Yarkoni, 2022</a>)</span>. Participants, especially those participating online, may also become familiar with frequently used stimuli, potentially introducing biases to their responses.</p>
<p>Of course, not all research involving face images uses stimuli from published databases. It is a common for researchers to purposefully collect images themselves. But these stimulus sets are seldom openly shared (e.g., due to ethical limitations). Although this circumvents the issues with frequently-used databases, it introduces its own set of problems. Chiefly, face images collected by researchers for their own research use are often insufficiently documented. Although the resulting images may be described and/or illustrated with an example, the process of image collection is rarely described in adequate detail. This is often the case for even those databases openly available for research use. This lack of image acquisition (and processing) methods documentation limits transparency, replication attempts–for example, another researcher attempting to replicate a finding using new face images collected themselves, rather than the images collected by the original researchers <span class="citation" data-cites="trebicky_grim_2024">(see <a href="#ref-trebicky_grim_2024" role="doc-biblioref">Třebický et al., 2024</a>)</span>–and ultimately comparisons of results.</p>
<p>In many databases (both publicly available and not), there are also limitations in the diversity of stimuli. For example, many include images of only White or Western targets/models <span class="citation" data-cites="cook_why_2021">(e.g., see <a href="#ref-cook_why_2021" role="doc-biblioref">Cook &amp; Over, 2021</a>)</span>. This reinforces the centering of Whiteness and Western culture in psychological research, importantly limiting the ecological validity and generalizability of conclusions <span class="citation" data-cites="henrich_weirdest_2010">(see also <a href="#ref-henrich_weirdest_2010" role="doc-biblioref">Henrich et al., 2010</a>)</span>. Recently created databases have sought to address this, specifically collecting images of multiracial/multi-ethnic and non-Western samples, for example <span class="citation" data-cites="chen_broadening_2021 meyers_hawaii_2024 saribay_bogazici_2018 trzewik_israeli_2025">(e.g., <a href="#ref-chen_broadening_2021" role="doc-biblioref">Chen et al., 2021</a>; <a href="#ref-meyers_hawaii_2024" role="doc-biblioref">Meyers et al., 2024</a>; <a href="#ref-saribay_bogazici_2018" role="doc-biblioref">Saribay et al., 2018</a>; <a href="#ref-trzewik_israeli_2025" role="doc-biblioref">Trzewik et al., 2025</a>)</span>, but more such work is needed to further diversify the pool of available face research stimuli. Many world regions remain underrepresented. This may be due, in part, to resource limitations: Equipment and setups for collecting images suitable for research use can be prohibitively expensive and complex, making database collection not feasible for everyone and everywhere, thereby limiting database diversity. Additionally, researchers with the necessary equipment may be limited in terms of the diversity of their available participant sample. Furthermore, to date, no extant database contains images of individuals from <em>multiple</em> world regions.</p>
<p>A final limitation of extant face image databases is their largely static state. That is, once collected, the number of stimuli available does not change. There are some exceptions to this, such as multiple waves of additions to the Chicago Face Database <span class="citation" data-cites="lakshmi_india_2021 ma_chicago_2021">(<a href="#ref-lakshmi_india_2021" role="doc-biblioref">Lakshmi et al., 2021</a>; <a href="#ref-ma_chicago_2021" role="doc-biblioref">Ma et al., 2021</a>)</span>. Continued additions to databases are not the norm, however. This not only constrains the number of stimuli in any given database, increasing the likelihood of the frequent reuse of any individual stimulus, but can also render some databases obsolete over time (e.g., due to very dated model appearance/styling).</p>
</section>
<section id="open-science-considerations" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="open-science-considerations"><span class="header-section-number">1.2</span> Open Science considerations</h3>
<p>Open and transparent methods are essential for fully understanding, evaluating, and replicating research. Although methods reporting has become more open and accessible with researchers sharing materials such as questionnaire wording, stimuli, and programming scripts, the reporting of face image database collection procedures often lacks crucial details for full transparency and reproducibility. This matters because differences in details such as focal length, for example, can importantly affect faces’ appearance, subsequently impacting perceptions of them <span class="citation" data-cites="trebicky_focal_2016 trebicky_grim_2024">(e.g., <a href="#ref-trebicky_focal_2016" role="doc-biblioref">Třebický et al., 2016</a>; see <a href="#ref-trebicky_grim_2024" role="doc-biblioref">Třebický et al., 2024</a> for further discussion)</span>.</p>
<p>Big Team Science (i.e., multi-lab) endeavours and large-scale collaborations are now recognized as a vital part of improving science. Within psychology, multiple such initiatives have importantly contributed to addressing a variety of research questions, improving diversity and generalizability in the field (e.g., in the domain of face perception, Jones et al.’s <span class="citation" data-cites="jones_which_2021">(<a href="#ref-jones_which_2021" role="doc-biblioref">2021</a>)</span> Psychological Science Accelerator project). Such an approach has yet to be applied to stimulus database collection, but it represents an opportunity to address many of the limitations of face image databases raised.</p>
</section>
<section id="the-current-work" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="the-current-work"><span class="header-section-number">1.3</span> The current work</h3>
<p>Here, we sought to address the limitations of existing face image databases through a Big Team Science approach. This served as the first project of ManyFaces, an international consortium of face perception and recognition researchers formed in 2022.</p>
<p>To address transparency and reproducibility issues, we developed an openly available, reproducible protocol for face image collection. The protocol covers collecting multiple images (varying in standardization, viewing angle, and facial expression) of each target/model to benefit a wide array of possible research areas and questions related to face perception and recognition. Following this protocol, we collected images (Phase 1) in 20 different labs across the world, spanning 11 countries and five world regions (Europe, Latin America, North America, the Middle East, and Southeast Asia). This resulted in a diverse set of models and images, more than would be possible to collect in or by a single lab. Moreover, this database is not static but can be added to in the future by any interested researcher following the protocol. We also validated the database (Phase 2) by recruiting online perceivers to rate a subset of the images (i.e., front-facing images) to generate norming data for key social trait perceptions and emotion recognition. We make this image database and norming data available for future research.</p>
<p>Altogether, we introduce a new, diverse, openly-available face stimulus set that cancontinue to grow in the future. We believe that this effort will broaden the generalizability of findings in face perception and recognition research.</p>
</section>
</section>
<section id="studyphase-1-protocol-development-stimulus-collection" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="studyphase-1-protocol-development-stimulus-collection"><span class="header-section-number">2</span> Study/phase 1: Protocol development &amp; stimulus collection</h2>
<p>To set face perception and recognition Big Team Science in motion and enable conducting multi-site studies, a set of stimuli suitable for most areas (e.g., in terms of research question, geographic location) is needed. Therefore, we developed a protocol allowing us to collect such a database of face images. We developed the protocol with maximum usability and accessibility in mind, making it transparent and readable to non-experts, and designed for use with attainable (vs.&nbsp;highly specialized and expensive) equipment, with minimal setup requirements, and without requiring expertise in photography. Note that the protocol is for in-lab image collection to minimize external noise, to enable the collection of models’ self-report data, and to ensure the consensual use of models’ images (vs.&nbsp;scraping images from online sources or generating them). Photographing faces in a controlled lab environment thus strikes a balance in terms of ecological validity and ethical concerns <span class="citation" data-cites="trzewik_israeli_2025">(see <a href="#ref-trzewik_israeli_2025" role="doc-biblioref">Trzewik et al., 2025</a> for discussion of the value of lab-photographed vs.&nbsp;artificially generated and ambient face images)</span>.</p>
<section id="method" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="method"><span class="header-section-number">2.1</span> Method</h3>
<p>For both studies/phases, the University of Glasgow provided ethical approval. We obtained additional local ethical approval at collaborating institutions where required.</p>
<section id="protocol-development" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="protocol-development"><span class="header-section-number">2.1.1</span> Protocol Development</h4>
<p>The team curated a set of equipment and developed a protocol for collecting standardized, reproducible images of faces for research. Following a survey among the ManyFaces members, we constructed the protocol for the collection of a variety of images that would be useful for a broad variety of research questions. Specifically, we included images varying in their standardization of appearance (standardized and unstandardized, e.g., white t-shirt and hair pulled back vs.&nbsp;clothing and hair as worn by the participant/model on that day), viewing angle (frontal, profile, ¾), and facial expression (neutral, natural, angry, disgusted, fearful, happy, sad, surprised).</p>
<p>Prior to beginning data collection, the ManyFaces team pre-tested the protocol to ensure clarity of instructions and consistency of images collected across sites. We revised the protocol to address issues that arose (e.g., revised facial expression elicitation instructions, clarified camera settings). The final protocol can be found at [https://docs.google.com/document/d/1D9TPGXCgTRZi7nqEIg6jb42R4gNQx9L3qasg8tFvu1I/edit?usp=sharing<!--to add to OSF-->] (note that this is a living document that may be updated in future). In sum, the protocol detailed the following:</p>
<section id="image-types." class="level5" data-number="2.1.1.1">
<h5 data-number="2.1.1.1" class="anchored" data-anchor-id="image-types."><span class="header-section-number">2.1.1.1</span> Image Types.</h5>
<p>The protocol defined the categories of images in terms of their standardisation of appearance, viewing angle, and facial expression.</p>
<ul>
<li>Standardisation of Appearance. In the <em>standardised</em> images, models wore a white crew neck t-shirt, had their hair pulled back or covered, included no adornments/accessories (excepting those that could not be removed for cultural reasons), and wore minimal or no makeup (models were informed about this beforehand). In contrast, <em>unstandardised</em> images showed models in their own clothing, with their hair as they came into the lab, and with adornments, except for glasses or anything that obscured the face or neck.<br>
</li>
<li>Viewing Angle. <em>Full frontal portraits</em> showed models facing the camera, <em>left and right profile portraits</em> showed each side of models’ faces in profile, and <em>left and right ¾ portraits</em> showed models’ faces at a 45-degree angle from the axis of the camera.<br>
</li>
<li>Facial Expression. <em>Neutral</em> images were of models refraining from making any facial expression, <em>natural</em> images displayed a natural expression for the model, and each facial expression of emotion displayed the expression the model would make if they were feeling the specified emotion (e.g., happiness).</li>
</ul>
</section>
<section id="image-prioritisation" class="level5" data-number="2.1.1.2">
<h5 data-number="2.1.1.2" class="anchored" data-anchor-id="image-prioritisation"><span class="header-section-number">2.1.1.2</span> Image Prioritisation</h5>
<p>The protocol specified which images to take (i.e., the combinations of appearance standardisation, viewing angle, and facial expression), in what order, and which were most crucial to collect (as voted by the ManyFaces members) if researchers were under time constraints. The order of image collection was as follows, with starred (*) images prioritised/required for each model:</p>
<ul>
<li>Exposure calibration photo*</li>
<li>Identification &amp; calibration photo*</li>
<li>Unstandardized - Natural - Full frontal portrait</li>
<li>Unstandardized - Neutral - Full frontal portrait*</li>
<li>Unstandardized - Neutral - Left and Right profile portraits</li>
<li>Unstandardized - Neutral - Left and Right ¾ portraits</li>
<li>Unstandardized - Happy - Full frontal portrait*</li>
<li>Unstandardized - Happy - Left and Right profile portraits</li>
<li>Unstandardized - Happy - Left and Right ¾ portraits</li>
<li>Unstandardized - All other expressions (in random or appropriate order) - Full frontal portrait</li>
<li>Standardized - Natural - Full frontal portrait</li>
<li>Standardized - Neutral - Full frontal portrait*</li>
<li>Standardized - Neutral - Left and Right profile portraits*</li>
<li>Standardized - Neutral - Left and Right ¾ portraits*</li>
<li>Standardized - Happy - Full frontal portrait*</li>
<li>Standardized - Happy - Left and Right profile portraits</li>
<li>Standardized - Happy - Left and Right ¾ portraits</li>
<li>Standardized - All other expressions (in random or appropriate order) - Full frontal portrait</li>
</ul>
<p>Researchers with additional time could also capture the other expressions at different viewing angles (Left/Right profile, ¾) with either unstandardized or standardized appearance, as well as a video of standardized appearance to test out 3D image capture.</p>
</section>
<section id="equipment" class="level5" data-number="2.1.1.3">
<h5 data-number="2.1.1.3" class="anchored" data-anchor-id="equipment"><span class="header-section-number">2.1.1.3</span> Equipment</h5>
<p>All collaborating sites/labs used the same set of equipment. <!--note the 2 US labs used their own equipment - check whether exact same or comparable--> Images were captured using a Canon EOS 250d (also called Canon EOS Rebel SL3 in some regions) camera with a kit lens (Canon 18-55mm IS STM lens), and lit by an LED Ring light (Fovitec Bi-Colour LED 18” Ring Light) on a stand. For colour calibration, Calibrite ColourChecker Classic card was used. The image collection spaces were to have a white background with a chair for models to sit in. Researchers provided white t-shirts for models to wear for standardised images.</p>
</section>
<section id="setup" class="level5" data-number="2.1.1.4">
<h5 data-number="2.1.1.4" class="anchored" data-anchor-id="setup"><span class="header-section-number">2.1.1.4</span> Setup</h5>
<p>The setup of the room used for image collection at each site required removing or minimising external light sources (e.g., using a windowless room, window blinds, turning off overhead lights) and colour spill. The protocol specified the distances at which to set up the chair for models to sit in relative to the white background and the lighting and camera rig. Camera setup instructions, including the specified shooting and focusing modes, shutter speed, aperture, ISO, white balance, colour space, and file type.</p>
</section>
<section id="procedure" class="level5" data-number="2.1.1.5">
<h5 data-number="2.1.1.5" class="anchored" data-anchor-id="procedure"><span class="header-section-number">2.1.1.5</span> Procedure</h5>
<p>Finally, the protocol detailed how to prepare and position models and how to take each kind of images listed in the image prioritisation section. This included the positioning of the model’s head for each viewing angle and facial expression elicitation instructions.</p>
</section>
</section>
<section id="stimulus-collection" class="level4" data-number="2.1.2">
<h4 data-number="2.1.2" class="anchored" data-anchor-id="stimulus-collection"><span class="header-section-number">2.1.2</span> Stimulus Collection</h4>
<div class="cell-container"><div class="cell-decorator"><pre>In [2]:</pre></div><div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lab_n <span class="ot">&lt;-</span> data_models<span class="sc">$</span>lab_id <span class="sc">|&gt;</span> <span class="fu">unique</span>() <span class="sc">|&gt;</span> <span class="fu">length</span>()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>models_per_site <span class="ot">&lt;-</span> <span class="fu">count</span>(data_models, lab_id)<span class="sc">$</span>n</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>mean_mps <span class="ot">&lt;-</span> <span class="fu">mean</span>(models_per_site) <span class="sc">|&gt;</span> <span class="fu">round</span>(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div>
<p>Following the protocol, 20 labs (1 in Austria, 2 in Brazil, 2 in Canada, 2 in Germany, 1 in Israel, 3 in Malaysia, 1 in Mexico, 1 in the Netherlands, 1 in Serbia, 4 in the UK, and 2 in the US) collected images of an average of 10.6 models per site. As specified by the protocol, researchers collected multiple images of each model, within the time constraints of the study session.</p>
<p>Models furthermore completed a demographic questionnaire [LINK] <!-- PDF is here, add to OSF: https://drive.google.com/file/d/1EkX4jCkwpFb-Cwi9yWge_JrNbus5re6W/view*--> Experimentum, reporting their age, gender, ethnicity, height, and weight. They also reported whether or not they were wearing makeup (specifying what kinds, e.g., foundation, eye makeup, semi-permanent makeup) and whether they had ever experienced anything that could affect the shape of their face (e.g., broken nose, cosmetic surgery or injections, orthodontic work) and specified this if they were willing. Finally, they completed a debriefing questionnaire [LINK] <!-- PDF is here, add to OSF: https://drive.google.com/file/d/1XFlAdcMZAEcC1NGIJeaOHbx3BYPwzYqn/view*--> asking them about their experience of posing for the images (e.g., whether instructions were clear, whether any part of the process was uncomfortable).</p>
</section>
</section>
<section id="results" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="results"><span class="header-section-number">2.2</span> Results</h3>
<section id="images" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="images"><span class="header-section-number">2.2.1</span> Images</h4>
<div class="cell-container"><div class="cell-decorator"><pre>In [3]:</pre></div><div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>model_n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data_models)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div>
<p>211 total models provided their images, following withdrawals and exclusions for poor image quality. <a href="#tbl-images-per-type" class="quarto-xref">Table&nbsp;1</a> details the number of models for each kind of photo. Example images are shown in <a href="#fig-img-examples" class="quarto-xref">Figure&nbsp;1</a>. Images are available for research use and can be requested here: [link] <!--add: link to where the images can be requested--></p>
<div class="cell-container"><div class="cell-decorator"><pre>In [4]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [5]:</pre></div><div><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>emo_levels <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="at">neu =</span> <span class="st">"neutral"</span>, </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">ang =</span> <span class="st">"anger"</span>, </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">dis =</span> <span class="st">"disgust"</span>, </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">fea =</span> <span class="st">"fear"</span>, </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">hap =</span> <span class="st">"happy"</span>, </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">sad =</span> <span class="st">"sad"</span>, </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">sur =</span> <span class="st">"surprised"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>data_models <span class="sc">|&gt;</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(model_id<span class="sc">:</span>unstd_neu) <span class="sc">|&gt;</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(std_ang<span class="sc">:</span>unstd_neu) <span class="sc">|&gt;</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(value <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">|&gt;</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(name) <span class="sc">|&gt;</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate</span>(name, <span class="fu">c</span>(<span class="st">"type"</span>, <span class="st">"emotion"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> type, <span class="at">values_from =</span> n) <span class="sc">|&gt;</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">emotion =</span> <span class="fu">factor</span>(emotion, <span class="fu">names</span>(emo_levels), emo_levels)) <span class="sc">|&gt;</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">arrange</span>(emotion)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-images-per-type" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-images-per-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: The number of images available for each image type. The column <code>std</code> is for standardised images, with neutral clothing, no makeup, and hair pulled back, while <code>unstd</code> is for unstandardised images with natural hair and makeup.
</figcaption>
<div aria-describedby="tbl-images-per-type-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["emotion"],"name":[1],"type":["fct"],"align":["left"]},{"label":["std"],"name":[2],"type":["int"],"align":["right"]},{"label":["unstd"],"name":[3],"type":["int"],"align":["right"]}],"data":[{"1":"neutral","2":"205","3":"177"},{"1":"anger","2":"187","3":"NA"},{"1":"disgust","2":"184","3":"NA"},{"1":"fear","2":"175","3":"NA"},{"1":"happy","2":"199","3":"NA"},{"1":"sad","2":"183","3":"NA"},{"1":"surprised","2":"184","3":"NA"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [6]:</pre></div><div id="cell-fig-img-examples" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-img-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-img-examples-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Example images of each level of standardisation, angle, and expression"><img src="index_files/figure-html/fig-img-examples-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Example images of each level of standardisation, angle, and expression
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="model-demographics" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="model-demographics"><span class="header-section-number">2.2.2</span> Model Demographics</h4>
<p>For data cleaning, we first downloaded and reshaped the raw data from Experimentum. In the next step, we ensured that the models’ gender, race/ethnicity, and units of height and weight were consistently formatted across labs. For gender and race/ethnicity, words presented in languages other than English were recoded to be presented in English (e.g., “mulher” to “female”, “preta” to “Black”). We then classified self-described race/ethnicity into one of seven categories (“White”, “Black”, “Asian”, “Indigenous”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), where possible. Descriptions that could not be clearly sorted into these categories were given “Ambiguous label” (non-entries were recoded as NA). For height, we ensured all data were presented in centimeters, and for weight, we ensured that all data were presented in kilograms. Models could report height and weight in metric or imperial units, so we converted from imperial to metric where required. We also sanity-checked the reported units, assuming heights &gt; 100 to be in centimeters and those &lt; 100 to be in inches, and weights &lt; 80 to be in kilograms. Any non-entries of height or weight for a model was recoded as NA .</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [7]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [8]:</pre></div><div><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-model-demog" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-model-demog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of model demographic info
</figcaption>
<div aria-describedby="tbl-model-demog-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
</section>
</section>
<section id="studyphase-2-validationnorming-data" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="studyphase-2-validationnorming-data"><span class="header-section-number">3</span> Study/phase 2: Validation/norming data</h2>
<p>We next obtained perceptions/ratings of a subset of the photos (front-facing) to validate the emotion expressions (their perceived emotion category and intensity) and collect norming data on central social perceptions, namely perceived attractiveness, dominance, trustworthiness, gender-typicality, memorability, and age. We chose these ratings due to their central importance in the person perception, face recognition, and emotion perception literatures <span class="citation" data-cites="oosterhof_functional_2008 sutherland_social_2013 perrett_your_2017">(e.g., <a href="#ref-oosterhof_functional_2008" role="doc-biblioref">Oosterhof &amp; Todorov, 2008</a>; <a href="#ref-perrett_your_2017" role="doc-biblioref">Perrett, 2017</a>; <a href="#ref-sutherland_social_2013" role="doc-biblioref">Sutherland et al., 2013</a>)</span> <!--add emotion & memorability/recognition refs-->. We preregistered this study on the OSF (<a href="https://osf.io/4d5v9" class="uri">https://osf.io/4d5v9</a>).</p>
<section id="method-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="method-1"><span class="header-section-number">3.1</span> Method</h3>
<section id="image-processing" class="level4" data-number="3.1.1">
<h4 data-number="3.1.1" class="anchored" data-anchor-id="image-processing"><span class="header-section-number">3.1.1</span> Image Processing</h4>
<!--move this section to phase 1?-->
<p>RAW images were processed using webmorphR <span class="citation" data-cites="debruine_reproducible_2022">(<a href="#ref-debruine_reproducible_2022" role="doc-biblioref">DeBruine et al., 2022</a>)</span>, which facilitates scriptable processing of images using imagemagick <span class="citation" data-cites="imagemagick">(<a href="#ref-imagemagick" role="doc-biblioref">ImageMagick Studio LLC, 2024</a>)</span>; a full script of the processing steps is available at [REPO <!--add link-->]. Briefly,</p>
<ul>
<li>Each face was delineated using the Face++ automatic face detection algorithm (see <a href="https://www.faceplusplus.com/" class="uri">https://www.faceplusplus.com/</a>) to generate a 106-point template.</li>
<li>All images were resized to 1000w by 1500h pixels to standardise size (two different RAW formats were used, which resulted in two different image sizes)</li>
<li>The median RGB colour value of a 100×100 pixel patch at the upper left corner was calculated to fill in any edges from alignment.</li>
<li>The image was repositioned and cropped (not rotated or resized) such that
<ul>
<li>The image size was 675w by 900h pixel</li>
<li>point 71 (between the eyes) was relocated to position [.5w, .4h]</li>
</ul></li>
<li>This aligned image was saved as a lossless PNG using imagemagick default settings (e.g., sRGB colour space).</li>
<li>A white balance correction was applied to the resulting images, calculated from the mean RGB values in the 25x25 pixel top-right corner patch (white background) <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>These images were converted to JPEGs with a quality setting of 75 to reduce file size for stimulus display online.</li>
</ul>
<div class="cell-container"><div class="cell-decorator"><pre>In [9]:</pre></div><div id="cell-fig-img-processing" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-img-processing" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-img-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-img-processing-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Visualise some steps of this with a delineated original, aligned version, and white balanced version."><img src="index_files/figure-html/fig-img-processing-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-img-processing-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Visualise some steps of this with a delineated original, aligned version, and white balanced version.
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="stimuli" class="level4" data-number="3.1.2">
<h4 data-number="3.1.2" class="anchored" data-anchor-id="stimuli"><span class="header-section-number">3.1.2</span> Stimuli</h4>
<p>The number of stimuli was determined by the number of models recruited across the 20 research labs and how many models posed for each image type. The maximum number of targets per image type was 205<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, meaning each rater saw up to 205 stimuli. We obtained ratings of the front-facing standardised neutral (n = 205), unstandardised neutral (n = 188), standardised angry (n = 187), standardised disgusted (n = 184), standardised fearful (n = 175), standardised happy (n = 199), standardised sad (n = 183), and standardised surprised (n = 184).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [10]:</pre></div><div id="cell-fig-stim-examples" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-stim-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stim-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-stim-examples-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: An example of each stimulus (could incorporate the N table)"><img src="index_files/figure-html/fig-stim-examples-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stim-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: An example of each stimulus (could incorporate the N table)
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="attention-check-stimuli" class="level4" data-number="3.1.3">
<h4 data-number="3.1.3" class="anchored" data-anchor-id="attention-check-stimuli"><span class="header-section-number">3.1.3</span> Attention Check Stimuli</h4>
<p>Additionally, we created stimuli for attention checks. These were white images with the same size and aspect ratio as the face stimuli, but contained only the written instruction to choose a specific response, (e.g., ‘Choose “fear”’ or ‘Choose “3”’).</p>
</section>
<section id="measures" class="level4" data-number="3.1.4">
<h4 data-number="3.1.4" class="anchored" data-anchor-id="measures"><span class="header-section-number">3.1.4</span> Measures</h4>
<!--can put screenshots of example trials from experimentum on OSF-->
<section id="standardised-neutral-faces" class="level5" data-number="3.1.4.1">
<h5 data-number="3.1.4.1" class="anchored" data-anchor-id="standardised-neutral-faces"><span class="header-section-number">3.1.4.1</span> Standardised Neutral Faces</h5>
<section id="trait-ratings" class="level6" data-number="3.1.4.1.1">
<h6 data-number="3.1.4.1.1" class="anchored" data-anchor-id="trait-ratings"><span class="header-section-number">3.1.4.1.1</span> Trait Ratings</h6>
<p>We obtained ratings of faces’ attractiveness, dominance, trustworthiness, memorability, and gender-typicality (‘How attractive [dominant, trustworthy, memorable, gender-typical] does this person look?’). Ratings were on scales ranging from 1 (<em>not at all</em>) to 7 (<em>very</em>).</p>
</section>
<section id="demographic-impressions" class="level6" data-number="3.1.4.1.2">
<h6 data-number="3.1.4.1.2" class="anchored" data-anchor-id="demographic-impressions"><span class="header-section-number">3.1.4.1.2</span> Demographic Impressions</h6>
<p>We obtained ratings of faces’ perceived age (‘How old does this person look?’), with responses collected in 5-year ranges/brackets (i.e., 16-20, 21-25, …, 76-80, 81+).</p>
<!--still to add perceived race/ethnicity-->
</section>
</section>
<section id="unstandardised-neutral-faces" class="level5" data-number="3.1.4.2">
<h5 data-number="3.1.4.2" class="anchored" data-anchor-id="unstandardised-neutral-faces"><span class="header-section-number">3.1.4.2</span> Unstandardised Neutral Faces</h5>
<section id="trait-ratings-1" class="level6" data-number="3.1.4.2.1">
<h6 data-number="3.1.4.2.1" class="anchored" data-anchor-id="trait-ratings-1"><span class="header-section-number">3.1.4.2.1</span> Trait Ratings</h6>
<p>We obtained ratings of faces’ attractiveness, dominance, and trustworthiness on scales ranging from 1 (<em>not at all</em>) to 7 (<em>very</em>).</p>
</section>
</section>
<section id="standardised-emotional-faces" class="level5" data-number="3.1.4.3">
<h5 data-number="3.1.4.3" class="anchored" data-anchor-id="standardised-emotional-faces"><span class="header-section-number">3.1.4.3</span> Standardised Emotional Faces</h5>
<section id="emotion-categorisation" class="level6" data-number="3.1.4.3.1">
<h6 data-number="3.1.4.3.1" class="anchored" data-anchor-id="emotion-categorisation"><span class="header-section-number">3.1.4.3.1</span> Emotion Categorisation</h6>
<p>We obtained impressions of the emotion each person was expressing (‘What emotion is this person expressing?’), choosing one from: <em>anger, disgust, fear, happiness, sadness, surprise, other</em>. Here, raters categorized a counterbalanced mixture of expressions (from one of six counterbalanced conditions) rather than faces all showing the same expression. The 201 identities with emotion images were divided into six groups of up to 34 images, and each counterbalanced condition showed a different emotion for each of the six groups, such that no identity was shown more than once to each rater. Since not all identities had all six emotions, the number of images in each counterbalanced condition ranged from 179 to 193.</p>
</section>
<section id="emotion-intensity-ratings" class="level6" data-number="3.1.4.3.2">
<h6 data-number="3.1.4.3.2" class="anchored" data-anchor-id="emotion-intensity-ratings"><span class="header-section-number">3.1.4.3.2</span> Emotion Intensity Ratings</h6>
<p>We obtained ratings of how intensely faces expressed each intended emotion (‘How intensely is this person expressing anger [disgust, fear, happiness, sadness, surprise]?’) from 1 (<em>not at all)</em> to 7 (<em>very</em>). Here, raters only rated all faces showing one emotion expression (e.g., all angry faces) and rated the intensity only of the intended expression (e.g., angry faces only rated on anger intensity).</p>
</section>
</section>
</section>
<section id="procedure-1" class="level4" data-number="3.1.5">
<h4 data-number="3.1.5" class="anchored" data-anchor-id="procedure-1"><span class="header-section-number">3.1.5</span> Procedure</h4>
<p>We collected ratings via Experimentum <span class="citation" data-cites="debruine_experimentum_2020">(<a href="#ref-debruine_experimentum_2020" role="doc-biblioref">DeBruine et al., 2020</a>)</span>; structure files for the exact experimental setup are available at [REPO]<!--add link-->. After a brief introduction to the study and online informed consent, each participant (rater) was randomly allocated to one of the ratings (e.g., rating all 205 standardised neutral faces on how memorable they look). All available faces were displayed one at a time, in a randomised order for each rater. The question/prompt and response scale remained visible at the top of the screen, above the photo throughout the study. The study automatically progressed to the next trial once the rater responded by clicking on the response scale. There was no time limit to provide a response. We included seven attention checks embedded in the study, which directed raters to provide a specific response.</p>
<p>Following rating or categorising all stimuli, raters self-reported their gender, age, race/ethnicity, country of residence, and device type used for the study (desktop, laptop, tablet, mobile)<!--add doc to OSF-->. They also completed an honesty/attention check question, asking them if they engaged with the study seriously, with assurance of payment regardless of response (choosing from ‘no, I was not really paying attention’ and ‘yes, I tried to give my authentic first impressions’).</p>
<p>We recruited fluent English-speaking raters through Prolific <!--add prolific inclusion/exclusion criteria-->. We collected all data in May 2025.</p>
</section>
<section id="participants" class="level4" data-number="3.1.6">
<h4 data-number="3.1.6" class="anchored" data-anchor-id="participants"><span class="header-section-number">3.1.6</span> Participants</h4>
<p>We aimed to collect 100 raters per rating condition to achieve stable averages and allow for exclusions <span class="citation" data-cites="hehman_assessing_2025">(<a href="#ref-hehman_assessing_2025" role="doc-biblioref">Hehman et al., 2025</a>)</span>, totalling 2100 raters. Altogether, 2115 raters completed the study. See Results for exclusions and demographics of the final sample.</p>
</section>
</section>
<section id="results-1" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="results-1"><span class="header-section-number">3.2</span> Results</h3>
<section id="data-cleaning-and-exclusions" class="level4" data-number="3.2.1">
<h4 data-number="3.2.1" class="anchored" data-anchor-id="data-cleaning-and-exclusions"><span class="header-section-number">3.2.1</span> Data Cleaning and Exclusions</h4>
<p>In the raters’ demographic questionnaire, we standardized participants’ recording of their race/ethnicity similarly to the models’ by recoding their inputs into one of seven categories (“White”, “Black”, “Asian”, “Indigenous/Pacific Islander”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), or as “Ambiguous label” when this was not possible. Any non-entries were recoded as NA.</p>
<p>A total of 2115 raters completed 2158 rating or categorisation tasks. We found that some raters did not complete all trials in tasks, some raters completed more than one task, and some raters completed more than the maximum number of trials in a task (likely by restarting the study or bypassing the back button block). Therefore, we removed incomplete tasks from our data, retained raters’ first complete tasks, and filtered our duplicate trials by keeping only raters’ first ratings for a duplicated trial. After these exclusions, and before implementing the pre-registered plan for data exclusions, we had complete and clean data from 1936 raters.</p>
<p>Our pre-registered plan for data exclusions included removing raters who gave overly consistent responses, committed overly fast responses, self-reported not taking the study seriously when asked whether or not they completed the study authentically, and failed attention checks. In total, we excluded 49 raters for our pre-registered reasons for data exclusions. <a href="#tbl-exclusions" class="quarto-xref">Table&nbsp;3</a> shows the number of raters we excluded for each of our reasons.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [11]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [12]:</pre></div><div><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: Multiple raters met more than one exclusion criterion; therefore, the counts for individual criteria do not sum to the total number of raters excluded.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-exclusions" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-exclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: The number of raters excluded from analysis.
</figcaption>
<div aria-describedby="tbl-exclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>We defined overly consistent responses as those raters who responded to at least 90% of the trials identically. We defined overly fast responses as those raters whose median reaction time fell below the 1st percentile of the overall distribution of median reaction times (see Figure 1 for the distribution of median reaction times). For our attention checks, the threshold for inclusion was to accurately complete six or more attention checks (i.e., participants were excluded if they failed more than one of the checks). After these exclusions, we had 1899 <!--update N--> raters in our sample (see <a href="#tbl-raters-per-task" class="quarto-xref">Table&nbsp;4</a> for the number of raters per task).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [13]:</pre></div><div id="cell-fig-rt-distribution" class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-rt-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rt-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-rt-distribution-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Median reaction time distribution"><img src="index_files/figure-html/fig-rt-distribution-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rt-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Median reaction time distribution
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [14]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [15]:</pre></div><div><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">a =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-per-task" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-per-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Number of raters per task
</figcaption>
<div aria-describedby="tbl-raters-per-task-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["a"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>Anonymous data are available on the OSF: [LINK] <!--[add link--></p>
</section>
<section id="rater-demographics" class="level4" data-number="3.2.2">
<h4 data-number="3.2.2" class="anchored" data-anchor-id="rater-demographics"><span class="header-section-number">3.2.2</span> Rater Demographics</h4>
<p>We collected the following demographic data from our raters (N = 1676 of 1899<!--update N--> provided data): Age, gender, residence, ethnicity, and devices on which the ratings were completed. See <a href="#fig-gender-age" class="quarto-xref">Figure&nbsp;5</a> and Tables 3-5 for demographic information.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [16]:</pre></div><div id="cell-fig-gender-age" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>data_qu <span class="sc">|&gt;</span> </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> age, <span class="at">fill =</span> gender)) <span class="sc">+</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"hotpink"</span>, <span class="st">"dodgerblue"</span>, <span class="st">"darkorchid"</span>, <span class="st">"gray"</span>)) <span class="sc">+</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">fill  =</span> <span class="fu">guide_legend</span>(<span class="at">position =</span> <span class="st">"inside"</span>)) <span class="sc">+</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position.inside =</span> <span class="fu">c</span>(.<span class="dv">7</span>, .<span class="dv">7</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-gender-age" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gender-age-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-gender-age-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Histogram of rater age, separated by gender"><img src="index_files/figure-html/fig-gender-age-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gender-age-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Histogram of rater age, separated by gender
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [17]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [18]:</pre></div><div><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-country" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-country-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Rater country of residence
</figcaption>
<div aria-describedby="tbl-raters-country-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [19]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [20]:</pre></div><div><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-eth" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-eth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Rater race/ethnicity
</figcaption>
<div aria-describedby="tbl-raters-eth-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [21]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [22]:</pre></div><div><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-raters-dev" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-raters-dev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: Devices used by raters to complete the study
</figcaption>
<div aria-describedby="tbl-raters-dev-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
<section id="agreement-indicators-for-the-ratings" class="level4" data-number="3.2.3">
<h4 data-number="3.2.3" class="anchored" data-anchor-id="agreement-indicators-for-the-ratings"><span class="header-section-number">3.2.3</span> Agreement Indicators for the Ratings</h4>
<section id="standardized-trait-ratings" class="level5" data-number="3.2.3.1">
<h5 data-number="3.2.3.1" class="anchored" data-anchor-id="standardized-trait-ratings"><span class="header-section-number">3.2.3.1</span> Standardized Trait Ratings</h5>
<p>In the first step, we calculated intraclass correlations for traits observed across tasks for our standardized images. The number of raters ranged from 83 to 94 for these traits. The average reliability across raters was very good for rating attractiveness, dominance, trustworthiness, and gender-typicality, but poorer for rating memorability (see <a href="#tbl-std-neu-agree" class="quarto-xref">Table&nbsp;8</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [23]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [24]:</pre></div><div><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-std-neu-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-std-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: Agreement for ratings of standardized neutral faces
</figcaption>
<div aria-describedby="tbl-std-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<p>Next, we examined the number of raters required for trait ratings to reach stable levels of reliability defined as ICC (2,k) values of 0.75 and 0.90 (see Figure 3). Attractiveness ratings stabilized with the fewest participants, reaching an ICC (2,k) of 0.75 at approximately 18 raters. It was also the only trait to exceed an ICC (2,k) of 0.90, which occurred with 50 raters. Ratings for dominance, trustworthiness, and gender-typicality reached an ICC (2,k) of 0.75 with approximately 35-40 raters (see <a href="#fig-icc-cos" class="quarto-xref">Figure&nbsp;6</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [25]:</pre></div><div id="cell-fig-icc-cos" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-icc-cos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-icc-cos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-icc-cos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Corridors of stability for ICCs"><img src="index_files/figure-html/fig-icc-cos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-icc-cos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Corridors of stability for ICCs
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>For checking internal consistency of our ratings, we used both Cronbach’s alpha (α) and McDonald’s omega total (ωt) (see <a href="#tbl-std-neu-agree" class="quarto-xref">Table&nbsp;8</a>). For four of the five traits, both α and ωt exceeded 0.9, indicating excellent internal consistency. For the fifth trait (memorability), α was lower (0.65), indicating doubtful internal consistency when assuming equal item contributions (tau-equivalence) in the ratings. However, ωt for memorability was substantially higher (0.87), indicating good internal consistency after taking into account variance among item or rater contributions.</p>
</section>
<section id="unstandardized-trait-ratings" class="level5" data-number="3.2.3.2">
<h5 data-number="3.2.3.2" class="anchored" data-anchor-id="unstandardized-trait-ratings"><span class="header-section-number">3.2.3.2</span> Unstandardized Trait Ratings</h5>
<p>As with our standardized trait ratings, but for three rather than five traits, we calculated intraclass correlations for ratings to our unstandardized images observed across rating tasks. The number of raters ranged from 84 to 95 for these traits. The The average reliability across raters was very good for all trait ratings (see <a href="#tbl-unstd-neu-agree" class="quarto-xref">Table&nbsp;9</a>). All three traits also demonstrated excellent internal consistency (see <a href="#tbl-unstd-neu-agree" class="quarto-xref">Table&nbsp;9</a>) with Cronbach’s alpha (α) ranging from 0.88 to 0.95 and McDonald’s omega total (ωt) ranging from 0.90 to 0.96.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [26]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [27]:</pre></div><div><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-unstd-neu-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-unstd-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: Agreement for ratings of unstandardized neutral faces
</figcaption>
<div aria-describedby="tbl-unstd-neu-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
<section id="emotion-intensity-ratings-1" class="level5" data-number="3.2.3.3">
<h5 data-number="3.2.3.3" class="anchored" data-anchor-id="emotion-intensity-ratings-1"><span class="header-section-number">3.2.3.3</span> Emotion Intensity Ratings</h5>
<p>Next, we calculated the intraclass correlations and internal consistency metrics for our emotion intensity ratings. The number of raters ranged from 84 to 103 for our six emotions. The average reliability across raters was excellent for all of the emotions, with ICC (2,k) ranging from 0.95 to 0.98. Internal consistency was also excellent for all emotions in terms of Cronbach’s alpha (α = 0.97 to 0.99) and McDonald’s omega total (ωt = 0.97 to 0.99; see <a href="#tbl-emo-agree" class="quarto-xref">Table&nbsp;10</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [28]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [29]:</pre></div><div><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: ICCs values are ICC (2,k) values.</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-emo-agree" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-emo-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: Agreement for ratings of emotional faces’ emotion intensity
</figcaption>
<div aria-describedby="tbl-emo-agree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
</section>
</section>
<section id="points-of-stability" class="level4" data-number="3.2.4">
<h4 data-number="3.2.4" class="anchored" data-anchor-id="points-of-stability"><span class="header-section-number">3.2.4</span> Points of Stability</h4>
<p>To determine the number of raters required for stable ratings, we computed the point of stability (Hehman et al., 2025). This approach estimates the smallest sample size at which mean ratings stabilize by exceeding a cosine similarity threshold of 0.5 with the full-sample mean. As shown in Figures 4-6, the ratings to the standardized images reached stability between 37 and 45 raters, the ratings to the unstandardized images reached stability between 37 and 42 raters, and the ratings for the emotion intensities reached stability between 29 and 50 raters.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [30]:</pre></div><div id="cell-fig-std-neu-pos" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-std-neu-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-std-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-std-neu-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Points of stability for ratings of standardised neutral faces"><img src="index_files/figure-html/fig-std-neu-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-std-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Points of stability for ratings of standardised neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [31]:</pre></div><div id="cell-fig-unstd-neu-pos" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-unstd-neu-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unstd-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-unstd-neu-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Points of stability for ratings of unstandardised neutral faces"><img src="index_files/figure-html/fig-unstd-neu-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unstd-neu-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Points of stability for ratings of unstandardised neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [32]:</pre></div><div id="cell-fig-emo-pos" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-emo-pos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emo-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-emo-pos-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Points of stability for emotion intensity ratings of standardised emotional faces of each expression"><img src="index_files/figure-html/fig-emo-pos-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emo-pos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Points of stability for emotion intensity ratings of standardised emotional faces of each expression
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="descriptive-statistics" class="level4" data-number="3.2.5">
<h4 data-number="3.2.5" class="anchored" data-anchor-id="descriptive-statistics"><span class="header-section-number">3.2.5</span> Descriptive Statistics</h4>
<p>In this section, we report the descriptive statistics for ratings of the neutral standardized images, the neutral unstandardized images, the emotion categorization task, the intensity of expressed emotions, and the ages of the models.</p>
<section id="standardized-neutral-trait-ratings" class="level5" data-number="3.2.5.1">
<h5 data-number="3.2.5.1" class="anchored" data-anchor-id="standardized-neutral-trait-ratings"><span class="header-section-number">3.2.5.1</span> Standardized Neutral Trait Ratings</h5>
<p>Trustworthiness, dominance, and memorability ratings were approximately normally distributed and showed similar patterns of central tendency and variance. In contrast, attractiveness ratings were somewhat right-skewed, reflected in a relatively lower mean, while gender-typicality ratings were left-skewed, with a relatively higher mean (see <a href="#tbl-std-neu-desc" class="quarto-xref">Table&nbsp;11</a> for descriptive statistics and <a href="#fig-std-neu-hist" class="quarto-xref">Figure&nbsp;10</a> for rating distributions).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [33]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [34]:</pre></div><div><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-std-neu-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-std-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11: Descriptive statistics for ratings of standardized neutral faces
</figcaption>
<div aria-describedby="tbl-std-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [35]:</pre></div><div id="cell-fig-std-neu-hist" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-std-neu-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-std-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-std-neu-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Histograms for ratings of standardized neutral faces"><img src="index_files/figure-html/fig-std-neu-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-std-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Histograms for ratings of standardized neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>A key aim of this study was to provide normative data for each face model in the database. To visualize rating patterns, we created heatmaps with rating values on the x-axis and face models (separated by contributing lab site) on the y-axis. These heatmaps illustrate how the distribution of ratings varied across models and traits, with lighter tiles indicating that more raters gave a particular rating to that model (see Appendix) — capturing both between-model differences within a trait and within-model differences across traits.</p>
</section>
<section id="unstandardized-neutral-trait-ratings" class="level5" data-number="3.2.5.2">
<h5 data-number="3.2.5.2" class="anchored" data-anchor-id="unstandardized-neutral-trait-ratings"><span class="header-section-number">3.2.5.2</span> Unstandardized Neutral Trait Ratings</h5>
<p>The ratings for dominance were approximately normally distributed, whereas the ratings for attractiveness were slightly skewed right and the ratings for trustworthiness were slightly skewed left, each reflected in their relatively low and high mean ratings (see <a href="#tbl-unstd-neu-desc" class="quarto-xref">Table&nbsp;12</a> for descriptive statistics and <a href="#fig-unstd-neu-hist" class="quarto-xref">Figure&nbsp;11</a> for rating distributions).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [36]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [37]:</pre></div><div><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-unstd-neu-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-unstd-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12: Descriptive statistics for ratings of unstandardized neutral faces
</figcaption>
<div aria-describedby="tbl-unstd-neu-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [38]:</pre></div><div id="cell-fig-unstd-neu-hist" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-unstd-neu-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-unstd-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-unstd-neu-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Histograms for ratings of unstandardized neutral faces"><img src="index_files/figure-html/fig-unstd-neu-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-unstd-neu-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Histograms for ratings of unstandardized neutral faces
</figcaption>
</figure>
</div>
</div>
</div></div>
<p>As with the ratings of the standardized images, we produced heatmaps for each of the three traits capturing between-model differences within a trait and within-model differences across the traits (see Appendix).</p>
</section>
<section id="emotion-categorization-and-intensity-ratings." class="level5" data-number="3.2.5.3">
<h5 data-number="3.2.5.3" class="anchored" data-anchor-id="emotion-categorization-and-intensity-ratings."><span class="header-section-number">3.2.5.3</span> Emotion Categorization and Intensity Ratings.</h5>
<p>Next, we explored raters’ emotion categorizations and perceived emotion intensity expressed by our models. Raters’ categorizations generally aligned with the models’ expression of emotion, with the greatest alignment was observed for expression of happiness, and the least for fear (<a href="#tbl-emo-desc" class="quarto-xref">Table&nbsp;13</a>, <a href="#fig-emo-freq" class="quarto-xref">Figure&nbsp;12</a>).</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [39]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [40]:</pre></div><div><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tbl-subcap: Note. Italicised values indicate correct categorisations (categorisations matching intended model expression).</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-emo-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-emo-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;13: Emotion categorisation proportions
</figcaption>
<div aria-describedby="tbl-emo-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [41]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [42]:</pre></div><div class=""><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="fig-emo-freq" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<div class="cell-output-display">
<div id="fig-emo-freq" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-emo-freq-1.png" class="lightbox" data-gallery="fig-emo-freq" title="Figure&nbsp;12&nbsp;(a): Downward arrows indicate correct categorisations (categorisations matching intended model expression). A=anger, D=disgust, F=fear, H=happiness, S=sadness, U=surprise, O=other"><img src="index_files/figure-html/fig-emo-freq-1.png" class="img-fluid figure-img" data-ref-parent="fig-emo-freq" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Downward arrows indicate correct categorisations (categorisations matching intended model expression). A=anger, D=disgust, F=fear, H=happiness, S=sadness, U=surprise, O=other
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-emo-freq-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Emotion categorisation frequencies
</figcaption>
</figure>
</div></div></div>
</div></div>
<p>Next, we calculated the average ratings of emotion intensity for each emotion (<a href="#tbl-intensity-desc" class="quarto-xref">Table&nbsp;14</a>). These ratings were very similar, on average, between the different emotions except for happiness, which was rated with greater intensity than all other emotions. On average, these ratings were located around the middle of the 7-point scale. <a href="#fig-intensity-hist" class="quarto-xref">Figure&nbsp;13</a> shows the distribution of ratings on the scale for each emotion.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [43]:</pre></div><div class="cell">

<div class="cell-container"><div class="cell-decorator"><pre>In [44]:</pre></div><div><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div><div id="tbl-intensity-desc" class="cell quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-intensity-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14: Descriptive statistics for emotion intensity ratings
</figcaption>
<div aria-describedby="tbl-intensity-desc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["X1.5"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"1"},{"1":"2"},{"1":"3"},{"1":"4"},{"1":"5"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</figure>
</div></div></div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [45]:</pre></div><div id="cell-fig-intensity-hist" class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-intensity-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intensity-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-intensity-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Histograms of emotion intensity ratings"><img src="index_files/figure-html/fig-intensity-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intensity-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Histograms of emotion intensity ratings
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
<section id="perceived-age" class="level5" data-number="3.2.5.4">
<h5 data-number="3.2.5.4" class="anchored" data-anchor-id="perceived-age"><span class="header-section-number">3.2.5.4</span> Perceived Age</h5>
<p>The distribution of raters’ perceptions of models’ ages is shown in <a href="#fig-age-hist" class="quarto-xref">Figure&nbsp;14</a>. 26-30 years was the most-chosen age across raters and models. The correlation between models’ mode perceived age and actual age appears in <a href="#fig-age-corr" class="quarto-xref">Figure&nbsp;15</a>, and heatmaps of perceived age for each model appear in the Appendix.</p>
<div class="cell-container"><div class="cell-decorator"><pre>In [46]:</pre></div><div id="cell-fig-age-hist" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-age-hist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-age-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-age-hist-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Histogram of perceived age"><img src="index_files/figure-html/fig-age-hist-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-age-hist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Histogram of perceived age
</figcaption>
</figure>
</div>
</div>
</div></div>
<div class="cell-container"><div class="cell-decorator"><pre>In [47]:</pre></div><div id="cell-fig-age-corr" class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-age-corr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-age-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="">
<a href="index_files/figure-html/fig-age-corr-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Correlation between models’ actual age and mode perceived age"><img src="index_files/figure-html/fig-age-corr-1.png" class="img-fluid figure-img" width="672"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-age-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Correlation between models’ actual age and mode perceived age
</figcaption>
</figure>
</div>
</div>
</div></div>
</section>
</section>
<section id="exploratory-inferential-statistics" class="level4" data-number="3.2.6">
<h4 data-number="3.2.6" class="anchored" data-anchor-id="exploratory-inferential-statistics"><span class="header-section-number">3.2.6</span> Exploratory Inferential Statistics</h4>
<!-- to add -->
<section id="standardized-vs-unstandardized-neutral-trait-ratings" class="level5" data-number="3.2.6.1">
<h5 data-number="3.2.6.1" class="anchored" data-anchor-id="standardized-vs-unstandardized-neutral-trait-ratings"><span class="header-section-number">3.2.6.1</span> Standardized vs Unstandardized Neutral Trait Ratings</h5>
<p>Compare attractiveness, dominance, and trustworthiness ratings across level of standardization for models w/ both kinds of photo (3 t-tests)</p>
</section>
<section id="emotion-categorizations" class="level5" data-number="3.2.6.2">
<h5 data-number="3.2.6.2" class="anchored" data-anchor-id="emotion-categorizations"><span class="header-section-number">3.2.6.2</span> Emotion Categorizations</h5>
<p>Compare the ‘correct’ proportions across expression type (i.e., to show that happiness is perceived significantly more correctly &amp; fear significantly more inaccurately than the others)</p>
</section>
<section id="emotion-intensity-ratings-2" class="level5" data-number="3.2.6.3">
<h5 data-number="3.2.6.3" class="anchored" data-anchor-id="emotion-intensity-ratings-2"><span class="header-section-number">3.2.6.3</span> Emotion Intensity Ratings</h5>
<p>Compare emotion intensity ratings across expression type (i.e., to show that happiness appears most intense)</p>
</section>
</section>
</section>
</section>
<section id="discussion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">4</span> Discussion</h2>
<section id="the-value-of-the-current-work" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="the-value-of-the-current-work"><span class="header-section-number">4.1</span> The value of the current work</h3>
<p>This database provides a uniquely diverse face image stimulus set in terms of model nationality and image variety (appearance standardization, viewing angle, and facial expression). The diversity of images within the database makes it potentially useful to address a variety of research questions in multiple subfields of psychology (e.g., face recognition, emotion). It accomplishes this while being transparent and reproducible via the openly available protocol. Additionally, the database need not remain static: Other researchers can follow the protocol and add to the database, further increasing its sample size and diversity. This protocol can also serve as a template for researchers interested in collecting stimulus images, in terms of highlighting the kinds of details that need to be considered and documented)</p>
<p>The data associated with the images are also valuable. First, we collected models’ self-reported demographic information, which is useful for various kinds of research questions. The validation/norming data we collected also provide a useful starting point for researchers interested in using the images, for example enabling researchers to choose subsets of images most suited to address their research questions. These data also provide information about the minimum number of raters needed for reliable mean ratings for different judgments <span class="citation" data-cites="hehman_assessing_2025">(see <a href="#ref-hehman_assessing_2025" role="doc-biblioref">Hehman et al., 2025</a>)</span>.</p>
<p>This work also provides proof of concept that various labs, all following the same protocol and using the same equipment, can take comparable images to form a coherent face database. This broadens opportunities for future image database collection: A database does not need to all be collected by one lab in one location.</p>
</section>
<section id="reflections-on-the-process-limitations" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="reflections-on-the-process-limitations"><span class="header-section-number">4.2</span> Reflections on the Process &amp; Limitations</h3>
<p>The leadership team found that having a broad variety of areas of expertise and diverse research backgrounds within the broader team was invaluable in both developing the database and in troubleshooting issues. We moreover found that setting out expectations at the start of the project through a collaboration agreement was essential. However, we ran into various issues throughout the process, which are common to Big Team Science initiatives. For example, with so many labs and individual researchers involved, timelines for completion necessarily stretched. This made momentum difficult to maintain at times. Planning in generous buffer time for such large-scale projects and managing team members’ expectations around timelines is therefore essential. We also faced difficulty in finding an optimal way to communicate effectively with all team members, given varying preferences (e.g., via email vs.&nbsp;other team communication platforms). It is perhaps worth surveying members of a big team at project start about communication possibilities, as well as outlining more specific communication expectations in a collaboration agreement.</p>
<p>After Phase 1, we surveyed the ManyFaces research team to provide feedback on the process of collecting images using the protocol. Team members commonly expressed the desire for additional concise guidance in following the protocol, including an additional abbreviated protocol with numbered steps to follow during data collection and a video tutorial (in addition to the provided illustrative images) showing the necessary steps for researchers unfamiliar with camera equipment. These are requests that we can incorporate into future protocol updates. Doing so could both make the process easier for researchers and minimize errors and deviations from the protocol, ensuring greater consistency between sites.</p>
<p>The major issue raised by the research team was the difficulty of eliciting facial expressions as described in the protocol. The protocol took a straightforward instructive approach to emotion elicitation (e.g., asking participants to face the camera with the expression they would make if they were feeling happy), following consultation with emotion experts in ManyFaces and discussion of feasibility with the wider team. However, models self-reported difficulty with the facial expression posing, in line with researchers’ feedback. The emotion validation data also indicate that models struggled to pose the facial expressions of emotion. The data furthermore suggest that perceivers had difficulty identifying emotions from these posed facial expressions. In support of this idea, recent research shows posed expressions to not appear genuine and reveals differing perceptions of posed and spontaneous expressions <span class="citation" data-cites="dawel_perceived_2017 dawel_faking_2025">(<a href="#ref-dawel_perceived_2017" role="doc-biblioref">Dawel et al., 2017</a>, <a href="#ref-dawel_faking_2025" role="doc-biblioref">2025</a>)</span>. Future work, including possible updates to our own protocol, may therefore focus on developing a better framework to elicit facial expressions. This could include taking inspiration from databases of naturally-induced emotion <span class="citation" data-cites="miolla_padova_2023 sneddon_belfast_2012">(e.g., <a href="#ref-miolla_padova_2023" role="doc-biblioref">Miolla et al., 2023</a>; <a href="#ref-sneddon_belfast_2012" role="doc-biblioref">Sneddon et al., 2012</a>)</span> and considering potential cultural differences and specific local needs.</p>
<p>The research team also raised several comments about the provided and not-provided equipment, specifically the flimsiness of the light stand and the missing standardized backdrop (due to shipping constraints, participating researchers were to source a white background, e.g., a wall, seamless fabric, or paper). It is worth highlighting that the images were captured with equipment selected due to several practical constraints: budget, university-approved vendors, shipping logistics, interoperability, and ease of use for non-experts in varying conditions. The available project budget and administrative restrictions on approved vendors (their offerings and stock) for the University of Glasgow primarily limited the attainable camera, lens, and lighting setup. In general, we opted to create a setup that would not be prohibitively expensive to acquire, could be shipped in a single parcel, and would be compatible (substitutable) with equipment other researchers may have available. The last consideration was creating a setup with the fewest degrees of freedom, and thus less room for error. Therefore, we opted to use an LED ring light with a camera mounted inside the ring in landscape orientation on a single stand (vs.&nbsp;portrait orientation using a different kind of mount or separate stands for the camera and light). Ring lights became ubiquitous and easy to acquire in recent years; they only need to be positioned square, level, and in front of the sitter. Other, more complex setups can be created; however, the price, mobility, and repeatability of such setups may represent major constraints to consistent and reliable image collection. It is therefore worth considering trade-offs between different kinds of equipment and setups in future work.</p>
<p>In addition to our choices of equipment, we also made certain pragmatic choices during image processing that future work could improve on. For example, we white-balanced the images rather than fully colour-correcting them, as we were able to create a fully reproducible scripted method for the former process, but not the latter. This was due to limitations such as varied colour checker chart placement and orientation in models’ photos and a lack of expertise on the research team to work around this in a reproducible way. We therefore opted to simply white-balance to keep the process entirely open and reproducible, rather than fully colour-correct using a manual and not fully reproducible method. The process of aligning and sizing the images was also driven by cropping needs. That is, due to the landscape orientation of the photos, there was limited vertical space that we could crop, constraining the possibilities for face alignment and image size. Future work could consider using alternative equipment setups to enable capture images in portrait mode. Variation in faces’ size in the images, due to some deviations from the distances specified in the protocol, also affected the image processing steps. This could be addressed in future work by more clearly highlighting not only the key aspects of the protocol that should be kept constant, but also why these aspects should be constant (i.e., clarifying the reasoning to all team members).</p>
<p>Finally, we collected images of 211 models, but no single image type included images of all models. Rather, the maximum number of models represented in a single image type was 205 images (standardized neutral, front-facing). This led to missing perceived age/ethnicity data for six models who had unstandardized but not standardized neutral images available.</p>
</section>
<section id="conclusion" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4.3</span> Conclusion</h3>
<p>Here, we introduce a new diverse face image database, which will be useful to researchers interested in a variety of questions related to social perception, person recognition, and vision science. We demonstrate that a cohesive database can be compiled across a variety of sites, opening doors for future additions to this database and the development of future multi-lab databases.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-burton_glasgow_2010" class="csl-entry" role="listitem">
Burton, A. M., White, D., &amp; McNeill, A. (2010). The <span>Glasgow</span> <span>Face</span> <span>Matching</span> <span>Test</span>. <em>Behavior Research Methods</em>, <em>42</em>(1), 286–291. <a href="https://doi.org/10.3758/BRM.42.1.286">https://doi.org/10.3758/BRM.42.1.286</a>
</div>
<div id="ref-chen_broadening_2021" class="csl-entry" role="listitem">
Chen, J. M., Norman, J. B., &amp; Nam, Y. (2021). Broadening the stimulus set: <span>Introducing</span> the <span>American</span> <span>Multiracial</span> <span>Faces</span> <span>Database</span>. <em>Behavior Research Methods</em>, <em>53</em>(1), 371–389. <a href="https://doi.org/10.3758/s13428-020-01447-8">https://doi.org/10.3758/s13428-020-01447-8</a>
</div>
<div id="ref-cook_why_2021" class="csl-entry" role="listitem">
Cook, R., &amp; Over, H. (2021). Why is the literature on first impressions so focused on <span>White</span> faces? <em>Royal Society Open Science</em>, <em>8</em>(9), 211146. <a href="https://doi.org/10.1098/rsos.211146">https://doi.org/10.1098/rsos.211146</a>
</div>
<div id="ref-dawel_faking_2025" class="csl-entry" role="listitem">
Dawel, A., Krumhuber, E. G., &amp; Palermo, R. (2025). Faking <span>It</span> <span>Isn</span>’t <span>Making</span> <span>It</span>: <span>Research</span> <span>Needs</span> <span>Spontaneous</span> and <span>Naturalistic</span> <span>Facial</span> <span>Expressions</span>. <em>Affective Science</em>. <a href="https://doi.org/10.1007/s42761-025-00320-1">https://doi.org/10.1007/s42761-025-00320-1</a>
</div>
<div id="ref-dawel_perceived_2017" class="csl-entry" role="listitem">
Dawel, A., Wright, L., Irons, J., Dumbleton, R., Palermo, R., O’Kearney, R., &amp; McKone, E. (2017). Perceived emotion genuineness: Normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets. <em>Behavior Research Methods</em>, <em>49</em>(4), 1539–1562. <a href="https://doi.org/10.3758/s13428-016-0813-2">https://doi.org/10.3758/s13428-016-0813-2</a>
</div>
<div id="ref-debruine_reproducible_2022" class="csl-entry" role="listitem">
DeBruine, L. M., Holzleitner, I. J., Tiddeman, B., &amp; Jones, B. C. (2022). <em>Reproducible <span>Methods</span> for <span>Face</span> <span>Research</span></em>. PsyArXiv. <a href="https://doi.org/10.31234/osf.io/j2754">https://doi.org/10.31234/osf.io/j2754</a>
</div>
<div id="ref-debruine_experimentum_2020" class="csl-entry" role="listitem">
DeBruine, L. M., Lai, R., Jones, B. C., Abdullah, R., &amp; Mahrholz, G. (2020). <em>Experimentum</em>. Zenodo. <a href="https://doi.org/10.5281/zenodo.4010579">https://doi.org/10.5281/zenodo.4010579</a>
</div>
<div id="ref-ebner_facesdatabase_2010" class="csl-entry" role="listitem">
Ebner, N. C., Riediger, M., &amp; Lindenberger, U. (2010). <span>FACES</span>—<span>A</span> database of facial expressions in young, middle-aged, and older women and men: <span>Development</span> and validation. <em>Behavior Research Methods</em>, <em>42</em>(1), 351–362. <a href="https://doi.org/10.3758/BRM.42.1.351">https://doi.org/10.3758/BRM.42.1.351</a>
</div>
<div id="ref-gao_cas-peal_2008" class="csl-entry" role="listitem">
Gao, W., Cao, B., Shan, S., Chen, X., Zhou, D., Zhang, X., &amp; Zhao, D. (2008). The <span>CAS</span>-<span>PEAL</span> <span>Large</span>-<span>Scale</span> <span>Chinese</span> <span>Face</span> <span>Database</span> and <span>Baseline</span> <span>Evaluations</span>. <em>IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans</em>, <em>38</em>(1), 149–161. <a href="https://doi.org/10.1109/TSMCA.2007.909557">https://doi.org/10.1109/TSMCA.2007.909557</a>
</div>
<div id="ref-hehman_assessing_2025" class="csl-entry" role="listitem">
Hehman, E., Xie, S. Y., Ofosu, E. K., &amp; Nespoli, G. A. (2025). Assessing the <span>Point</span> at <span>Which</span> <span>Averages</span> <span>Are</span> <span>Stable</span>: <span>A</span> <span>Tutorial</span> in the <span>Context</span> of <span>Impression</span> <span>Formation</span>. <em>Social Cognition</em>, <em>43</em>(5), 488–501. <a href="https://doi.org/10.1521/soco.2025.43.5.488">https://doi.org/10.1521/soco.2025.43.5.488</a>
</div>
<div id="ref-henrich_weirdest_2010" class="csl-entry" role="listitem">
Henrich, J., Heine, S. J., &amp; Norenzayan, A. (2010). The weirdest people in the world? <em>Behavioral and Brain Sciences</em>, <em>33</em>(2-3), 61–83. <a href="https://doi.org/10.1017/S0140525X0999152X">https://doi.org/10.1017/S0140525X0999152X</a>
</div>
<div id="ref-imagemagick" class="csl-entry" role="listitem">
ImageMagick Studio LLC. (2024). <em>ImageMagick</em> (Version 7.1.1) [Computer software]. <a href="https://imagemagick.org">https://imagemagick.org</a>
</div>
<div id="ref-jones_which_2021" class="csl-entry" role="listitem">
Jones, B. C., DeBruine, L. M., Flake, J. K., Liuzza, M. T., Antfolk, J., Arinze, N. C., Ndukaihe, I. L. G., Bloxsom, N. G., Lewis, S. C., Foroni, F., Willis, M. L., Cubillas, C. P., Vadillo, M. A., Turiegano, E., Gilead, M., Simchon, A., Saribay, S. A., Owsley, N. C., Jang, C., … Coles, N. A. (2021). To which world regions does the valence–dominance model of social perception apply? <em>Nature Human Behaviour</em>, <em>5</em>(1), 159–169. <a href="https://doi.org/10.1038/s41562-020-01007-2">https://doi.org/10.1038/s41562-020-01007-2</a>
</div>
<div id="ref-lakshmi_india_2021" class="csl-entry" role="listitem">
Lakshmi, A., Wittenbrink, B., Correll, J., &amp; Ma, D. S. (2021). The <span>India</span> <span>Face</span> <span>Set</span>: <span>International</span> and <span>Cultural</span> <span>Boundaries</span> <span>Impact</span> <span>Face</span> <span>Impressions</span> and <span>Perceptions</span> of <span>Category</span> <span>Membership</span>. <em>Frontiers in Psychology</em>, <em>12</em>. <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2021.627678">https://www.frontiersin.org/articles/10.3389/fpsyg.2021.627678</a>
</div>
<div id="ref-langner_presentation_2010" class="csl-entry" role="listitem">
Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D. H. J., Hawk, S. T., &amp; Knippenberg, A. van. (2010). Presentation and validation of the <span>Radboud</span> <span>Faces</span> <span>Database</span>. <em>Cognition and Emotion</em>, <em>24</em>(8), 1377–1388. <a href="https://doi.org/10.1080/02699930903485076">https://doi.org/10.1080/02699930903485076</a>
</div>
<div id="ref-lundqvist_karolinska_2015" class="csl-entry" role="listitem">
Lundqvist, D., Flykt, A., &amp; Öhman, A. (2015). <em>Karolinska <span>Directed</span> <span>Emotional</span> <span>Faces</span></em>. <a href="https://doi.org/10.1037/t27732-000">https://doi.org/10.1037/t27732-000</a>
</div>
<div id="ref-ma_chicago_2015" class="csl-entry" role="listitem">
Ma, D. S., Correll, J., &amp; Wittenbrink, B. (2015). The <span>Chicago</span> face database: <span>A</span> free stimulus set of faces and norming data. <em>Behavior Research Methods</em>, <em>47</em>(4), 1122–1135. <a href="https://doi.org/10.3758/s13428-014-0532-5">https://doi.org/10.3758/s13428-014-0532-5</a>
</div>
<div id="ref-ma_chicago_2021" class="csl-entry" role="listitem">
Ma, D. S., Kantner, J., &amp; Wittenbrink, B. (2021). Chicago <span>Face</span> <span>Database</span>: <span>Multiracial</span> expansion. <em>Behavior Research Methods</em>, <em>53</em>(3), 1289–1300. <a href="https://doi.org/10.3758/s13428-020-01482-5">https://doi.org/10.3758/s13428-020-01482-5</a>
</div>
<div id="ref-meyers_hawaii_2024" class="csl-entry" role="listitem">
Meyers, C., Garay, M., &amp; Pauker, K. (2024). <em>Hawai‘i <span>Face</span> <span>Database</span>: <span>A</span> racially and ethnically diverse set of facial stimuli</em>. <a href="https://doi.org/10.31234/osf.io/wde4b">https://doi.org/10.31234/osf.io/wde4b</a>
</div>
<div id="ref-miolla_padova_2023" class="csl-entry" role="listitem">
Miolla, A., Cardaioli, M., &amp; Scarpazza, C. (2023). Padova <span>Emotional</span> <span>Dataset</span> of <span>Facial</span> <span>Expressions</span> (<span>PEDFE</span>): <span>A</span> unique dataset of genuine and posed emotional facial expressions. <em>Behavior Research Methods</em>, <em>55</em>(5), 2559–2574. <a href="https://doi.org/10.3758/s13428-022-01914-4">https://doi.org/10.3758/s13428-022-01914-4</a>
</div>
<div id="ref-oosterhof_functional_2008" class="csl-entry" role="listitem">
Oosterhof, N. N., &amp; Todorov, A. (2008). The functional basis of face evaluation. <em>Proceedings of the National Academy of Sciences</em>, <em>105</em>(32), 11087–11092. <a href="https://doi.org/10.1073/pnas.0805664105">https://doi.org/10.1073/pnas.0805664105</a>
</div>
<div id="ref-perrett_your_2017" class="csl-entry" role="listitem">
Perrett, D. I. (2017). <em>In <span>Your</span> <span>Face</span>: <span>The</span> new science of human attraction</em>. Bloomsbury Publishing.
</div>
<div id="ref-saribay_bogazici_2018" class="csl-entry" role="listitem">
Saribay, S. A., Biten, A. F., Meral, E. O., Aldan, P., Třebický, V., &amp; Kleisner, K. (2018). The <span>Bogazici</span> face database: <span>Standardized</span> photographs of <span>Turkish</span> faces with supporting materials. <em>PLOS ONE</em>, <em>13</em>(2), e0192018. <a href="https://doi.org/10.1371/journal.pone.0192018">https://doi.org/10.1371/journal.pone.0192018</a>
</div>
<div id="ref-van_der_schalk_moving_2011" class="csl-entry" role="listitem">
Schalk, J. van der, Hawk, S. T., Fischer, A. H., &amp; Doosje, B. (2011). Moving faces, looking places: <span>Validation</span> of the <span>Amsterdam</span> <span>Dynamic</span> <span>Facial</span> <span>Expression</span> <span>Set</span> (<span>ADFES</span>). <em>Emotion</em>, <em>11</em>(4), 907–920. <a href="https://doi.org/10.1037/a0023853">https://doi.org/10.1037/a0023853</a>
</div>
<div id="ref-sneddon_belfast_2012" class="csl-entry" role="listitem">
Sneddon, I., McRorie, M., McKeown, G., &amp; Hanratty, J. (2012). The <span>Belfast</span> <span>Induced</span> <span>Natural</span> <span>Emotion</span> <span>Database</span>. <em>IEEE Transactions on Affective Computing</em>, <em>3</em>(1), 32–41. <a href="https://doi.org/10.1109/T-AFFC.2011.26">https://doi.org/10.1109/T-AFFC.2011.26</a>
</div>
<div id="ref-sutherland_social_2013" class="csl-entry" role="listitem">
Sutherland, C. A. M., Oldmeadow, J. A., Santos, I. M., Towler, J., Michael Burt, D., &amp; Young, A. W. (2013). Social inferences from faces: <span>Ambient</span> images generate a three-dimensional model. <em>Cognition</em>, <em>127</em>(1), 105–118. <a href="https://doi.org/10.1016/j.cognition.2012.12.001">https://doi.org/10.1016/j.cognition.2012.12.001</a>
</div>
<div id="ref-tottenham_nimstim_2009" class="csl-entry" role="listitem">
Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., Marcus, D. J., Westerlund, A., Casey, B., &amp; Nelson, C. (2009). The <span>NimStim</span> set of facial expressions: <span>Judgments</span> from untrained research participants. <em>Psychiatry Research</em>, <em>168</em>(3), 242–249. <a href="https://doi.org/10.1016/j.psychres.2008.05.006">https://doi.org/10.1016/j.psychres.2008.05.006</a>
</div>
<div id="ref-trebicky_focal_2016" class="csl-entry" role="listitem">
Třebický, V., Fialová, J., Kleisner, K., &amp; Havlíček, J. (2016). Focal <span>Length</span> <span>Affects</span> <span>Depicted</span> <span>Shape</span> and <span>Perception</span> of <span>Facial</span> <span>Images</span>. <em>PLOS ONE</em>, <em>11</em>(2), e0149313. <a href="https://doi.org/10.1371/journal.pone.0149313">https://doi.org/10.1371/journal.pone.0149313</a>
</div>
<div id="ref-trebicky_grim_2024" class="csl-entry" role="listitem">
Třebický, V., Třebická Fialová, J., Bjornsdottir, R. T., &amp; DeBruine, L. M. (2024). <em>A <span>Grim</span> <span>Image</span>: <span>Considerations</span> for <span>Methods</span> of portrait photography in psychological science</em>. OSF Preprints. <a href="https://doi.org/10.31219/osf.io/z4svb">https://doi.org/10.31219/osf.io/z4svb</a>
</div>
<div id="ref-trzewik_israeli_2025" class="csl-entry" role="listitem">
Trzewik, M., Navon, M., Moran, T., Wardi, H., Langer, A., Hadad, B.-S., Sofer, C., &amp; Reggev, N. (2025). The <span>Israeli</span> <span>Face</span> <span>Database</span> (<span>IFD</span>): <span>A</span> multi-ethnic database of faces with supporting social norming data. <em>Behavior Research Methods</em>, <em>57</em>(7), 197. <a href="https://doi.org/10.3758/s13428-025-02723-1">https://doi.org/10.3758/s13428-025-02723-1</a>
</div>
<div id="ref-workman_face_2021" class="csl-entry" role="listitem">
Workman, C. I., &amp; Chatterjee, A. (2021). The <span>Face</span> <span>Image</span> <span>Meta</span>-<span>Database</span> (<span class="nocase">fIMDb</span>) &amp; <span>ChatLab</span> <span>Facial</span> <span>Anomaly</span> <span>Database</span> (<span>CFAD</span>): <span>Tools</span> for research on face perception and social stigma. <em>Methods in Psychology</em>, <em>5</em>, 100063. <a href="https://doi.org/10.1016/j.metip.2021.100063">https://doi.org/10.1016/j.metip.2021.100063</a>
</div>
<div id="ref-yarkoni_generalizability_2022" class="csl-entry" role="listitem">
Yarkoni, T. (2022). The generalizability crisis. <em>Behavioral and Brain Sciences</em>, <em>45</em>, e1. <a href="https://doi.org/10.1017/S0140525X20001685">https://doi.org/10.1017/S0140525X20001685</a>
</div>
<div id="ref-young_recognizing_2017" class="csl-entry" role="listitem">
Young, A. W., &amp; Burton, A. M. (2017). Recognizing <span>Faces</span>. <em>Current Directions in Psychological Science</em>, <em>26</em>(3), 212–217. <a href="https://doi.org/10.1177/0963721416688114">https://doi.org/10.1177/0963721416688114</a>
</div>
<div id="ref-zebrowitz_reading_1997" class="csl-entry" role="listitem">
Zebrowitz, L. (1997). <em>Reading <span>Faces</span>: <span>Window</span> <span>To</span> <span>The</span> <span>Soul</span>?</em> Routledge.
</div>
</div>
</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>We did not fully colour-calibrate the images here (see Discussion) but future researchers may wish to do so.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>The preregistration mis-stated the total number of models, rather than the maximum number of images, to be 205. The total number of models was 211<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
     </main>
<!-- /main column -->  <script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>  </div> <!-- /content -->  <script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script> 
  
</body></html>