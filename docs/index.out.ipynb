{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing Many Faces\n",
    "\n",
    "R. Thora Bjornsdottir [![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==)](https://orcid.org/0000-0002-1016-3829) (University of Stirling)  \n",
    "Vít Třebický [![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==)](https://orcid.org/0000-0003-1440-1772) (Charles University)  \n",
    "Lisa DeBruine [![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==)](https://orcid.org/0000-0002-7523-5539) (University of Glasgow)"
   ],
   "id": "a7825d01-4277-4833-9295-39801dbc5912"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- to add rest of team as authors w/ CRediT -->"
   ],
   "id": "d87c3b5a-fe97-4c22-bb91-c973190fcc2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr     1.1.4     ✔ readr     2.1.5\n",
      "✔ forcats   1.0.1     ✔ stringr   1.5.1\n",
      "✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n",
      "✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n",
      "✔ purrr     1.1.0     \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "************\n",
      "Welcome to webmorphR. For support and examples visit:\n",
      "https://debruine.github.io/webmorphR/\n",
      "************"
     ]
    }
   ],
   "source": [
    "library(tidyverse)"
   ],
   "id": "24eb23d4-0404-4fa0-b8bb-47eae340167b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Faces are rich sources of information, playing a key role in human social perception. They strongly draw attention, serving as a primary means of person recognition and informing social inferences (e.g., [Young & Burton, 2017](#ref-young_recognizing_2017); [Zebrowitz, 1997](#ref-zebrowitz_reading_1997))<!--other refs?-->. Various subfields within psychology—including vision science, person perception, social cognition, affective science, behavioural science, and person recognition—address research questions involving faces and thus necessitate the use of face images as stimuli. Here, we introduce a novel face image database for research use. This database was collected as part of an international multi-lab collaboration and addresses various limitations of existing databases. Crucially, we take a transparent approach aligned with Open Science best practices and make openly available the reproducible protocol we followed to collect the images, enabling future expansion of the database.\n",
    "\n",
    "### 1.1 Existing face databases\n",
    "\n",
    "A wealth of face stimulus databases exists, created for various purposes. Many of these are available (see e.g., [Workman & Chatterjee, 2021](#ref-workman_face_2021) face image meta-database to search many of these) for research purposes allowing access to a broad variety of face stimuli, well-suited to address questions in a given subfield. For example, there are many different databases in which photographed individuals (referred to as targets or models) display different facial expressions of emotion (e.g., [Ebner et al., 2010](#ref-ebner_facesdatabase_2010); [Schalk et al., 2011](#ref-van_der_schalk_moving_2011), of use in affective science), appear in varying lighting and angles (e.g., [Burton et al., 2010](#ref-burton_glasgow_2010); [Gao et al., 2008](#ref-gao_cas-peal_2008), useful for person recognition research), or belong to various age or racial/ethnic groups (for perception researcher). However, many studies tend to rely on a small number of those stimulus databases, which introduces potential problems. For example, 4107 published papers cite the Karolinska Directed Emotional Faces ([Lundqvist et al., 2015](#ref-lundqvist_karolinska_2015)), 4375 cite the NimStim Set of Facial Expressions ([Tottenham et al., 2009](#ref-tottenham_nimstim_2009)), 2461 cite the Chicago Face Database ([Ma et al., 2015](#ref-ma_chicago_2015)), and 3219 the Radboud Faces Database ([Langner et al., 2010](#ref-langner_presentation_2010)). This frequent (over)use of the same sets of stimuli can compromise generalizability when various research questions are tested on only a limited sample of face images ([DeBruine et al., 2022](#ref-debruine_reproducible_2022); [Yarkoni, 2022](#ref-yarkoni_generalizability_2022)). Participants, especially those participating online, may also become familiar with frequently used stimuli, potentially introducing biases to their responses.\n",
    "\n",
    "Of course, not all research involving face images uses stimuli from published databases. It is a common for researchers to purposefully collect images themselves. But these stimulus sets are seldom openly shared (e.g., due to ethical limitations). Although this circumvents the issues with frequently-used databases, it introduces its own set of problems. Chiefly, face images collected by researchers for their own research use are often insufficiently documented. Although the resulting images may be described and/or illustrated with an example, the process of image collection is rarely described in adequate detail. This is often the case for even those databases openly available for research use. This lack of image acquisition (and processing) methods documentation limits transparency, replication attempts–for example, another researcher attempting to replicate a finding using new face images collected themselves, rather than the images collected by the original researchers (see [Třebický et al., 2024](#ref-trebicky_grim_2024))–and ultimately comparisons of results.\n",
    "\n",
    "In many databases (both publicly available and not), there are also limitations in the diversity of stimuli. For example, many include images of only White or Western targets/models (e.g., see [Cook & Over, 2021](#ref-cook_why_2021)). This reinforces the centering of Whiteness and Western culture in psychological research, importantly limiting the ecological validity and generalizability of conclusions (see also [Henrich et al., 2010](#ref-henrich_weirdest_2010)). Recently created databases have sought to address this, specifically collecting images of multiracial/multi-ethnic and non-Western samples, for example (e.g., [Chen et al., 2021](#ref-chen_broadening_2021); [Meyers et al., 2024](#ref-meyers_hawaii_2024); [Saribay et al., 2018](#ref-saribay_bogazici_2018); [Trzewik et al., 2025](#ref-trzewik_israeli_2025)), but more such work is needed to further diversify the pool of available face research stimuli. Many world regions remain underrepresented. This may be due, in part, to resource limitations: Equipment and setups for collecting images suitable for research use can be prohibitively expensive and complex, making database collection not feasible for everyone and everywhere, thereby limiting database diversity. Additionally, researchers with the necessary equipment may be limited in terms of the diversity of their available participant sample. Furthermore, to date, no extant database contains images of individuals from *multiple* world regions.\n",
    "\n",
    "A final limitation of extant face image databases is their largely static state. That is, once collected, the number of stimuli available does not change. There are some exceptions to this, such as multiple waves of additions to the Chicago Face Database ([Lakshmi et al., 2021](#ref-lakshmi_india_2021); [Ma et al., 2021](#ref-ma_chicago_2021)). Continued additions to databases are not the norm, however. This not only constrains the number of stimuli in any given database, increasing the likelihood of the frequent reuse of any individual stimulus, but can also render some databases obsolete over time (e.g., due to very dated model appearance/styling).\n",
    "\n",
    "### 1.2 Open Science considerations\n",
    "\n",
    "Open and transparent methods are essential for fully understanding, evaluating, and replicating research. Although methods reporting has become more open and accessible with researchers sharing materials such as questionnaire wording, stimuli, and programming scripts, the reporting of face image database collection procedures often lacks crucial details for full transparency and reproducibility. This matters because differences in details such as focal length, for example, can importantly affect faces’ appearance, subsequently impacting perceptions of them (e.g., [Třebický et al., 2016](#ref-trebicky_focal_2016); see [Třebický et al., 2024](#ref-trebicky_grim_2024) for further discussion).\n",
    "\n",
    "Big Team Science (i.e., multi-lab) endeavours and large-scale collaborations are now recognized as a vital part of improving science. Within psychology, multiple such initiatives have importantly contributed to addressing a variety of research questions, improving diversity and generalizability in the field (e.g., in the domain of face perception, Jones et al.’s ([2021](#ref-jones_which_2021)) Psychological Science Accelerator project). Such an approach has yet to be applied to stimulus database collection, but it represents an opportunity to address many of the limitations of face image databases raised.\n",
    "\n",
    "### 1.3 The current work\n",
    "\n",
    "Here, we sought to address the limitations of existing face image databases through a Big Team Science approach. This served as the first project of ManyFaces, an international consortium of face perception and recognition researchers formed in 2022.\n",
    "\n",
    "To address transparency and reproducibility issues, we developed an openly available, reproducible protocol for face image collection. The protocol covers collecting multiple images (varying in standardization, viewing angle, and facial expression) of each target/model to benefit a wide array of possible research areas and questions related to face perception and recognition. Following this protocol, we collected images (Phase 1) in 20 different labs across the world, spanning 11 countries and five world regions (Europe, Latin America, North America, the Middle East, and Southeast Asia). This resulted in a diverse set of models and images, more than would be possible to collect in or by a single lab. Moreover, this database is not static but can be added to in the future by any interested researcher following the protocol. We also validated the database (Phase 2) by recruiting online perceivers to rate a subset of the images (i.e., front-facing images) to generate norming data for key social trait perceptions and emotion recognition. We make this image database and norming data available for future research.\n",
    "\n",
    "Altogether, we introduce a new, diverse, openly-available face stimulus set that cancontinue to grow in the future. We believe that this effort will broaden the generalizability of findings in face perception and recognition research.\n",
    "\n",
    "## 2 Study/phase 1: Protocol development & stimulus collection\n",
    "\n",
    "To set face perception and recognition Big Team Science in motion and enable conducting multi-site studies, a set of stimuli suitable for most areas (e.g., in terms of research question, geographic location) is needed. Therefore, we developed a protocol allowing us to collect such a database of face images. We developed the protocol with maximum usability and accessibility in mind, making it transparent and readable to non-experts, and designed for use with attainable (vs. highly specialized and expensive) equipment, with minimal setup requirements, and without requiring expertise in photography. Note that the protocol is for in-lab image collection to minimize external noise, to enable the collection of models’ self-report data, and to ensure the consensual use of models’ images (vs. scraping images from online sources or generating them). Photographing faces in a controlled lab environment thus strikes a balance in terms of ecological validity and ethical concerns (see [Trzewik et al., 2025](#ref-trzewik_israeli_2025) for discussion of the value of lab-photographed vs. artificially generated and ambient face images).\n",
    "\n",
    "### 2.1 Method\n",
    "\n",
    "For both studies/phases, the University of Glasgow provided ethical approval. We obtained additional local ethical approval at collaborating institutions where required.\n",
    "\n",
    "#### 2.1.1 Protocol Development\n",
    "\n",
    "The team curated a set of equipment and developed a protocol for collecting standardized, reproducible images of faces for research. Following a survey among the ManyFaces members, we constructed the protocol for the collection of a variety of images that would be useful for a broad variety of research questions. Specifically, we included images varying in their standardization of appearance (standardized and unstandardized, e.g., white t-shirt and hair pulled back vs. clothing and hair as worn by the participant/model on that day), viewing angle (frontal, profile, ¾), and facial expression (neutral, natural, angry, disgusted, fearful, happy, sad, surprised).\n",
    "\n",
    "Prior to beginning data collection, the ManyFaces team pre-tested the protocol to ensure clarity of instructions and consistency of images collected across sites. We revised the protocol to address issues that arose (e.g., revised facial expression elicitation instructions, clarified camera settings). The final protocol can be found at \\[https://docs.google.com/document/d/1D9TPGXCgTRZi7nqEIg6jb42R4gNQx9L3qasg8tFvu1I/edit?usp=sharing<!--to add to OSF-->\\] (note that this is a living document that may be updated in future). In sum, the protocol detailed the following:\n",
    "\n",
    "##### 2.1.1.1 Image Types.\n",
    "\n",
    "The protocol defined the categories of images in terms of their standardisation of appearance, viewing angle, and facial expression.\n",
    "\n",
    "-   Standardisation of Appearance. In the *standardised* images, models wore a white crew neck t-shirt, had their hair pulled back or covered, included no adornments/accessories (excepting those that could not be removed for cultural reasons), and wore minimal or no makeup (models were informed about this beforehand). In contrast, *unstandardised* images showed models in their own clothing, with their hair as they came into the lab, and with adornments, except for glasses or anything that obscured the face or neck.  \n",
    "-   Viewing Angle. *Full frontal portraits* showed models facing the camera, *left and right profile portraits* showed each side of models’ faces in profile, and *left and right ¾ portraits* showed models’ faces at a 45-degree angle from the axis of the camera.  \n",
    "-   Facial Expression. *Neutral* images were of models refraining from making any facial expression, *natural* images displayed a natural expression for the model, and each facial expression of emotion displayed the expression the model would make if they were feeling the specified emotion (e.g., happiness).\n",
    "\n",
    "##### 2.1.1.2 Image Prioritisation\n",
    "\n",
    "The protocol specified which images to take (i.e., the combinations of appearance standardisation, viewing angle, and facial expression), in what order, and which were most crucial to collect (as voted by the ManyFaces members) if researchers were under time constraints. The order of image collection was as follows, with starred (\\*) images prioritised/required for each model:\n",
    "\n",
    "-   Exposure calibration photo\\*\n",
    "-   Identification & calibration photo\\*\n",
    "-   Unstandardized - Natural - Full frontal portrait\n",
    "-   Unstandardized - Neutral - Full frontal portrait\\*\n",
    "-   Unstandardized - Neutral - Left and Right profile portraits\n",
    "-   Unstandardized - Neutral - Left and Right ¾ portraits\n",
    "-   Unstandardized - Happy - Full frontal portrait\\*\n",
    "-   Unstandardized - Happy - Left and Right profile portraits\n",
    "-   Unstandardized - Happy - Left and Right ¾ portraits\n",
    "-   Unstandardized - All other expressions (in random or appropriate order) - Full frontal portrait\n",
    "-   Standardized - Natural - Full frontal portrait\n",
    "-   Standardized - Neutral - Full frontal portrait\\*\n",
    "-   Standardized - Neutral - Left and Right profile portraits\\*\n",
    "-   Standardized - Neutral - Left and Right ¾ portraits\\*\n",
    "-   Standardized - Happy - Full frontal portrait\\*\n",
    "-   Standardized - Happy - Left and Right profile portraits\n",
    "-   Standardized - Happy - Left and Right ¾ portraits\n",
    "-   Standardized - All other expressions (in random or appropriate order) - Full frontal portrait\n",
    "\n",
    "Researchers with additional time could also capture the other expressions at different viewing angles (Left/Right profile, ¾) with either unstandardized or standardized appearance, as well as a video of standardized appearance to test out 3D image capture.\n",
    "\n",
    "##### 2.1.1.3 Equipment\n",
    "\n",
    "All collaborating sites/labs used the same set of equipment. <!--note the 2 US labs used their own equipment - check whether exact same or comparable--> Images were captured using a Canon EOS 250d (also called Canon EOS Rebel SL3 in some regions) camera with a kit lens (Canon 18-55mm IS STM lens), and lit by an LED Ring light (Fovitec Bi-Colour LED 18” Ring Light) on a stand. For colour calibration, Calibrite ColourChecker Classic card was used. The image collection spaces were to have a white background with a chair for models to sit in. Researchers provided white t-shirts for models to wear for standardised images.\n",
    "\n",
    "##### 2.1.1.4 Setup\n",
    "\n",
    "The setup of the room used for image collection at each site required removing or minimising external light sources (e.g., using a windowless room, window blinds, turning off overhead lights) and colour spill. The protocol specified the distances at which to set up the chair for models to sit in relative to the white background and the lighting and camera rig. Camera setup instructions, including the specified shooting and focusing modes, shutter speed, aperture, ISO, white balance, colour space, and file type.\n",
    "\n",
    "##### 2.1.1.5 Procedure\n",
    "\n",
    "Finally, the protocol detailed how to prepare and position models and how to take each kind of images listed in the image prioritisation section. This included the positioning of the model’s head for each viewing angle and facial expression elicitation instructions.\n",
    "\n",
    "#### 2.1.2 Stimulus Collection"
   ],
   "id": "21f20f57-9fb8-42f8-985d-6f127f5517d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_n <- data_models$lab_id |> unique() |> length()\n",
    "\n",
    "models_per_site <- count(data_models, lab_id)$n\n",
    "mean_mps <- mean(models_per_site) |> round(1)"
   ],
   "id": "e574faf5-14cc-415b-bbc3-5a898473f9c0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the protocol, 20 labs (1 in Austria, 2 in Brazil, 2 in Canada, 2 in Germany, 1 in Israel, 3 in Malaysia, 1 in Mexico, 1 in the Netherlands, 1 in Serbia, 4 in the UK, and 2 in the US) collected images of an average of 10.6 models per site. As specified by the protocol, researchers collected multiple images of each model, within the time constraints of the study session.\n",
    "\n",
    "Models furthermore completed a demographic questionnaire \\[LINK\\] <!-- PDF is here, add to OSF: https://drive.google.com/file/d/1EkX4jCkwpFb-Cwi9yWge_JrNbus5re6W/view*--> Experimentum, reporting their age, gender, ethnicity, height, and weight. They also reported whether or not they were wearing makeup (specifying what kinds, e.g., foundation, eye makeup, semi-permanent makeup) and whether they had ever experienced anything that could affect the shape of their face (e.g., broken nose, cosmetic surgery or injections, orthodontic work) and specified this if they were willing. Finally, they completed a debriefing questionnaire \\[LINK\\] <!-- PDF is here, add to OSF: https://drive.google.com/file/d/1XFlAdcMZAEcC1NGIJeaOHbx3BYPwzYqn/view*--> asking them about their experience of posing for the images (e.g., whether instructions were clear, whether any part of the process was uncomfortable).\n",
    "\n",
    "### 2.2 Results\n",
    "\n",
    "#### 2.2.1 Images"
   ],
   "id": "d20952ea-ad35-42f5-adf7-9cd363222168"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_n <- nrow(data_models)"
   ],
   "id": "8d8f8c51-c7b9-4d5c-9a80-d30f840cfcd2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "211 total models provided their images, following withdrawals and exclusions for poor image quality. <a href=\"#tbl-images-per-type\" class=\"quarto-xref\">Table 1</a> details the number of models for each kind of photo. Example images are shown in <a href=\"#fig-img-examples\" class=\"quarto-xref\">Figure 1</a>. Images are available for research use and can be requested here: \\[link\\] <!--add: link to where the images can be requested-->"
   ],
   "id": "ba447af2-ffba-46f5-8725-10a75d0e59d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_levels <- c(neu = \"neutral\", \n",
    "                ang = \"anger\", \n",
    "                dis = \"disgust\", \n",
    "                fea = \"fear\", \n",
    "                hap = \"happy\", \n",
    "                sad = \"sad\", \n",
    "                sur = \"surprised\")\n",
    "\n",
    "data_models |>\n",
    "  select(model_id:unstd_neu) |>\n",
    "  pivot_longer(std_ang:unstd_neu) |>\n",
    "  filter(value == 1) |>\n",
    "  count(name) |>\n",
    "  separate(name, c(\"type\", \"emotion\")) |>\n",
    "  pivot_wider(names_from = type, values_from = n) |>\n",
    "  mutate(emotion = factor(emotion, names(emo_levels), emo_levels)) |>\n",
    "  arrange(emotion)"
   ],
   "id": "9e12d2ef-91d4-4ac8-8e2e-179b0beecdb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-img-examples"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Model Demographics\n",
    "\n",
    "For data cleaning, we first downloaded and reshaped the raw data from Experimentum. In the next step, we ensured that the models’ gender, race/ethnicity, and units of height and weight were consistently formatted across labs. For gender and race/ethnicity, words presented in languages other than English were recoded to be presented in English (e.g., “mulher” to “female”, “preta” to “Black”). We then classified self-described race/ethnicity into one of seven categories (“White”, “Black”, “Asian”, “Indigenous”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), where possible. Descriptions that could not be clearly sorted into these categories were given “Ambiguous label” (non-entries were recoded as NA). For height, we ensured all data were presented in centimeters, and for weight, we ensured that all data were presented in kilograms. Models could report height and weight in metric or imperial units, so we converted from imperial to metric where required. We also sanity-checked the reported units, assuming heights \\> 100 to be in centimeters and those \\< 100 to be in inches, and weights \\< 80 to be in kilograms. Any non-entries of height or weight for a model was recoded as NA ."
   ],
   "id": "ac206335-8a4b-4fec-9f86-2c5699590cac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(a = 1:5)"
   ],
   "id": "972f1a53-73a7-419b-b572-16a7badc8be8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Study/phase 2: Validation/norming data\n",
    "\n",
    "We next obtained perceptions/ratings of a subset of the photos (front-facing) to validate the emotion expressions (their perceived emotion category and intensity) and collect norming data on central social perceptions, namely perceived attractiveness, dominance, trustworthiness, gender-typicality, memorability, and age. We chose these ratings due to their central importance in the person perception, face recognition, and emotion perception literatures (e.g., [Oosterhof & Todorov, 2008](#ref-oosterhof_functional_2008); [Perrett, 2017](#ref-perrett_your_2017); [Sutherland et al., 2013](#ref-sutherland_social_2013)) <!--add emotion & memorability/recognition refs-->. We preregistered this study on the OSF (<https://osf.io/4d5v9>).\n",
    "\n",
    "### 3.1 Method\n",
    "\n",
    "#### 3.1.1 Image Processing"
   ],
   "id": "e8ee4fd4-4e4c-4544-9d23-4cd5939f904c"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--move this section to phase 1?-->"
   ],
   "id": "499b3d1b-bb29-4208-9dda-a4b13943b709"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAW images were processed using webmorphR ([DeBruine et al., 2022](#ref-debruine_reproducible_2022)), which facilitates scriptable processing of images using imagemagick ([ImageMagick Studio LLC, 2024](#ref-imagemagick)); a full script of the processing steps is available at \\[REPO <!--add link-->\\]. Briefly,\n",
    "\n",
    "-   Each face was delineated using the Face++ automatic face detection algorithm (see <https://www.faceplusplus.com/>) to generate a 106-point template.\n",
    "-   All images were resized to 1000w by 1500h pixels to standardise size (two different RAW formats were used, which resulted in two different image sizes)\n",
    "-   The median RGB colour value of a 100×100 pixel patch at the upper left corner was calculated to fill in any edges from alignment.\n",
    "-   The image was repositioned and cropped (not rotated or resized) such that\n",
    "    -   The image size was 675w by 900h pixel\n",
    "    -   point 71 (between the eyes) was relocated to position \\[.5w, .4h\\]\n",
    "-   This aligned image was saved as a lossless PNG using imagemagick default settings (e.g., sRGB colour space).\n",
    "-   A white balance correction was applied to the resulting images, calculated from the mean RGB values in the 25x25 pixel top-right corner patch (white background) [1]\n",
    "-   These images were converted to JPEGs with a quality setting of 75 to reduce file size for stimulus display online.\n",
    "\n",
    "[1] We did not fully colour-calibrate the images here (see Discussion) but future researchers may wish to do so."
   ],
   "id": "85333d84-c1e0-47f4-8a03-39aca45d237e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-img-processing"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Stimuli\n",
    "\n",
    "The number of stimuli was determined by the number of models recruited across the 20 research labs and how many models posed for each image type. The maximum number of targets per image type was 205[1], meaning each rater saw up to 205 stimuli. We obtained ratings of the front-facing standardised neutral (n = 205), unstandardised neutral (n = 188), standardised angry (n = 187), standardised disgusted (n = 184), standardised fearful (n = 175), standardised happy (n = 199), standardised sad (n = 183), and standardised surprised (n = 184).\n",
    "\n",
    "[1] The preregistration mis-stated the total number of models, rather than the maximum number of images, to be 205. The total number of models was 211"
   ],
   "id": "01757276-f325-4022-a58b-be5dd7adccd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-stim-examples"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Attention Check Stimuli\n",
    "\n",
    "Additionally, we created stimuli for attention checks. These were white images with the same size and aspect ratio as the face stimuli, but contained only the written instruction to choose a specific response, (e.g., ‘Choose “fear”’ or ‘Choose “3”’).\n",
    "\n",
    "#### 3.1.4 Measures"
   ],
   "id": "eccc679f-1358-42e7-9d02-95796e5479a7"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--can put screenshots of example trials from experimentum on OSF-->"
   ],
   "id": "e4db693e-611d-4b1c-b7b0-46bb253527bb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4.1 Standardised Neutral Faces\n",
    "\n",
    "###### 3.1.4.1.1 Trait Ratings\n",
    "\n",
    "We obtained ratings of faces’ attractiveness, dominance, trustworthiness, memorability, and gender-typicality (‘How attractive \\[dominant, trustworthy, memorable, gender-typical\\] does this person look?’). Ratings were on scales ranging from 1 (*not at all*) to 7 (*very*).\n",
    "\n",
    "###### 3.1.4.1.2 Demographic Impressions\n",
    "\n",
    "We obtained ratings of faces’ perceived age (‘How old does this person look?’), with responses collected in 5-year ranges/brackets (i.e., 16-20, 21-25, …, 76-80, 81+)."
   ],
   "id": "ac50cfbe-7d06-4095-9ac1-473787823faa"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!--still to add perceived race/ethnicity-->"
   ],
   "id": "b45145bc-4f0b-4466-ad11-fc58f74d0450"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4.2 Unstandardised Neutral Faces\n",
    "\n",
    "###### 3.1.4.2.1 Trait Ratings\n",
    "\n",
    "We obtained ratings of faces’ attractiveness, dominance, and trustworthiness on scales ranging from 1 (*not at all*) to 7 (*very*).\n",
    "\n",
    "##### 3.1.4.3 Standardised Emotional Faces\n",
    "\n",
    "###### 3.1.4.3.1 Emotion Categorisation\n",
    "\n",
    "We obtained impressions of the emotion each person was expressing (‘What emotion is this person expressing?’), choosing one from: *anger, disgust, fear, happiness, sadness, surprise, other*. Here, raters categorized a counterbalanced mixture of expressions (from one of six counterbalanced conditions) rather than faces all showing the same expression. The 201 identities with emotion images were divided into six groups of up to 34 images, and each counterbalanced condition showed a different emotion for each of the six groups, such that no identity was shown more than once to each rater. Since not all identities had all six emotions, the number of images in each counterbalanced condition ranged from 179 to 193.\n",
    "\n",
    "###### 3.1.4.3.2 Emotion Intensity Ratings\n",
    "\n",
    "We obtained ratings of how intensely faces expressed each intended emotion (‘How intensely is this person expressing anger \\[disgust, fear, happiness, sadness, surprise\\]?’) from 1 (*not at all)* to 7 (*very*). Here, raters only rated all faces showing one emotion expression (e.g., all angry faces) and rated the intensity only of the intended expression (e.g., angry faces only rated on anger intensity).\n",
    "\n",
    "#### 3.1.5 Procedure\n",
    "\n",
    "We collected ratings via Experimentum ([DeBruine et al., 2020](#ref-debruine_experimentum_2020)); structure files for the exact experimental setup are available at \\[REPO\\]<!--add link-->. After a brief introduction to the study and online informed consent, each participant (rater) was randomly allocated to one of the ratings (e.g., rating all 205 standardised neutral faces on how memorable they look). All available faces were displayed one at a time, in a randomised order for each rater. The question/prompt and response scale remained visible at the top of the screen, above the photo throughout the study. The study automatically progressed to the next trial once the rater responded by clicking on the response scale. There was no time limit to provide a response. We included seven attention checks embedded in the study, which directed raters to provide a specific response.\n",
    "\n",
    "Following rating or categorising all stimuli, raters self-reported their gender, age, race/ethnicity, country of residence, and device type used for the study (desktop, laptop, tablet, mobile)<!--add doc to OSF-->. They also completed an honesty/attention check question, asking them if they engaged with the study seriously, with assurance of payment regardless of response (choosing from ‘no, I was not really paying attention’ and ‘yes, I tried to give my authentic first impressions’).\n",
    "\n",
    "We recruited fluent English-speaking raters through Prolific <!--add prolific inclusion/exclusion criteria-->. We collected all data in May 2025.\n",
    "\n",
    "#### 3.1.6 Participants\n",
    "\n",
    "We aimed to collect 100 raters per rating condition to achieve stable averages and allow for exclusions ([Hehman et al., 2025](#ref-hehman_assessing_2025)), totalling 2100 raters. Altogether, 2115 raters completed the study. See Results for exclusions and demographics of the final sample.\n",
    "\n",
    "### 3.2 Results\n",
    "\n",
    "#### 3.2.1 Data Cleaning and Exclusions\n",
    "\n",
    "In the raters’ demographic questionnaire, we standardized participants’ recording of their race/ethnicity similarly to the models’ by recoding their inputs into one of seven categories (“White”, “Black”, “Asian”, “Indigenous/Pacific Islander”, “MENA” (Middle Eastern or North African), “Latine”, or “Mixed”), or as “Ambiguous label” when this was not possible. Any non-entries were recoded as NA.\n",
    "\n",
    "A total of 2115 raters completed 2158 rating or categorisation tasks. We found that some raters did not complete all trials in tasks, some raters completed more than one task, and some raters completed more than the maximum number of trials in a task (likely by restarting the study or bypassing the back button block). Therefore, we removed incomplete tasks from our data, retained raters’ first complete tasks, and filtered our duplicate trials by keeping only raters’ first ratings for a duplicated trial. After these exclusions, and before implementing the pre-registered plan for data exclusions, we had complete and clean data from 1936 raters.\n",
    "\n",
    "Our pre-registered plan for data exclusions included removing raters who gave overly consistent responses, committed overly fast responses, self-reported not taking the study seriously when asked whether or not they completed the study authentically, and failed attention checks. In total, we excluded 49 raters for our pre-registered reasons for data exclusions. <a href=\"#tbl-exclusions\" class=\"quarto-xref\">Table 3</a> shows the number of raters we excluded for each of our reasons."
   ],
   "id": "1cbc1659-1493-453a-b19f-e37d231f2dbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl-subcap: Multiple raters met more than one exclusion criterion; therefore, the counts for individual criteria do not sum to the total number of raters excluded.\n",
    "\n",
    "data.frame(a = 1:5)"
   ],
   "id": "88dcf787-c4bc-4f6f-804a-a0d349f067e9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined overly consistent responses as those raters who responded to at least 90% of the trials identically. We defined overly fast responses as those raters whose median reaction time fell below the 1st percentile of the overall distribution of median reaction times (see Figure 1 for the distribution of median reaction times). For our attention checks, the threshold for inclusion was to accurately complete six or more attention checks (i.e., participants were excluded if they failed more than one of the checks). After these exclusions, we had 1899 <!--update N--> raters in our sample (see <a href=\"#tbl-raters-per-task\" class=\"quarto-xref\">Table 4</a> for the number of raters per task)."
   ],
   "id": "5ac5157e-b157-4889-92c2-422f008dfcb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-rt-distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(a = 1:5)"
   ],
   "id": "950c5fca-6e99-482b-9c28-a2c87c995fa4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymous data are available on the OSF: \\[LINK\\] <!--[add link-->\n",
    "\n",
    "#### 3.2.2 Rater Demographics\n",
    "\n",
    "We collected the following demographic data from our raters (N = 1676 of 1899<!--update N--> provided data): Age, gender, residence, ethnicity, and devices on which the ratings were completed. See <a href=\"#fig-gender-age\" class=\"quarto-xref\">Figure 5</a> and Tables 3-5 for demographic information."
   ],
   "id": "e7eb7e7e-d58e-482c-aab3-9e273baac43c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "data_qu |> \n",
    "  ggplot(aes(x = age, fill = gender)) +\n",
    "  geom_histogram(binwidth = 1) +\n",
    "  scale_fill_manual(values = c(\"hotpink\", \"dodgerblue\", \"darkorchid\", \"gray\")) +\n",
    "  guides(fill  = guide_legend(position = \"inside\")) +\n",
    "  theme(legend.position.inside = c(.7, .7))"
   ],
   "id": "cell-fig-gender-age"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "d2849655-3e23-41fd-8f51-0b7a93e195e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "2e9c60b8-98d6-4810-941f-4c790984446a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "45c0990b-91da-4be8-9eea-a370cacc0b63"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Agreement Indicators for the Ratings\n",
    "\n",
    "##### 3.2.3.1 Standardized Trait Ratings\n",
    "\n",
    "In the first step, we calculated intraclass correlations for traits observed across tasks for our standardized images. The number of raters ranged from 83 to 94 for these traits. The average reliability across raters was very good for rating attractiveness, dominance, trustworthiness, and gender-typicality, but poorer for rating memorability (see <a href=\"#tbl-std-neu-agree\" class=\"quarto-xref\">Table 8</a>)."
   ],
   "id": "1cd48569-cb07-4b9f-aaf9-d319d44b9dff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl-subcap: ICCs values are ICC (2,k) values.\n",
    "\n",
    "data.frame(1:5)"
   ],
   "id": "86e028ba-bc79-4907-a846-a105e6cb3f5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examined the number of raters required for trait ratings to reach stable levels of reliability defined as ICC (2,k) values of 0.75 and 0.90 (see Figure 3). Attractiveness ratings stabilized with the fewest participants, reaching an ICC (2,k) of 0.75 at approximately 18 raters. It was also the only trait to exceed an ICC (2,k) of 0.90, which occurred with 50 raters. Ratings for dominance, trustworthiness, and gender-typicality reached an ICC (2,k) of 0.75 with approximately 35-40 raters (see <a href=\"#fig-icc-cos\" class=\"quarto-xref\">Figure 6</a>)."
   ],
   "id": "1cf671ec-ce79-4450-ab38-0e783b3a58dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-icc-cos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking internal consistency of our ratings, we used both Cronbach’s alpha (α) and McDonald’s omega total (ωt) (see <a href=\"#tbl-std-neu-agree\" class=\"quarto-xref\">Table 8</a>). For four of the five traits, both α and ωt exceeded 0.9, indicating excellent internal consistency. For the fifth trait (memorability), α was lower (0.65), indicating doubtful internal consistency when assuming equal item contributions (tau-equivalence) in the ratings. However, ωt for memorability was substantially higher (0.87), indicating good internal consistency after taking into account variance among item or rater contributions.\n",
    "\n",
    "##### 3.2.3.2 Unstandardized Trait Ratings\n",
    "\n",
    "As with our standardized trait ratings, but for three rather than five traits, we calculated intraclass correlations for ratings to our unstandardized images observed across rating tasks. The number of raters ranged from 84 to 95 for these traits. The The average reliability across raters was very good for all trait ratings (see <a href=\"#tbl-unstd-neu-agree\" class=\"quarto-xref\">Table 9</a>). All three traits also demonstrated excellent internal consistency (see <a href=\"#tbl-unstd-neu-agree\" class=\"quarto-xref\">Table 9</a>) with Cronbach’s alpha (α) ranging from 0.88 to 0.95 and McDonald’s omega total (ωt) ranging from 0.90 to 0.96."
   ],
   "id": "404a3ba4-6499-400b-adc5-10798290220d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl-subcap: ICCs values are ICC (2,k) values.\n",
    "\n",
    "data.frame(1:5)"
   ],
   "id": "85830346-0d9c-42cc-a104-4e48057e4e9e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3.3 Emotion Intensity Ratings\n",
    "\n",
    "Next, we calculated the intraclass correlations and internal consistency metrics for our emotion intensity ratings. The number of raters ranged from 84 to 103 for our six emotions. The average reliability across raters was excellent for all of the emotions, with ICC (2,k) ranging from 0.95 to 0.98. Internal consistency was also excellent for all emotions in terms of Cronbach’s alpha (α = 0.97 to 0.99) and McDonald’s omega total (ωt = 0.97 to 0.99; see <a href=\"#tbl-emo-agree\" class=\"quarto-xref\">Table 10</a>)."
   ],
   "id": "3375a496-ed28-4907-a063-caeed87e16e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl-subcap: ICCs values are ICC (2,k) values.\n",
    "\n",
    "data.frame(1:5)"
   ],
   "id": "0672a7ed-fca3-4b3b-9333-c77c680b8a91"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Points of Stability\n",
    "\n",
    "To determine the number of raters required for stable ratings, we computed the point of stability (Hehman et al., 2025). This approach estimates the smallest sample size at which mean ratings stabilize by exceeding a cosine similarity threshold of 0.5 with the full-sample mean. As shown in Figures 4-6, the ratings to the standardized images reached stability between 37 and 45 raters, the ratings to the unstandardized images reached stability between 37 and 42 raters, and the ratings for the emotion intensities reached stability between 29 and 50 raters."
   ],
   "id": "e5bc0876-9867-47d1-bde3-4cf6e231d25a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-std-neu-pos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-unstd-neu-pos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-emo-pos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Descriptive Statistics\n",
    "\n",
    "In this section, we report the descriptive statistics for ratings of the neutral standardized images, the neutral unstandardized images, the emotion categorization task, the intensity of expressed emotions, and the ages of the models.\n",
    "\n",
    "##### 3.2.5.1 Standardized Neutral Trait Ratings\n",
    "\n",
    "Trustworthiness, dominance, and memorability ratings were approximately normally distributed and showed similar patterns of central tendency and variance. In contrast, attractiveness ratings were somewhat right-skewed, reflected in a relatively lower mean, while gender-typicality ratings were left-skewed, with a relatively higher mean (see <a href=\"#tbl-std-neu-desc\" class=\"quarto-xref\">Table 11</a> for descriptive statistics and <a href=\"#fig-std-neu-hist\" class=\"quarto-xref\">Figure 10</a> for rating distributions)."
   ],
   "id": "6611ff74-1660-432b-be42-89639b334de8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "9f19e9b8-2cb9-4373-97d5-e593aee84c6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-std-neu-hist"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key aim of this study was to provide normative data for each face model in the database. To visualize rating patterns, we created heatmaps with rating values on the x-axis and face models (separated by contributing lab site) on the y-axis. These heatmaps illustrate how the distribution of ratings varied across models and traits, with lighter tiles indicating that more raters gave a particular rating to that model (see Appendix) — capturing both between-model differences within a trait and within-model differences across traits.\n",
    "\n",
    "##### 3.2.5.2 Unstandardized Neutral Trait Ratings\n",
    "\n",
    "The ratings for dominance were approximately normally distributed, whereas the ratings for attractiveness were slightly skewed right and the ratings for trustworthiness were slightly skewed left, each reflected in their relatively low and high mean ratings (see <a href=\"#tbl-unstd-neu-desc\" class=\"quarto-xref\">Table 12</a> for descriptive statistics and <a href=\"#fig-unstd-neu-hist\" class=\"quarto-xref\">Figure 11</a> for rating distributions)."
   ],
   "id": "81fd12f8-16ea-40df-a48c-79c42365b8fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "ede538cb-bf08-4df5-8d22-152f78e7853f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-unstd-neu-hist"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the ratings of the standardized images, we produced heatmaps for each of the three traits capturing between-model differences within a trait and within-model differences across the traits (see Appendix).\n",
    "\n",
    "##### 3.2.5.3 Emotion Categorization and Intensity Ratings.\n",
    "\n",
    "Next, we explored raters’ emotion categorizations and perceived emotion intensity expressed by our models. Raters’ categorizations generally aligned with the models’ expression of emotion, with the greatest alignment was observed for expression of happiness, and the least for fear (<a href=\"#tbl-emo-desc\" class=\"quarto-xref\">Table 13</a>, <a href=\"#fig-emo-freq\" class=\"quarto-xref\">Figure 12</a>)."
   ],
   "id": "b272cef8-7399-4fe1-9b3c-90ecd25ef587"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbl-subcap: Note. Italicised values indicate correct categorisations (categorisations matching intended model expression).\n",
    "\n",
    "data.frame(1:5)"
   ],
   "id": "7e61ec24-0554-484f-a4f3-f01cff0a788d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot()"
   ],
   "id": "e05ab430-e023-4235-ad7c-091478faab73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculated the average ratings of emotion intensity for each emotion (<a href=\"#tbl-intensity-desc\" class=\"quarto-xref\">Table 14</a>). These ratings were very similar, on average, between the different emotions except for happiness, which was rated with greater intensity than all other emotions. On average, these ratings were located around the middle of the 7-point scale. <a href=\"#fig-intensity-hist\" class=\"quarto-xref\">Figure 13</a> shows the distribution of ratings on the scale for each emotion."
   ],
   "id": "04ae5379-627b-4050-a70a-abe50edb834e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(1:5)"
   ],
   "id": "94b7b9c7-8bb0-459b-916b-0056da50cc34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-intensity-hist"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.5.4 Perceived Age\n",
    "\n",
    "The distribution of raters’ perceptions of models’ ages is shown in <a href=\"#fig-age-hist\" class=\"quarto-xref\">Figure 14</a>. 26-30 years was the most-chosen age across raters and models. The correlation between models’ mode perceived age and actual age appears in <a href=\"#fig-age-corr\" class=\"quarto-xref\">Figure 15</a>, and heatmaps of perceived age for each model appear in the Appendix."
   ],
   "id": "82fd21d2-1b4e-495e-ac88-e294c3d27f5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-age-hist"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "metadata": {},
     "data": {}
    }
   ],
   "source": [
    "ggplot()"
   ],
   "id": "cell-fig-age-corr"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6 Exploratory Inferential Statistics"
   ],
   "id": "670562ba-a863-46ea-877d-873fd4d7acbf"
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<!-- to add -->"
   ],
   "id": "c45b9110-5505-49fc-ab99-a879e2e66738"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.6.1 Standardized vs Unstandardized Neutral Trait Ratings\n",
    "\n",
    "Compare attractiveness, dominance, and trustworthiness ratings across level of standardization for models w/ both kinds of photo (3 t-tests)\n",
    "\n",
    "##### 3.2.6.2 Emotion Categorizations\n",
    "\n",
    "Compare the ‘correct’ proportions across expression type (i.e., to show that happiness is perceived significantly more correctly & fear significantly more inaccurately than the others)\n",
    "\n",
    "##### 3.2.6.3 Emotion Intensity Ratings\n",
    "\n",
    "Compare emotion intensity ratings across expression type (i.e., to show that happiness appears most intense)\n",
    "\n",
    "## 4 Discussion\n",
    "\n",
    "### 4.1 The value of the current work\n",
    "\n",
    "This database provides a uniquely diverse face image stimulus set in terms of model nationality and image variety (appearance standardization, viewing angle, and facial expression). The diversity of images within the database makes it potentially useful to address a variety of research questions in multiple subfields of psychology (e.g., face recognition, emotion). It accomplishes this while being transparent and reproducible via the openly available protocol. Additionally, the database need not remain static: Other researchers can follow the protocol and add to the database, further increasing its sample size and diversity. This protocol can also serve as a template for researchers interested in collecting stimulus images, in terms of highlighting the kinds of details that need to be considered and documented)\n",
    "\n",
    "The data associated with the images are also valuable. First, we collected models’ self-reported demographic information, which is useful for various kinds of research questions. The validation/norming data we collected also provide a useful starting point for researchers interested in using the images, for example enabling researchers to choose subsets of images most suited to address their research questions. These data also provide information about the minimum number of raters needed for reliable mean ratings for different judgments (see [Hehman et al., 2025](#ref-hehman_assessing_2025)).\n",
    "\n",
    "This work also provides proof of concept that various labs, all following the same protocol and using the same equipment, can take comparable images to form a coherent face database. This broadens opportunities for future image database collection: A database does not need to all be collected by one lab in one location.\n",
    "\n",
    "### 4.2 Reflections on the Process & Limitations\n",
    "\n",
    "The leadership team found that having a broad variety of areas of expertise and diverse research backgrounds within the broader team was invaluable in both developing the database and in troubleshooting issues. We moreover found that setting out expectations at the start of the project through a collaboration agreement was essential. However, we ran into various issues throughout the process, which are common to Big Team Science initiatives. For example, with so many labs and individual researchers involved, timelines for completion necessarily stretched. This made momentum difficult to maintain at times. Planning in generous buffer time for such large-scale projects and managing team members’ expectations around timelines is therefore essential. We also faced difficulty in finding an optimal way to communicate effectively with all team members, given varying preferences (e.g., via email vs. other team communication platforms). It is perhaps worth surveying members of a big team at project start about communication possibilities, as well as outlining more specific communication expectations in a collaboration agreement.\n",
    "\n",
    "After Phase 1, we surveyed the ManyFaces research team to provide feedback on the process of collecting images using the protocol. Team members commonly expressed the desire for additional concise guidance in following the protocol, including an additional abbreviated protocol with numbered steps to follow during data collection and a video tutorial (in addition to the provided illustrative images) showing the necessary steps for researchers unfamiliar with camera equipment. These are requests that we can incorporate into future protocol updates. Doing so could both make the process easier for researchers and minimize errors and deviations from the protocol, ensuring greater consistency between sites.\n",
    "\n",
    "The major issue raised by the research team was the difficulty of eliciting facial expressions as described in the protocol. The protocol took a straightforward instructive approach to emotion elicitation (e.g., asking participants to face the camera with the expression they would make if they were feeling happy), following consultation with emotion experts in ManyFaces and discussion of feasibility with the wider team. However, models self-reported difficulty with the facial expression posing, in line with researchers’ feedback. The emotion validation data also indicate that models struggled to pose the facial expressions of emotion. The data furthermore suggest that perceivers had difficulty identifying emotions from these posed facial expressions. In support of this idea, recent research shows posed expressions to not appear genuine and reveals differing perceptions of posed and spontaneous expressions ([Dawel et al., 2017](#ref-dawel_perceived_2017), [2025](#ref-dawel_faking_2025)). Future work, including possible updates to our own protocol, may therefore focus on developing a better framework to elicit facial expressions. This could include taking inspiration from databases of naturally-induced emotion (e.g., [Miolla et al., 2023](#ref-miolla_padova_2023); [Sneddon et al., 2012](#ref-sneddon_belfast_2012)) and considering potential cultural differences and specific local needs.\n",
    "\n",
    "The research team also raised several comments about the provided and not-provided equipment, specifically the flimsiness of the light stand and the missing standardized backdrop (due to shipping constraints, participating researchers were to source a white background, e.g., a wall, seamless fabric, or paper). It is worth highlighting that the images were captured with equipment selected due to several practical constraints: budget, university-approved vendors, shipping logistics, interoperability, and ease of use for non-experts in varying conditions. The available project budget and administrative restrictions on approved vendors (their offerings and stock) for the University of Glasgow primarily limited the attainable camera, lens, and lighting setup. In general, we opted to create a setup that would not be prohibitively expensive to acquire, could be shipped in a single parcel, and would be compatible (substitutable) with equipment other researchers may have available. The last consideration was creating a setup with the fewest degrees of freedom, and thus less room for error. Therefore, we opted to use an LED ring light with a camera mounted inside the ring in landscape orientation on a single stand (vs. portrait orientation using a different kind of mount or separate stands for the camera and light). Ring lights became ubiquitous and easy to acquire in recent years; they only need to be positioned square, level, and in front of the sitter. Other, more complex setups can be created; however, the price, mobility, and repeatability of such setups may represent major constraints to consistent and reliable image collection. It is therefore worth considering trade-offs between different kinds of equipment and setups in future work.\n",
    "\n",
    "In addition to our choices of equipment, we also made certain pragmatic choices during image processing that future work could improve on. For example, we white-balanced the images rather than fully colour-correcting them, as we were able to create a fully reproducible scripted method for the former process, but not the latter. This was due to limitations such as varied colour checker chart placement and orientation in models’ photos and a lack of expertise on the research team to work around this in a reproducible way. We therefore opted to simply white-balance to keep the process entirely open and reproducible, rather than fully colour-correct using a manual and not fully reproducible method. The process of aligning and sizing the images was also driven by cropping needs. That is, due to the landscape orientation of the photos, there was limited vertical space that we could crop, constraining the possibilities for face alignment and image size. Future work could consider using alternative equipment setups to enable capture images in portrait mode. Variation in faces’ size in the images, due to some deviations from the distances specified in the protocol, also affected the image processing steps. This could be addressed in future work by more clearly highlighting not only the key aspects of the protocol that should be kept constant, but also why these aspects should be constant (i.e., clarifying the reasoning to all team members).\n",
    "\n",
    "Finally, we collected images of 211 models, but no single image type included images of all models. Rather, the maximum number of models represented in a single image type was 205 images (standardized neutral, front-facing). This led to missing perceived age/ethnicity data for six models who had unstandardized but not standardized neutral images available.\n",
    "\n",
    "### 4.3 Conclusion\n",
    "\n",
    "Here, we introduce a new diverse face image database, which will be useful to researchers interested in a variety of questions related to social perception, person recognition, and vision science. We demonstrate that a cohesive database can be compiled across a variety of sites, opening doors for future additions to this database and the development of future multi-lab databases.\n",
    "\n",
    "Burton, A. M., White, D., & McNeill, A. (2010). The Glasgow Face Matching Test. *Behavior Research Methods*, *42*(1), 286–291. <https://doi.org/10.3758/BRM.42.1.286>\n",
    "\n",
    "Chen, J. M., Norman, J. B., & Nam, Y. (2021). Broadening the stimulus set: Introducing the American Multiracial Faces Database. *Behavior Research Methods*, *53*(1), 371–389. <https://doi.org/10.3758/s13428-020-01447-8>\n",
    "\n",
    "Cook, R., & Over, H. (2021). Why is the literature on first impressions so focused on White faces? *Royal Society Open Science*, *8*(9), 211146. <https://doi.org/10.1098/rsos.211146>\n",
    "\n",
    "Dawel, A., Krumhuber, E. G., & Palermo, R. (2025). Faking It Isn’t Making It: Research Needs Spontaneous and Naturalistic Facial Expressions. *Affective Science*. <https://doi.org/10.1007/s42761-025-00320-1>\n",
    "\n",
    "Dawel, A., Wright, L., Irons, J., Dumbleton, R., Palermo, R., O’Kearney, R., & McKone, E. (2017). Perceived emotion genuineness: Normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets. *Behavior Research Methods*, *49*(4), 1539–1562. <https://doi.org/10.3758/s13428-016-0813-2>\n",
    "\n",
    "DeBruine, L. M., Holzleitner, I. J., Tiddeman, B., & Jones, B. C. (2022). *Reproducible Methods for Face Research*. PsyArXiv. <https://doi.org/10.31234/osf.io/j2754>\n",
    "\n",
    "DeBruine, L. M., Lai, R., Jones, B. C., Abdullah, R., & Mahrholz, G. (2020). *Experimentum*. Zenodo. <https://doi.org/10.5281/zenodo.4010579>\n",
    "\n",
    "Ebner, N. C., Riediger, M., & Lindenberger, U. (2010). FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation. *Behavior Research Methods*, *42*(1), 351–362. <https://doi.org/10.3758/BRM.42.1.351>\n",
    "\n",
    "Gao, W., Cao, B., Shan, S., Chen, X., Zhou, D., Zhang, X., & Zhao, D. (2008). The CAS-PEAL Large-Scale Chinese Face Database and Baseline Evaluations. *IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans*, *38*(1), 149–161. <https://doi.org/10.1109/TSMCA.2007.909557>\n",
    "\n",
    "Hehman, E., Xie, S. Y., Ofosu, E. K., & Nespoli, G. A. (2025). Assessing the Point at Which Averages Are Stable: A Tutorial in the Context of Impression Formation. *Social Cognition*, *43*(5), 488–501. <https://doi.org/10.1521/soco.2025.43.5.488>\n",
    "\n",
    "Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? *Behavioral and Brain Sciences*, *33*(2-3), 61–83. <https://doi.org/10.1017/S0140525X0999152X>\n",
    "\n",
    "ImageMagick Studio LLC. (2024). *ImageMagick* (Version 7.1.1) \\[Computer software\\]. <https://imagemagick.org>\n",
    "\n",
    "Jones, B. C., DeBruine, L. M., Flake, J. K., Liuzza, M. T., Antfolk, J., Arinze, N. C., Ndukaihe, I. L. G., Bloxsom, N. G., Lewis, S. C., Foroni, F., Willis, M. L., Cubillas, C. P., Vadillo, M. A., Turiegano, E., Gilead, M., Simchon, A., Saribay, S. A., Owsley, N. C., Jang, C., … Coles, N. A. (2021). To which world regions does the valence–dominance model of social perception apply? *Nature Human Behaviour*, *5*(1), 159–169. <https://doi.org/10.1038/s41562-020-01007-2>\n",
    "\n",
    "Lakshmi, A., Wittenbrink, B., Correll, J., & Ma, D. S. (2021). The India Face Set: International and Cultural Boundaries Impact Face Impressions and Perceptions of Category Membership. *Frontiers in Psychology*, *12*. <https://www.frontiersin.org/articles/10.3389/fpsyg.2021.627678>\n",
    "\n",
    "Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D. H. J., Hawk, S. T., & Knippenberg, A. van. (2010). Presentation and validation of the Radboud Faces Database. *Cognition and Emotion*, *24*(8), 1377–1388. <https://doi.org/10.1080/02699930903485076>\n",
    "\n",
    "Lundqvist, D., Flykt, A., & Öhman, A. (2015). *Karolinska Directed Emotional Faces*. <https://doi.org/10.1037/t27732-000>\n",
    "\n",
    "Ma, D. S., Correll, J., & Wittenbrink, B. (2015). The Chicago face database: A free stimulus set of faces and norming data. *Behavior Research Methods*, *47*(4), 1122–1135. <https://doi.org/10.3758/s13428-014-0532-5>\n",
    "\n",
    "Ma, D. S., Kantner, J., & Wittenbrink, B. (2021). Chicago Face Database: Multiracial expansion. *Behavior Research Methods*, *53*(3), 1289–1300. <https://doi.org/10.3758/s13428-020-01482-5>\n",
    "\n",
    "Meyers, C., Garay, M., & Pauker, K. (2024). *Hawai‘i Face Database: A racially and ethnically diverse set of facial stimuli*. <https://doi.org/10.31234/osf.io/wde4b>\n",
    "\n",
    "Miolla, A., Cardaioli, M., & Scarpazza, C. (2023). Padova Emotional Dataset of Facial Expressions (PEDFE): A unique dataset of genuine and posed emotional facial expressions. *Behavior Research Methods*, *55*(5), 2559–2574. <https://doi.org/10.3758/s13428-022-01914-4>\n",
    "\n",
    "Oosterhof, N. N., & Todorov, A. (2008). The functional basis of face evaluation. *Proceedings of the National Academy of Sciences*, *105*(32), 11087–11092. <https://doi.org/10.1073/pnas.0805664105>\n",
    "\n",
    "Perrett, D. I. (2017). *In Your Face: The new science of human attraction*. Bloomsbury Publishing.\n",
    "\n",
    "Saribay, S. A., Biten, A. F., Meral, E. O., Aldan, P., Třebický, V., & Kleisner, K. (2018). The Bogazici face database: Standardized photographs of Turkish faces with supporting materials. *PLOS ONE*, *13*(2), e0192018. <https://doi.org/10.1371/journal.pone.0192018>\n",
    "\n",
    "Schalk, J. van der, Hawk, S. T., Fischer, A. H., & Doosje, B. (2011). Moving faces, looking places: Validation of the Amsterdam Dynamic Facial Expression Set (ADFES). *Emotion*, *11*(4), 907–920. <https://doi.org/10.1037/a0023853>\n",
    "\n",
    "Sneddon, I., McRorie, M., McKeown, G., & Hanratty, J. (2012). The Belfast Induced Natural Emotion Database. *IEEE Transactions on Affective Computing*, *3*(1), 32–41. <https://doi.org/10.1109/T-AFFC.2011.26>\n",
    "\n",
    "Sutherland, C. A. M., Oldmeadow, J. A., Santos, I. M., Towler, J., Michael Burt, D., & Young, A. W. (2013). Social inferences from faces: Ambient images generate a three-dimensional model. *Cognition*, *127*(1), 105–118. <https://doi.org/10.1016/j.cognition.2012.12.001>\n",
    "\n",
    "Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., Marcus, D. J., Westerlund, A., Casey, B., & Nelson, C. (2009). The NimStim set of facial expressions: Judgments from untrained research participants. *Psychiatry Research*, *168*(3), 242–249. <https://doi.org/10.1016/j.psychres.2008.05.006>\n",
    "\n",
    "Třebický, V., Fialová, J., Kleisner, K., & Havlíček, J. (2016). Focal Length Affects Depicted Shape and Perception of Facial Images. *PLOS ONE*, *11*(2), e0149313. <https://doi.org/10.1371/journal.pone.0149313>\n",
    "\n",
    "Třebický, V., Třebická Fialová, J., Bjornsdottir, R. T., & DeBruine, L. M. (2024). *A Grim Image: Considerations for Methods of portrait photography in psychological science*. OSF Preprints. <https://doi.org/10.31219/osf.io/z4svb>\n",
    "\n",
    "Trzewik, M., Navon, M., Moran, T., Wardi, H., Langer, A., Hadad, B.-S., Sofer, C., & Reggev, N. (2025). The Israeli Face Database (IFD): A multi-ethnic database of faces with supporting social norming data. *Behavior Research Methods*, *57*(7), 197. <https://doi.org/10.3758/s13428-025-02723-1>\n",
    "\n",
    "Workman, C. I., & Chatterjee, A. (2021). The Face Image Meta-Database (<span class=\"nocase\">fIMDb</span>) & ChatLab Facial Anomaly Database (CFAD): Tools for research on face perception and social stigma. *Methods in Psychology*, *5*, 100063. <https://doi.org/10.1016/j.metip.2021.100063>\n",
    "\n",
    "Yarkoni, T. (2022). The generalizability crisis. *Behavioral and Brain Sciences*, *45*, e1. <https://doi.org/10.1017/S0140525X20001685>\n",
    "\n",
    "Young, A. W., & Burton, A. M. (2017). Recognizing Faces. *Current Directions in Psychological Science*, *26*(3), 212–217. <https://doi.org/10.1177/0963721416688114>\n",
    "\n",
    "Zebrowitz, L. (1997). *Reading Faces: Window To The Soul?* Routledge."
   ],
   "id": "a35e7b8b-9dbd-477b-a6f4-31ce85c489a1"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
